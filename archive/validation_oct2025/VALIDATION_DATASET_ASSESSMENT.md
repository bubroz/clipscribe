# ClipScribe Validation Dataset Assessment
*Comprehensive Analysis of Ground Truth Resources for WhisperX + Gemini Validation*

**Date:** October 21, 2025  
**Context:** Post-Gemini integration, need ground truth data to validate speaker diarization accuracy  
**Goal:** Find professionally transcribed multi-speaker audio to measure actual system performance

---

## Executive Summary

**RECOMMENDATION: Start with AnnoMI (easiest) â†’ CHiME-6 (most challenging) â†’ AMI (comprehensive)**

After thorough research, **3 of 8 datasets** pass ClipScribe's architectural and practical requirements:

| Priority | Dataset | Fit Score | Ease of Use | Validation Value |
|----------|---------|-----------|-------------|------------------|
| **#1** | **AnnoMI** | â­â­â­â­ | ðŸŸ¢ Easy | High (dyadic, clean format) |
| **#2** | **CHiME-6** | â­â­â­â­â­ | ðŸŸ¡ Medium | **Highest** (stress test) |
| **#3** | **AMI** | â­â­â­â­â­ | ðŸ”´ Hard | Comprehensive |
| â€” | ICSI | â­â­â­ | ðŸ”´ Hard | Good but similar to AMI |
| âŒ | Mandarin datasets | â­ | N/A | Wrong language |

---

## Detailed Analysis

### âœ… TIER 1: RECOMMENDED

#### **#1 PRIORITY: AnnoMI Dataset**

**Why This Fits:**
- âœ… **Format**: CSV with YouTube URLs (already ClipScribe-compatible!)
- âœ… **Speakers**: 2 per conversation (dyadic) - perfect for validation
- âœ… **Duration**: 133 conversations, many 30+ min
- âœ… **Quality**: Expert-annotated, therapeutic conversations
- âœ… **Access**: GitHub repo, immediate download
- âœ… **Integration**: Zero conversion needed - CSV â†’ Python dict â†’ validation

**Architecture Alignment:**
```python
# AnnoMI â†’ ClipScribe validation pipeline
# 1. Read CSV (pandas)
# 2. Download audio via yt-dlp (already in ClipScribe)
# 3. Process with WhisperX + Gemini
# 4. Compare against ground truth speaker labels
# 5. Calculate DER (Diarization Error Rate)
```

**Challenges:**
- ðŸŸ¡ Audio from YouTube (requires yt-dlp extraction)
- ðŸŸ¡ Only 2 speakers (doesn't test complex multi-party)
- ðŸŸ¡ Therapy context (specific domain, but diverse accents)

**Download:**
```bash
git clone https://github.com/uccollab/AnnoMI
# 133 transcripts with speaker labels immediately available
# Audio via YouTube URLs in CSV
```

**Validation Metrics:**
- Word Error Rate (WER): Compare transcript.text vs ground truth
- Speaker Attribution Accuracy: % of segments with correct speaker
- Diarization Error Rate (DER): Standard metric for speaker ID

**Estimated Effort:** 4-6 hours to build validation pipeline

---

#### **#2 PRIORITY: CHiME-6 Challenge Dataset**

**Why This is CRITICAL:**
- âœ… **Format**: JSON transcriptions (native ClipScribe format!)
- âœ… **Speakers**: 4 per meeting (realistic multi-party)
- âœ… **Duration**: 40 hours, sessions 30-120min each
- âœ… **Quality**: Benchmark dataset, professionally verified
- âœ… **Challenge Level**: Far-field, noise, overlaps - **STRESS TEST**
- âœ… **License**: CC BY-SA 4.0 (research-friendly)

**Architecture Alignment:**
- **Perfect fit**: JSON transcriptions = direct comparison
- **Stress test**: Far-field audio will reveal WhisperX weaknesses
- **Benchmark**: Industry-standard metrics (DER from CHiME challenge)

**Challenges:**
- ðŸ”´ **Size**: 120GB total (97GB train, 11GB dev, 12GB eval)
- ðŸŸ¡ **Format complexity**: Multi-channel audio (can use single channel)
- ðŸŸ¡ **Setup**: Requires understanding CHiME challenge structure

**Download:**
```bash
# Start with dev set (11GB) for validation
wget https://openslr.trmal.net/resources/150/CHiME6_dev.tar.gz
wget https://openslr.trmal.net/resources/150/CHiME6_transcriptions.tar.gz
```

**Why This Matters:**
- **Real-world conditions**: If ClipScribe handles CHiME-6, it handles anything
- **Benchmark comparison**: Compare our DER to CHiME-6 challenge winners
- **Quality proof**: "Validated against CHiME-6 benchmark" = credibility

**Validation Metrics:**
- DER (Diarization Error Rate): Industry standard
- WER in noisy conditions
- Speaker confusion matrix (which speakers get mixed up)

**Estimated Effort:** 8-12 hours (download + setup + validation)

---

#### **#3 PRIORITY: AMI Meeting Corpus**

**Why This is Comprehensive:**
- âœ… **Scale**: 100 hours, ~170 meetings
- âœ… **Speakers**: 4 per meeting (consistent)
- âœ… **Duration**: 30-60min per meeting
- âœ… **Quality**: Gold standard for meeting diarization
- âœ… **Annotations**: Orthographic, dialog acts, speaker labels
- âœ… **License**: CC BY 4.0

**Architecture Alignment:**
- **Industry standard**: Most cited meeting corpus in ASR research
- **Comprehensive**: Covers scenario-based and natural meetings
- **Multi-modal**: Can ignore video, use audio + transcripts

**Challenges:**
- ðŸ”´ **Format**: NXT XML (requires conversion to JSON)
- ðŸ”´ **Complexity**: Need NXT toolkit or custom parser
- ðŸ”´ **Size**: 100 hours = significant storage/processing
- ðŸŸ¡ **European accents**: May differ from U.S.-focused content

**Download:**
Follow instructions at https://groups.inf.ed.ac.uk/ami/download

**Integration Strategy:**
1. Download sample meetings (5-10 hours)
2. Convert NXT XML â†’ JSON (write parser or use existing tools)
3. Extract WAV audio from multi-channel (use headset mix)
4. Run ClipScribe validation pipeline

**Validation Metrics:**
- Full DER calculation (gold standard)
- Per-speaker accuracy
- Meeting-level quality assessment

**Estimated Effort:** 12-20 hours (format conversion + validation)

---

### âŒ TIER 2: NOT RECOMMENDED

#### **ICSI Meeting Corpus**

**Why Skip:**
- âš ï¸ Similar to AMI (meeting domain)
- ðŸ”´ Smaller (70 hours vs AMI's 100 hours)
- ðŸ”´ Same NXT format challenges
- ðŸŸ¡ Technical meetings (less diverse than AMI scenarios)

**Decision:** Use AMI instead for meeting validation (better documented, larger)

---

#### **Mandarin Datasets (AISHELL-4/5, AliMeeting, MAGICDATA)**

**Why Skip:**
- âŒ **Language mismatch**: ClipScribe is English-focused
- âŒ **Architecture**: WhisperX models optimized for English
- âŒ **Use case**: User base is primarily English content

**Decision:** Not relevant for current validation needs

---

#### **DiPCo (Dinner Party Corpus)**

**Why Skip:**
- ðŸ”´ **Size**: Only 5 hours total
- ðŸ”´ **Availability**: Requires paper contact/request
- ðŸŸ¡ **Format**: Not as well documented as CHiME-6

**Decision:** CHiME-6 provides better far-field validation with easier access

---

## Prioritized Implementation Plan

### **Phase 1: Quick Win (Week 1)**
**Dataset:** AnnoMI  
**Goal:** Validate dyadic (2-speaker) accuracy

**Steps:**
1. Download AnnoMI CSV from GitHub (5 min)
2. Select 10 conversations (30+ min each)
3. Download audio via yt-dlp
4. Run ClipScribe pipeline
5. Compare speaker attribution vs ground truth
6. Calculate metrics: WER, speaker accuracy, DER

**Code:**
```python
import pandas as pd
import subprocess

# Load AnnoMI
df = pd.read_csv('AnnoMI-simple.csv')

# Group by transcript_id, filter for long conversations
transcripts = df.groupby('transcript_id').agg({
    'video_url': 'first',
    'utterance_id': 'count'  # Number of utterances
}).query('utterance_id > 50')  # ~30+ min conversations

# Download and process
for tid, row in transcripts.head(10).iterrows():
    # Download audio
    subprocess.run(['yt-dlp', '-x', '--audio-format', 'mp3', row['video_url']])
    
    # Process with ClipScribe
    result = process_video(row['video_url'])
    
    # Compare speakers
    ground_truth = df[df.transcript_id == tid]
    validate_speakers(result, ground_truth)
```

**Expected Output:**
- Speaker attribution accuracy: Target >90%
- Identify systematic errors (if any)
- Confidence in Gemini corrections

**Estimated Time:** 6-8 hours total

---

### **Phase 2: Stress Test (Week 2)**
**Dataset:** CHiME-6  
**Goal:** Validate under challenging conditions

**Steps:**
1. Download CHiME-6 dev set (11GB) + transcriptions (2.4MB)
2. Extract single-channel audio from multi-channel
3. Process 10-20 meetings with ClipScribe
4. Compare against JSON ground truth
5. Calculate DER using pyannote.metrics
6. Compare to CHiME-6 challenge baselines

**Code:**
```python
import json
from pyannote.core import Annotation, Segment
from pyannote.metrics.diarization import DiarizationErrorRate

# Load CHiME-6 ground truth
with open('chime6_transcript.json') as f:
    gt = json.load(f)

# Process with ClipScribe
result = process_video('chime6_audio.wav')

# Calculate DER
reference = Annotation()  # Build from ground truth
hypothesis = Annotation()  # Build from ClipScribe output

der_metric = DiarizationErrorRate()
der_score = der_metric(reference, hypothesis)

print(f'DER: {der_score:.2%}')
```

**Expected Challenges:**
- Far-field audio will increase WER
- Overlapping speech will challenge diarization
- May reveal Gemini limitations

**Expected Time:** 10-15 hours total

---

### **Phase 3: Comprehensive Benchmark (Future)**
**Dataset:** AMI  
**Goal:** Full validation against industry standard

**Steps:**
1. Download AMI subset (10-20 meetings)
2. Build NXT XML â†’ JSON converter
3. Process meetings with ClipScribe
4. Generate comprehensive performance report
5. Publish results as ClipScribe validation study

**Deferred Because:**
- NXT format requires significant parser work
- Can validate with AnnoMI + CHiME-6 first
- AMI is overkill for initial validation

---

## Architecture Compatibility Analysis

### âœ… **Format Compatibility**

| Dataset | Audio Format | Transcript Format | ClipScribe Integration |
|---------|--------------|-------------------|------------------------|
| AnnoMI | YouTube (MP3) | CSV with timestamps | **Native** (yt-dlp exists) |
| CHiME-6 | WAV (multi-channel) | **JSON** | **Perfect** match |
| AMI | WAV | XML (NXT) | Requires parser |

### âœ… **Cost Analysis**

| Dataset | Download Size | Processing Cost | Storage Cost |
|---------|---------------|-----------------|--------------|
| AnnoMI | ~5GB audio | $0.24 (133 Ã— $0.0018/min avg) | Minimal |
| CHiME-6 | 11GB (dev only) | $0.72 (40hrs Ã— $0.0018/min) | Moderate |
| AMI | 50GB+ (subset) | $1.80 (100hrs) | Significant |

### âœ… **Validation Metrics Supported**

**Can Measure:**
- âœ… Word Error Rate (WER): Character-level comparison
- âœ… Speaker Attribution Accuracy: % correct speaker labels
- âœ… Diarization Error Rate (DER): Using pyannote.metrics
- âœ… Gemini Correction Quality: Precision/recall of corrections
- âœ… Processing Cost vs Accuracy trade-offs

**Dependencies Needed:**
```bash
pip install jiwer  # WER calculation
pip install pyannote.metrics  # DER calculation
pip install pandas  # AnnoMI processing
```

---

## Project Guidelines Compliance

### âœ… **Security** (from user philosophy)
- All datasets are public, licensed for research
- No PII concerns (consent obtained for all recordings)
- Can be processed locally or on Modal (no data leakage)

### âœ… **Cost-First Design** (ClipScribe principle)
- AnnoMI: Minimal cost (~$0.24 for full validation)
- CHiME-6: Moderate ($0.72 for stress testing)
- Validates cost/quality trade-offs

### âœ… **Testing Standards** (from rules)
- Provides 100% objective ground truth
- No mocking needed - real-world validation
- Quantifiable metrics (WER, DER, accuracy)

### âœ… **Documentation** (from rules)
- Can publish validation results in docs/
- Transparent benchmark comparisons
- Builds credibility ("Validated against CHiME-6")

---

## Datasets REJECTED

### âŒ **AISHELL-4, AISHELL-5, AliMeeting, MAGICDATA**
**Reason:** Mandarin language, incompatible with English-focused architecture  
**Impact:** Zero value for English content validation

### âŒ **LibriSpeech**
**Reason:** Read speech (audiobooks), single speaker per file  
**Impact:** No multi-speaker diarization ground truth

### âŒ **DiPCo**
**Reason:** Only 5 hours, harder to access than CHiME-6  
**Impact:** CHiME-6 provides better far-field validation

### âŒ **Government Archives (C-SPAN, NPR)**
**Reason:** No structured ground truth, manual pairing required  
**Impact:** Too much manual work vs academic datasets

---

## Implementation Roadmap

### **Immediate (This Week): AnnoMI Validation**

**Deliverable:** `scripts/validation/annomi_validator.py`

```python
"""
AnnoMI Validation Script for ClipScribe

Validates WhisperX + Gemini against expert-annotated
motivational interviewing conversations.

Metrics:
  - Speaker attribution accuracy
  - Word error rate (WER)
  - Gemini correction precision

Usage:
    poetry run python scripts/validation/annomi_validator.py
"""

import pandas as pd
from pathlib import Path
from jiwer import wer
from clipscribe.retrievers import UniversalVideoClient

def validate_annomi(num_samples=10):
    """Run validation against AnnoMI dataset."""
    
    # Load ground truth
    df = pd.read_csv('validation_data/AnnoMI-simple.csv')
    
    # Select diverse samples (high/low quality, different topics)
    samples = df.groupby('transcript_id').first().head(num_samples)
    
    results = []
    for tid, row in samples.iterrows():
        # Process with ClipScribe
        client = UniversalVideoClient()
        result = client.process_url(row['video_url'])
        
        # Compare against ground truth
        gt_segments = df[df.transcript_id == tid]
        
        # Calculate metrics
        metrics = {
            'transcript_id': tid,
            'speaker_accuracy': calculate_speaker_accuracy(result, gt_segments),
            'wer': calculate_wer(result, gt_segments),
            'gemini_corrections': count_gemini_corrections(result)
        }
        
        results.append(metrics)
    
    # Report
    print_validation_report(results)
    return results

if __name__ == "__main__":
    validate_annomi()
```

**Success Criteria:**
- Speaker attribution accuracy >90% (dyadic is easier)
- WER <15% (clean therapy audio, clear speech)
- Gemini corrections precision >80%

---

### **Next Month: CHiME-6 Stress Test**

**Deliverable:** `scripts/validation/chime6_benchmark.py`

**Goal:** Prove ClipScribe handles challenging real-world conditions

**Steps:**
1. Download CHiME-6 dev set (11GB)
2. Extract single-channel audio (reduce from multi-channel)
3. Process 20 meetings
4. Calculate industry-standard DER
5. Compare to CHiME-6 challenge baselines

**Success Criteria:**
- DER <20% (CHiME-6 baseline is ~15-25%)
- Handles overlapping speech gracefully
- Far-field audio doesn't break diarization

**Value:**
- **Marketing**: "Validated against CHiME-6, achieving X% DER"
- **Quality assurance**: Proves system works in worst-case
- **Research**: Can publish results, cite in papers

---

### **Future: AMI Comprehensive Validation**

**Only if:** Previous validations reveal issues needing deeper analysis

**Reason to defer:** 
- Complex format (NXT XML)
- Large download (50GB+)
- Overlaps with ICSI (same domain)
- AnnoMI + CHiME-6 cover 90% of validation needs

---

## Validation Pipeline Architecture

### **Proposed Structure:**

```
clipscribe/
â”œâ”€â”€ validation_data/
â”‚   â”œâ”€â”€ annomi/
â”‚   â”‚   â”œâ”€â”€ AnnoMI-simple.csv
â”‚   â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ chime6/
â”‚   â”‚   â”œâ”€â”€ transcriptions/
â”‚   â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ results/
â”‚       â”œâ”€â”€ annomi_results.json
â”‚       â””â”€â”€ chime6_results.json
â”‚
â”œâ”€â”€ scripts/validation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ annomi_validator.py
â”‚   â”œâ”€â”€ chime6_validator.py
â”‚   â”œâ”€â”€ metrics.py  # WER, DER calculations
â”‚   â””â”€â”€ report_generator.py
â”‚
â””â”€â”€ docs/
    â””â”€â”€ VALIDATION_RESULTS.md  # Public benchmark results
```

### **Metrics Module:**

```python
# scripts/validation/metrics.py

from jiwer import wer
from pyannote.metrics.diarization import DiarizationErrorRate

def calculate_wer(hypothesis: str, reference: str) -> float:
    """Calculate Word Error Rate."""
    return wer(reference, hypothesis)

def calculate_der(hypothesis_segments: list, reference_segments: list) -> float:
    """Calculate Diarization Error Rate using pyannote."""
    der_metric = DiarizationErrorRate()
    # Convert to pyannote Annotation objects
    ref = build_annotation(reference_segments)
    hyp = build_annotation(hypothesis_segments)
    return der_metric(ref, hyp)

def calculate_speaker_accuracy(result: dict, ground_truth: pd.DataFrame) -> float:
    """
    Calculate percentage of segments with correct speaker attribution.
    
    Note: Requires speaker label alignment (SPEAKER_01 â†’ therapist mapping)
    """
    # Map ClipScribe speakers to ground truth speakers
    mapping = align_speaker_labels(result, ground_truth)
    
    correct = 0
    total = 0
    
    for seg in result['segments']:
        mapped_speaker = mapping.get(seg['speaker'])
        gt_speaker = find_ground_truth_speaker(seg, ground_truth)
        
        if mapped_speaker == gt_speaker:
            correct += 1
        total += 1
    
    return correct / total if total > 0 else 0.0
```

---

## Cost-Benefit Analysis

### **AnnoMI (Phase 1)**

| Metric | Value |
|--------|-------|
| Download time | 30 minutes |
| Processing cost | $0.24 (133 conversations) |
| Setup effort | 4-6 hours |
| **Validation confidence** | **High** (dyadic, clear) |
| **ROI** | **Excellent** (quick wins) |

### **CHiME-6 (Phase 2)**

| Metric | Value |
|--------|-------|
| Download time | 2-4 hours (11GB) |
| Processing cost | $0.72 (40 hours) |
| Setup effort | 8-12 hours |
| **Validation confidence** | **Very High** (stress test) |
| **ROI** | **Excellent** (benchmark credibility) |

### **AMI (Future)**

| Metric | Value |
|--------|-------|
| Download time | 6-8 hours (50GB) |
| Processing cost | $1.80 (100 hours) |
| Setup effort | 12-20 hours (format conversion) |
| **Validation confidence** | **Highest** (comprehensive) |
| **ROI** | **Good** (if needed for research) |

---

## Architectural Considerations

### **Integration Points:**

1. **Video Retriever**: Already handles YouTube (AnnoMI ready)
2. **Transcriber**: WhisperX + Gemini pipeline is stable
3. **Output Format**: JSON matches CHiME-6 perfectly
4. **Metrics**: Need to add `jiwer` and `pyannote.metrics`

### **New Dependencies:**

```toml
# pyproject.toml additions
[tool.poetry.group.validation]
optional = true

[tool.poetry.group.validation.dependencies]
jiwer = "^3.0.0"  # WER calculation
pyannote-metrics = "^3.2.0"  # DER calculation
pandas = "^2.0.0"  # AnnoMI CSV processing
```

### **Storage Requirements:**

| Phase | Storage Needed |
|-------|----------------|
| AnnoMI | 5GB (audio) + 1MB (CSV) |
| CHiME-6 | 11GB (dev) + 2.4MB (transcripts) |
| Results cache | 100MB (processed JSON) |
| **Total** | **~16GB** for Phases 1-2 |

---

## Success Criteria

### **Minimum Viable Validation:**
- âœ… AnnoMI: >90% speaker accuracy on dyadic conversations
- âœ… WER <15% on clean audio
- âœ… Gemini corrections have >80% precision

### **Benchmark Quality:**
- âœ… CHiME-6: DER <20% (competitive with challenge baselines)
- âœ… Handles far-field and noisy conditions
- âœ… No catastrophic failures on overlapping speech

### **Publication Ready:**
- âœ… Documented methodology in docs/VALIDATION_RESULTS.md
- âœ… Reproducible scripts in scripts/validation/
- âœ… Comparison to state-of-the-art systems

---

## Risks and Mitigation

### **Risk #1: Format Conversion Complexity**
- **Mitigation**: Start with JSON-native CHiME-6
- **Fallback**: Use only AnnoMI if conversion blocked

### **Risk #2: Validation Reveals Low Accuracy**
- **Mitigation**: This is GOOD - identifies improvement areas
- **Response**: Tune thresholds, improve Gemini prompts

### **Risk #3: Dataset Download Failures**
- **Mitigation**: Multiple mirrors for CHiME-6
- **Fallback**: AnnoMI is tiny, always accessible

---

## Final Recommendation

### **START NOW: AnnoMI**
1. Clone repo: `git clone https://github.com/uccollab/AnnoMI`
2. Select 10 long conversations
3. Run validation this week
4. Document results

### **NEXT: CHiME-6**
1. Download dev set (11GB)
2. Run stress test
3. Compare to benchmark
4. Publish results

### **SKIP: Mandarin datasets, government archives, DiPCo**
- Wrong language or too much manual work
- AnnoMI + CHiME-6 provide sufficient validation

---

## Quality Assurance Impact

**With this validation:**
- âœ… Quantifiable accuracy metrics (not just "looks good")
- âœ… Benchmark comparison (industry credibility)
- âœ… Identifies systematic errors
- âœ… Validates cost/quality trade-offs
- âœ… Proves Gemini corrections add value

**Documentation Output:**
```markdown
# ClipScribe Validation Results

Validated against academic benchmark datasets:

- **AnnoMI (Dyadic)**: 94.2% speaker accuracy, WER 11.3%
- **CHiME-6 (Challenging)**: DER 18.7% (competitive with SOTA)
- **Gemini Corrections**: 87% precision, 4.2% improvement over baseline

Tested on 143 conversations (173 hours total).
```

---

## FINAL VERDICT

**PROCEED WITH:**
1. âœ… **AnnoMI** (this week) - Quick win, easy integration
2. âœ… **CHiME-6** (next month) - Stress test, benchmark proof
3. â¸ï¸ **AMI** (deferred) - Only if needed for comprehensive study

**SKIP:**
- âŒ Mandarin datasets (language mismatch)
- âŒ Government archives (manual effort too high)
- âŒ DiPCo (CHiME-6 is better)
- âŒ ICSI (AMI is better alternative)

**TIME TO VALUE:**
- AnnoMI validation: 1 week
- CHiME-6 validation: 2-3 weeks
- Published results: 1 month total

**This passes all architectural standards and provides the quantifiable validation you need.**

