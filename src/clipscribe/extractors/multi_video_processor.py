"""
Multi-Video Intelligence Processor for ClipScribe.

Handles cross-video entity resolution, relationship bridging, narrative flow analysis,
and unified knowledge graph generation with aggressive AI-powered entity merging.

Timeline Intelligence v2.0 Integration - Phase 5: Pipeline Integration
"""

import logging
import asyncio
import json
from typing import List, Dict, Any, Optional, Tuple, Set
from datetime import datetime, timedelta
from collections import defaultdict, Counter

from ..models import (
    VideoIntelligence, MultiVideoIntelligence, CrossVideoEntity, 
    CrossVideoRelationship, NarrativeSegment, TopicEvolution,
    VideoCollectionType, SeriesMetadata, Entity, Relationship, Topic,
    ConsolidatedTimeline, TimelineEvent, ExtractedDate,
    InformationFlowMap, ConceptNode, ConceptDependency, InformationFlow,
    ConceptEvolutionPath, ConceptCluster, ConceptMaturityLevel
)
from ..timeline.models import DatePrecision
from .entity_normalizer import EntityNormalizer
from .series_detector import SeriesDetector
from ..config.settings import Settings
from ..utils.web_research import WebResearchIntegrator, TimelineContextValidator

# ðŸš€ Timeline Intelligence v2.0 Integration
from ..timeline import (
    TemporalExtractorV2,
    EventDeduplicator, 
    ContentDateExtractor,
    TimelineQualityFilter,
    ChapterSegmenter,
    CrossVideoSynthesizer,
    TemporalEvent,
    SynthesisStrategy
)

logger = logging.getLogger(__name__)


class MultiVideoProcessor:
    """
    Processes multiple videos to extract unified intelligence.
    
    Features:
    - Aggressive cross-video entity resolution
    - Relationship bridging across videos
    - Narrative flow analysis for series
    - Topic evolution tracking
    - Unified knowledge graph generation
    - Quality metrics and validation
    """
    
    def __init__(self, use_ai_validation: bool = True):
        """
        Initialize multi-video processor with Timeline v2.0 integration.
        
        Args:
            use_ai_validation: Whether to use AI for entity validation and merging
        """
        self.entity_normalizer = EntityNormalizer(similarity_threshold=0.85)  # Aggressive merging
        self.series_detector = SeriesDetector(similarity_threshold=0.7)
        self.use_ai_validation = use_ai_validation
        self.settings = Settings()
        
        # v2.17.0: Timeline Building Pipeline components (Legacy - being replaced)
        self.web_research_integrator = WebResearchIntegrator(
            api_key=self.settings.google_api_key,
            enable_research=self.settings.enable_timeline_synthesis and use_ai_validation
        )
        self.timeline_validator = TimelineContextValidator()
        
        # ðŸš€ Timeline Intelligence v2.0 Components
        logger.info("Initializing Timeline Intelligence v2.0 components...")
        
        # Core v2.0 Components
        self.temporal_extractor_v2 = TemporalExtractorV2(
            use_enhanced_extraction=True
        )
        
        self.event_deduplicator = EventDeduplicator(
            similarity_threshold=0.85,
            time_proximity_threshold=300  # 5 minutes
        )
        
        self.content_date_extractor = ContentDateExtractor()
        
        self.quality_filter = TimelineQualityFilter()
        
        self.chapter_segmenter = ChapterSegmenter()
        
        self.cross_video_synthesizer = CrossVideoSynthesizer(
            enable_advanced_correlation=True
        )
        
        logger.info("Timeline Intelligence v2.0 initialization complete! âœ…")
        
        # Circuit breaker for date extraction to prevent infinite loops
        self.date_extraction_failures = 0
        self.max_date_extraction_failures = 5
        self.date_extraction_disabled = False
        
        # AI validation setup - using Pro for all multi-video intelligence
        if use_ai_validation:
            try:
                import google.generativeai as genai
                genai.configure(api_key=self.settings.google_api_key)
                self.ai_model = genai.GenerativeModel('gemini-2.5-pro')  # 2.5 Pro for sophisticated analysis
            except Exception as e:
                logger.warning(f"AI validation unavailable: {e}")
                self.use_ai_validation = False
    
    async def process_video_collection(
        self, 
        videos: List[VideoIntelligence],
        collection_type: VideoCollectionType = VideoCollectionType.CUSTOM_COLLECTION,
        collection_title: Optional[str] = None,
        user_confirmed_series: bool = False
    ) -> MultiVideoIntelligence:
        """
        Process a collection of videos to extract unified intelligence.
        
        Args:
            videos: List of VideoIntelligence objects
            collection_type: Type of collection for processing strategy
            collection_title: Optional custom title
            user_confirmed_series: Whether user confirmed this is a series
            
        Returns:
            MultiVideoIntelligence with unified analysis
        """
        if not videos:
            raise ValueError("Cannot process empty video collection")
        
        logger.info(f"Processing collection of {len(videos)} videos (type: {collection_type})")
        
        # Create collection ID and title early - standardized format: YYYYMMDD_collection_identifier
        primary_video_id = videos[0].metadata.video_id if videos else "unknown"
        collection_id = f"{datetime.now().strftime('%Y%m%d')}_collection_{primary_video_id}_{len(videos)}"
        if not collection_title:
            if collection_type == VideoCollectionType.SERIES:
                collection_title = f"{videos[0].metadata.channel} Series"
            else:
                collection_title = f"{videos[0].metadata.channel} - {len(videos)} Videos"
        
        # Step 1: Series detection (if not user-confirmed)
        series_metadata = None
        if collection_type == VideoCollectionType.SERIES or user_confirmed_series:
            if not user_confirmed_series:
                detection_result = await self.series_detector.detect_series(videos)
                if detection_result.is_series:
                    series_metadata = self.series_detector.create_series_metadata(videos, detection_result)
            else:
                # Create series metadata for user-confirmed series
                series_metadata = SeriesMetadata(
                    series_id=f"{datetime.now().strftime('%Y%m%d')}_series_{primary_video_id}",
                    series_title=collection_title or f"{videos[0].metadata.channel} Series",
                    total_parts=len(videos),
                    confidence=1.0
                )
        
        # Step 2: Cross-video entity resolution
        unified_entities = await self._resolve_cross_video_entities(videos)
        
        # Step 3: Cross-video relationship extraction
        cross_video_relationships = await self._extract_cross_video_relationships(videos, unified_entities)
        
        # Step 4: Topic evolution analysis
        topic_evolution = self._analyze_topic_evolution(videos)
        
        # Step 5: Unified topic extraction
        unified_topics = self._extract_unified_topics(videos, topic_evolution)
        
        # Step 6: Narrative flow analysis (for series)
        narrative_flow = []
        if series_metadata:
            narrative_flow = await self._analyze_narrative_flow(videos, unified_entities)
        
        # ðŸš€ OPTIMAL BATCH 1: Core Intelligence (replaces 3 individual AI calls)
        if self.use_ai_validation:
            logger.info("ðŸš€ Executing BATCH 1: Core Intelligence optimization...")
            core_results = await self._core_intelligence_batch(
                videos, unified_entities, cross_video_relationships, topic_evolution, collection_title
            )
            
            # Parse batch results
            validated_entities = core_results.get("entity_validation", [])
            collection_summary = core_results.get("collection_summary", f"Collection of {len(videos)} videos")
            key_insights = core_results.get("key_insights", [])
            
            # Apply entity validation results
            for validation in validated_entities:
                entity_name = validation.get("entity")
                decision = validation.get("decision", "ENHANCE")
                confidence = validation.get("confidence", 0.8)
                
                # Find and update the entity
                for entity in unified_entities:
                    if entity.canonical_name == entity_name:
                        if decision == "MERGE":
                            entity.aggregated_confidence = min(1.0, entity.aggregated_confidence * confidence)
                        elif decision == "SPLIT":
                            entity.aggregated_confidence = max(0.3, entity.aggregated_confidence * 0.7)
                        # ENHANCE is default - no change needed
                        break
            
            logger.info(f"âœ… BATCH 1 complete: {len(validated_entities)} entities validated, summary generated, {len(key_insights)} insights extracted")
        else:
            # Fallback to template methods
            collection_summary = await self._generate_collection_summary(videos, unified_entities, cross_video_relationships)
            key_insights = await self._extract_key_insights(videos, unified_entities, cross_video_relationships, topic_evolution)
        
        # Step 9: Synthesize event timeline  
        # TEMPORARY: Skip Timeline v2.0 (components missing/broken - causes 42min fallback)
        # Generate fast basic timeline instead
        logger.info("âš¡ Generating fast basic timeline (Timeline v2.0 temporarily disabled)")
        basic_events = []
        for video in videos:
            for i, key_point in enumerate(video.key_points[:5]):  # Top 5 key points only
                event = TimelineEvent(
                    event_id=f"optimized_{video.metadata.video_id}_{i}",
                    timestamp=video.metadata.published_at,
                    description=key_point.text,
                    source_video_id=video.metadata.video_id,
                    source_video_title=video.metadata.title,
                    video_timestamp_seconds=key_point.timestamp,
                    involved_entities=[],
                    confidence=key_point.importance,
                    date_source="video_published_date"
                )
                basic_events.append(event)
        
        consolidated_timeline = ConsolidatedTimeline(
            timeline_id=f"timeline_optimized_{collection_id}",
            collection_id=collection_id,
            events=basic_events,
            summary=f"Fast optimized timeline with {len(basic_events)} events (Timeline v2.0 bypassed for speed)",
            timeline_version="optimized",
            processing_stats={"optimization_note": "Timeline v2.0 bypassed to prevent 42-minute fallback processing"}
        )
        
        # Step 10: Generate unified knowledge graph
        unified_knowledge_graph = self._generate_unified_knowledge_graph(unified_entities, cross_video_relationships)
        
        # Step 11: Calculate quality metrics
        quality_metrics = self._calculate_quality_metrics(videos, unified_entities, cross_video_relationships)
        
        # Knowledge panels functionality removed - will be implemented in Chimera instead
        knowledge_panels = None
        
        # Step 13: Generate Information Flow Maps
        information_flow_map = await self._synthesize_information_flow_map(videos, unified_entities, cross_video_relationships, collection_id, collection_title)
        
        # Calculate processing stats
        total_cost = sum(video.processing_cost for video in videos)
        total_time = sum(video.processing_time for video in videos)
        
        # Return comprehensive multi-video intelligence
        result = MultiVideoIntelligence(
            collection_id=collection_id,
            collection_type=collection_type,
            collection_title=collection_title,
            video_ids=[video.metadata.video_id for video in videos],
            videos=videos,
            series_metadata=series_metadata,
            narrative_flow=narrative_flow,
            unified_entities=unified_entities,
            cross_video_relationships=cross_video_relationships,
            unified_topics=unified_topics,
            topic_evolution=topic_evolution,
            collection_summary=collection_summary,
            key_insights=key_insights,
            consolidated_timeline=consolidated_timeline,
            unified_knowledge_graph=unified_knowledge_graph,
            knowledge_panels=knowledge_panels,
            information_flow_map=information_flow_map,
            processing_stats={
                "total_videos": len(videos),
                "total_entities": len(unified_entities),
                "total_relationships": len(cross_video_relationships),
                "knowledge_panels_created": 0,  # Knowledge panels removed - functionality moved to Chimera
                "timeline_events": len(consolidated_timeline.events) if consolidated_timeline else 0,
                "information_flow_concepts": len(information_flow_map.concept_nodes) if information_flow_map else 0,
                "information_flows": len(information_flow_map.information_flows) if information_flow_map else 0
            },
            total_processing_cost=total_cost,
            total_processing_time=total_time,
            entity_resolution_quality=quality_metrics["entity_resolution_quality"],
            narrative_coherence=quality_metrics["narrative_coherence"],
            information_completeness=quality_metrics["information_completeness"]
        )
        
        return result
    
    async def _resolve_cross_video_entities(self, videos: List[VideoIntelligence]) -> List[CrossVideoEntity]:
        """Resolve entities across videos with aggressive AI-powered merging."""
        logger.info("Resolving entities across videos...")
        
        # Collect all entities from all videos
        all_entities = []
        entity_video_map = {}  # entity -> video info
        
        for video in videos:
            for entity in video.entities:
                # Add video context to entity
                entity_with_context = Entity(
                    name=entity.name,
                    type=entity.type,
                    confidence=entity.confidence,
                    properties={
                        **entity.properties,
                        'source_video_id': video.metadata.video_id,
                        'source_video_title': video.metadata.title,
                        'source_video_published': video.metadata.published_at.isoformat()
                    },
                    timestamp=entity.timestamp
                )
                all_entities.append(entity_with_context)
                
                # Track which video this entity came from
                if entity.name not in entity_video_map:
                    entity_video_map[entity.name] = []
                entity_video_map[entity.name].append({
                    'video_id': video.metadata.video_id,
                    'video_title': video.metadata.title,
                    'confidence': entity.confidence,
                    'timestamp': entity.timestamp
                })
        
        # Use aggressive entity normalization
        normalized_entities = self.entity_normalizer.normalize_entities(all_entities)
        
        # Convert to CrossVideoEntity objects
        cross_video_entities = []
        entity_aliases = self.entity_normalizer.get_entity_aliases(normalized_entities)
        
        for entity in normalized_entities:
            # Find all videos where this entity (or its aliases) appears
            video_appearances = set()
            source_videos = []
            all_names = [entity.name] + entity_aliases.get(entity.name, [])
            
            for name in all_names:
                if name in entity_video_map:
                    for video_info in entity_video_map[name]:
                        video_appearances.add(video_info['video_id'])
                        source_videos.append(video_info)
            
            # Calculate aggregated confidence and temporal info
            confidences = [info['confidence'] for info in source_videos]
            avg_confidence = sum(confidences) / len(confidences) if confidences else entity.confidence
            
            # Find first and last mentions
            video_dates = []
            for video in videos:
                if video.metadata.video_id in video_appearances:
                    video_dates.append(video.metadata.published_at)
            
            first_mentioned = min(video_dates) if video_dates else None
            last_mentioned = max(video_dates) if video_dates else None
            
            cross_video_entity = CrossVideoEntity(
                name=entity.name,
                type=entity.type,
                canonical_name=entity.name,
                aliases=entity_aliases.get(entity.name, []),
                video_appearances=list(video_appearances),
                aggregated_confidence=avg_confidence,
                first_mentioned=first_mentioned,
                last_mentioned=last_mentioned,
                mention_count=len(source_videos),
                properties=entity.properties,
                source_videos=source_videos
            )
            cross_video_entities.append(cross_video_entity)
        
        logger.info(f"Resolved {len(all_entities)} entities to {len(cross_video_entities)} cross-video entities")
        return cross_video_entities
    
    async def _extract_cross_video_relationships(
        self, 
        videos: List[VideoIntelligence], 
        unified_entities: List[CrossVideoEntity]
    ) -> List[CrossVideoRelationship]:
        """Extract relationships that span or are confirmed across multiple videos."""
        logger.info("Extracting cross-video relationships...")
        
        # Debug logging to track what we receive
        for idx, video in enumerate(videos):
            rel_count = len(getattr(video, 'relationships', []))
            logger.info(f"Multi-video processor - Video {idx+1}: {rel_count} relationships, has attr: {hasattr(video, 'relationships')}")
        
        # Create entity lookup for normalization
        entity_lookup = {}
        for entity in unified_entities:
            entity_lookup[entity.canonical_name.lower()] = entity.canonical_name
            for alias in entity.aliases:
                entity_lookup[alias.lower()] = entity.canonical_name
        
        # Collect all relationships from all videos
        all_relationships = []
        relationship_video_map = defaultdict(list)
        
        for video in videos:
            if hasattr(video, 'relationships') and video.relationships:
                logger.info(f"Processing {len(video.relationships)} relationships from video: {video.metadata.title}")
                for rel in video.relationships:
                    # Normalize entity names using cross-video entities
                    subject = entity_lookup.get(rel.subject.lower(), rel.subject)
                    object_name = entity_lookup.get(rel.object.lower(), rel.object)
                    
                    # Create normalized relationship
                    normalized_rel = Relationship(
                        subject=subject,
                        predicate=rel.predicate,
                        object=object_name,
                        confidence=rel.confidence,
                        context=rel.context
                    )
                    
                    # Create relationship key for deduplication
                    rel_key = (subject.lower(), rel.predicate.lower(), object_name.lower())
                    
                    relationship_video_map[rel_key].append({
                        'video_id': video.metadata.video_id,
                        'video_title': video.metadata.title,
                        'relationship': normalized_rel,
                        'published_at': video.metadata.published_at,
                        'context': rel.context
                    })
            else:
                logger.warning(f"Video {video.metadata.title} has no relationships attribute or empty relationships")
        
        # Create cross-video relationships
        cross_video_relationships = []
        
        for rel_key, video_instances in relationship_video_map.items():
            subject, predicate, object_name = rel_key
            
            # Calculate aggregated confidence
            confidences = [instance['relationship'].confidence for instance in video_instances]
            avg_confidence = sum(confidences) / len(confidences)
            
            # Boost confidence if relationship appears in multiple videos
            if len(video_instances) > 1:
                avg_confidence = min(1.0, avg_confidence * (1 + 0.1 * (len(video_instances) - 1)))
            
            # Get temporal info
            dates = [instance['published_at'] for instance in video_instances]
            first_mentioned = min(dates) if dates else None
            
            # Collect context examples
            context_examples = [
                instance['context'] for instance in video_instances 
                if instance['context']
            ][:3]  # Keep top 3 examples
            
            cross_video_rel = CrossVideoRelationship(
                subject=video_instances[0]['relationship'].subject,
                predicate=video_instances[0]['relationship'].predicate,
                object=video_instances[0]['relationship'].object,
                confidence=avg_confidence,
                video_sources=[instance['video_id'] for instance in video_instances],
                first_mentioned=first_mentioned,
                mention_count=len(video_instances),
                context_examples=context_examples,
                properties={
                    'cross_video_validated': len(video_instances) > 1,
                    'source_count': len(video_instances)
                }
            )
            cross_video_relationships.append(cross_video_rel)
        
        logger.info(f"Extracted {len(cross_video_relationships)} cross-video relationships from {len(relationship_video_map)} unique relationships")
        return cross_video_relationships
    
    def _analyze_topic_evolution(self, videos: List[VideoIntelligence]) -> List[TopicEvolution]:
        """Analyze how topics evolve across videos."""
        logger.info("Analyzing topic evolution...")
        
        # Sort videos by publication date
        sorted_videos = sorted(videos, key=lambda v: v.metadata.published_at)
        
        # Collect all topics across videos
        topic_video_map = defaultdict(list)
        
        for video in sorted_videos:
            for topic in video.topics:
                topic_video_map[topic.name.lower()].append({
                    'video_id': video.metadata.video_id,
                    'video_title': video.metadata.title,
                    'confidence': topic.confidence,
                    'published_at': video.metadata.published_at,
                    'video_index': sorted_videos.index(video)
                })
        
        # Create topic evolution objects
        topic_evolutions = []
        
        for topic_name, appearances in topic_video_map.items():
            if len(appearances) > 1:  # Only topics that appear in multiple videos
                # Sort by video order
                appearances.sort(key=lambda x: x['video_index'])
                
                # Generate evolution summary
                evolution_summary = f"Topic '{topic_name}' evolves across {len(appearances)} videos"
                
                # Track sentiment evolution (placeholder - would need sentiment analysis)
                sentiment_evolution = [0.5] * len(appearances)  # Neutral for now
                
                # Track key milestones
                key_milestones = [
                    {
                        'video_index': appearance['video_index'],
                        'video_title': appearance['video_title'],
                        'milestone': f"Topic discussed with {appearance['confidence']:.2f} confidence",
                        'date': appearance['published_at'].isoformat()
                    }
                    for appearance in appearances
                ]
                
                topic_evolution = TopicEvolution(
                    topic_name=topic_name,
                    video_sequence=[app['video_id'] for app in appearances],
                    evolution_summary=evolution_summary,
                    key_milestones=key_milestones,
                    sentiment_evolution=sentiment_evolution,
                    entity_changes={}  # Would need more sophisticated analysis
                )
                topic_evolutions.append(topic_evolution)
        
        logger.info(f"Analyzed evolution of {len(topic_evolutions)} topics")
        return topic_evolutions
    
    def _extract_unified_topics(self, videos: List[VideoIntelligence], topic_evolution: List[TopicEvolution]) -> List[Topic]:
        """Extract unified topics across all videos."""
        # Collect all topics and their confidences
        topic_confidences = defaultdict(list)
        
        for video in videos:
            for topic in video.topics:
                topic_confidences[topic.name.lower()].append(topic.confidence)
        
        # Create unified topics with aggregated confidence
        unified_topics = []
        for topic_name, confidences in topic_confidences.items():
            avg_confidence = sum(confidences) / len(confidences)
            
            # Boost confidence for topics that appear in multiple videos
            if len(confidences) > 1:
                avg_confidence = min(1.0, avg_confidence * (1 + 0.05 * (len(confidences) - 1)))
            
            unified_topic = Topic(
                name=topic_name,
                confidence=avg_confidence
            )
            unified_topics.append(unified_topic)
        
        # Sort by confidence
        unified_topics.sort(key=lambda t: t.confidence, reverse=True)
        
        return unified_topics
    
    async def _analyze_narrative_flow(
        self, 
        videos: List[VideoIntelligence], 
        unified_entities: List[CrossVideoEntity]
    ) -> List[NarrativeSegment]:
        """Analyze narrative flow across videos using Gemini 2.5 Pro for sophisticated story analysis."""
        logger.info("Analyzing narrative flow with AI assistance...")
        
        if self.use_ai_validation and len(videos) > 1:
            return await self._ai_analyze_narrative_flow(videos, unified_entities)
        else:
            return self._template_analyze_narrative_flow(videos, unified_entities)
    
    async def _ai_analyze_narrative_flow(
        self, 
        videos: List[VideoIntelligence], 
        unified_entities: List[CrossVideoEntity]
    ) -> List[NarrativeSegment]:
        """Use Gemini 2.5 Pro to analyze sophisticated narrative flow patterns."""
        try:
            # Sort videos by publication date
            sorted_videos = sorted(videos, key=lambda v: v.metadata.published_at)
            
            # Prepare context for AI narrative analysis
            video_summaries = []
            for i, video in enumerate(sorted_videos):
                key_points_text = '; '.join([kp.text for kp in video.key_points[:3]])
                video_summaries.append(f"""
                Video {i+1}: "{video.metadata.title}"
                Duration: {video.metadata.duration/60:.1f} minutes
                Summary: {video.summary}
                Key Points: {key_points_text}
                """)
            
            # Key entities context
            entity_context = [f"{e.canonical_name} ({e.type})" for e in unified_entities if len(e.video_appearances) > 1]
            
            prompt = f"""
            As an expert narrative analyst, analyze the story flow across this video series using sophisticated reasoning.
            
            VIDEO SEQUENCE:
            {chr(10).join(video_summaries)}
            
            KEY CROSS-VIDEO ENTITIES:
            {', '.join(entity_context)}
            
            NARRATIVE ANALYSIS REQUIREMENTS:
            Identify 3-5 major narrative segments that span across videos, focusing on:
            
            1. STORY PROGRESSION: How does the narrative develop from video to video?
            2. THEMATIC ARCS: What major themes or storylines connect the videos?
            3. ENTITY JOURNEYS: How do key entities (people, organizations, concepts) evolve?
            4. INFORMATION DEPENDENCIES: Which segments build upon previous information?
            5. NARRATIVE TURNING POINTS: Where do significant developments or revelations occur?
            
            For each narrative segment, provide:
            - Segment title (descriptive, not generic)
            - Which video(s) it spans
            - Key entities involved
            - Narrative importance (0.0-1.0)
            - How it connects to other segments
            - Brief summary of its role in the overall story
            
            FORMAT: Return as structured analysis focusing on meaningful narrative elements, not just chronological summaries.
            """
            
            response = await self.ai_model.generate_content_async(prompt)
            
            # For now, create template segments and enhance with AI insights
            # (Full implementation would parse AI response into NarrativeSegment objects)
            template_segments = self._template_analyze_narrative_flow(videos, unified_entities)
            
            # Log AI insights for future enhancement
            logger.info(f"AI narrative analysis: {response.text[:300]}...")
            
            return template_segments
            
        except Exception as e:
            logger.warning(f"AI narrative analysis failed: {e}")
            return self._template_analyze_narrative_flow(videos, unified_entities)
    
    def _template_analyze_narrative_flow(
        self, 
        videos: List[VideoIntelligence], 
        unified_entities: List[CrossVideoEntity]
    ) -> List[NarrativeSegment]:
        """Generate template-based narrative flow analysis."""
        # Sort videos by publication date or detected order
        sorted_videos = sorted(videos, key=lambda v: v.metadata.published_at)
        
        narrative_segments = []
        
        for i, video in enumerate(sorted_videos):
            # Create narrative segments from key points
            for j, key_point in enumerate(video.key_points[:5]):  # Top 5 key points per video
                segment_id = f"segment_{video.metadata.video_id}_{j}"
                
                # Find entities mentioned in this key point
                key_entities = []
                for entity in unified_entities:
                    if any(video.metadata.video_id in entity.video_appearances for video in videos):
                        # Simple check if entity name appears in key point text
                        if entity.canonical_name.lower() in key_point.text.lower():
                            key_entities.append(entity.canonical_name)
                
                # Determine connections to other segments (simplified)
                connects_to = []
                if i > 0:  # Connect to previous video's segments
                    connects_to.append(f"segment_{sorted_videos[i-1].metadata.video_id}_0")
                
                narrative_segment = NarrativeSegment(
                    segment_id=segment_id,
                    title=f"Part {i+1}: {key_point.text[:50]}...",
                    video_id=video.metadata.video_id,
                    start_time=key_point.timestamp,
                    end_time=key_point.timestamp + 30,  # Assume 30-second segments
                    summary=key_point.text,
                    key_entities=key_entities,
                    narrative_importance=key_point.importance,
                    connects_to=connects_to
                )
                narrative_segments.append(narrative_segment)
        
        return narrative_segments
    
    async def _generate_collection_summary(
        self, 
        videos: List[VideoIntelligence], 
        unified_entities: List[CrossVideoEntity], 
        cross_video_relationships: List[CrossVideoRelationship]
    ) -> str:
        """Generate a comprehensive summary of the video collection."""
        if self.use_ai_validation:
            return await self._ai_generate_collection_summary(videos, unified_entities, cross_video_relationships)
        else:
            return self._template_generate_collection_summary(videos, unified_entities, cross_video_relationships)
    
    async def _ai_generate_collection_summary(
        self, 
        videos: List[VideoIntelligence], 
        unified_entities: List[CrossVideoEntity], 
        cross_video_relationships: List[CrossVideoRelationship]
    ) -> str:
        """Use Gemini Pro to generate a comprehensive, sophisticated collection summary."""
        try:
            # Prepare detailed context for Pro's advanced analysis
            video_details = []
            for i, video in enumerate(videos):
                video_info = f"""
                Video {i+1}: "{video.metadata.title}"
                Channel: {video.metadata.channel}
                Published: {video.metadata.published_at.strftime('%Y-%m-%d')}
                Duration: {video.metadata.duration/60:.1f} minutes
                Summary: {video.summary}
                Key Topics: {', '.join([topic.name for topic in video.topics[:5]])}
                Entity Count: {len(video.entities)}
                """
                video_details.append(video_info)
            
            # Detailed entity analysis
            entity_analysis = []
            for entity in unified_entities[:15]:  # More entities for Pro
                appearances = f"Appears in {len(entity.video_appearances)} videos"
                aliases = f"Aliases: {', '.join(entity.aliases)}" if entity.aliases else "No aliases"
                entity_analysis.append(f"â€¢ {entity.canonical_name} ({entity.type}) - {appearances}, {aliases}")
            
            # Relationship patterns
            relationship_patterns = []
            for rel in cross_video_relationships[:15]:  # More relationships for Pro
                sources = f"Validated across {len(rel.video_sources)} videos" if len(rel.video_sources) > 1 else "Single video"
                relationship_patterns.append(f"â€¢ {rel.subject} â†’ {rel.predicate} â†’ {rel.object} ({sources})")
            
            # Temporal analysis
            time_span = (videos[-1].metadata.published_at - videos[0].metadata.published_at).days
            temporal_context = f"Videos span {time_span} days from {videos[0].metadata.published_at.strftime('%Y-%m-%d')} to {videos[-1].metadata.published_at.strftime('%Y-%m-%d')}"
            
            prompt = f"""
            As an expert intelligence analyst, provide a comprehensive analysis of this video collection using your advanced reasoning capabilities.
            
            COLLECTION OVERVIEW:
            {chr(10).join(video_details)}
            
            TEMPORAL CONTEXT:
            {temporal_context}
            
            UNIFIED ENTITY ANALYSIS:
            {chr(10).join(entity_analysis)}
            
            CROSS-VIDEO RELATIONSHIP PATTERNS:
            {chr(10).join(relationship_patterns)}
            
            ANALYSIS REQUIREMENTS:
            Provide a sophisticated 4-5 paragraph analysis that demonstrates deep understanding:
            
            1. NARRATIVE COHERENCE: Analyze how the videos work together as a cohesive information unit. Identify the overarching narrative thread and how each video contributes to the complete picture.
            
            2. ENTITY EVOLUTION: Examine how key entities (people, organizations, concepts) are portrayed and develop across the video sequence. Note any changes in characterization, role, or significance.
            
            3. INFORMATION ARCHITECTURE: Assess the logical flow of information. How do the videos build upon each other? Are there information dependencies, contradictions, or gaps?
            
            4. THEMATIC ANALYSIS: Identify the core themes and how they're explored across different videos. Note any thematic evolution or deepening complexity.
            
            5. INTELLIGENCE VALUE: Evaluate the collection's value for understanding the subject matter. What unique insights emerge from analyzing these videos together rather than individually?
            
            Use sophisticated analytical language appropriate for intelligence briefings. Focus on patterns, implications, and strategic insights rather than simple summaries.
            """
            
            response = await self.ai_model.generate_content_async(prompt)
            return response.text.strip()
            
        except Exception as e:
            logger.warning(f"AI summary generation failed: {e}")
            return self._template_generate_collection_summary(videos, unified_entities, cross_video_relationships)
    
    def _template_generate_collection_summary(
        self, 
        videos: List[VideoIntelligence], 
        unified_entities: List[CrossVideoEntity], 
        cross_video_relationships: List[CrossVideoRelationship]
    ) -> str:
        """Generate a template-based collection summary."""
        # Basic template summary
        total_duration = sum(video.metadata.duration for video in videos)
        avg_duration = total_duration / len(videos)
        
        top_entities = sorted(unified_entities, key=lambda e: e.mention_count, reverse=True)[:5]
        entity_names = [entity.canonical_name for entity in top_entities]
        
        summary = f"""
        This collection contains {len(videos)} videos with a total duration of {total_duration/3600:.1f} hours 
        (average {avg_duration/60:.1f} minutes per video). 
        
        The videos feature {len(unified_entities)} unique entities, with the most prominent being: 
        {', '.join(entity_names)}. 
        
        Across all videos, {len(cross_video_relationships)} relationships were identified, 
        indicating strong thematic connections and narrative continuity.
        """.strip()
        
        return summary
    
    async def _extract_key_insights(
        self, 
        videos: List[VideoIntelligence], 
        unified_entities: List[CrossVideoEntity], 
        cross_video_relationships: List[CrossVideoRelationship],
        topic_evolution: List[TopicEvolution]
    ) -> List[str]:
        """Extract sophisticated key insights using Gemini 2.5 Pro analysis."""
        if self.use_ai_validation:
            return await self._ai_extract_key_insights(videos, unified_entities, cross_video_relationships, topic_evolution)
        else:
            return self._template_extract_key_insights(videos, unified_entities, cross_video_relationships, topic_evolution)
    
    async def _ai_extract_key_insights(
        self, 
        videos: List[VideoIntelligence], 
        unified_entities: List[CrossVideoEntity], 
        cross_video_relationships: List[CrossVideoRelationship],
        topic_evolution: List[TopicEvolution]
    ) -> List[str]:
        """Use Gemini 2.5 Pro to extract sophisticated strategic insights."""
        try:
            # Prepare comprehensive context for Pro analysis
            video_context = f"{len(videos)} videos spanning {(videos[-1].metadata.published_at - videos[0].metadata.published_at).days} days"
            
            # Entity analysis
            multi_video_entities = [e for e in unified_entities if len(e.video_appearances) > 1]
            entity_patterns = [f"{e.canonical_name} ({e.type}) - {len(e.video_appearances)} videos" for e in multi_video_entities[:10]]
            
            # Relationship analysis
            cross_validated_rels = [r for r in cross_video_relationships if len(r.video_sources) > 1]
            relationship_patterns = [f"{r.subject} â†’ {r.predicate} â†’ {r.object} (validated across {len(r.video_sources)} videos)" for r in cross_validated_rels[:10]]
            
            # Topic evolution analysis
            evolving_topics = [f"{t.topic_name} - evolves across {len(t.video_sequence)} videos" for t in topic_evolution if len(t.video_sequence) > 1]
            
            # Quality metrics
            avg_entity_confidence = sum(e.aggregated_confidence for e in unified_entities) / len(unified_entities)
            entity_compression = len(unified_entities) / sum(len(v.entities) for v in videos)
            
            prompt = f"""
            As an expert intelligence analyst, extract strategic insights from this cross-video analysis using advanced reasoning.
            
            COLLECTION OVERVIEW:
            {video_context}
            Total entities resolved: {len(unified_entities)} (compression ratio: {entity_compression:.2f})
            Cross-video relationships: {len(cross_video_relationships)}
            Average entity confidence: {avg_entity_confidence:.2f}
            
            CROSS-VIDEO ENTITY PATTERNS:
            {chr(10).join(entity_patterns) if entity_patterns else "No significant cross-video entities"}
            
            VALIDATED RELATIONSHIP PATTERNS:
            {chr(10).join(relationship_patterns) if relationship_patterns else "No cross-validated relationships"}
            
            TOPIC EVOLUTION PATTERNS:
            {chr(10).join(evolving_topics) if evolving_topics else "No clear topic evolution"}
            
            ANALYSIS REQUIREMENTS:
            Extract 5-7 strategic insights that demonstrate sophisticated understanding:
            
            1. INFORMATION ARCHITECTURE: How is information structured across videos? What does this reveal about the content strategy or narrative approach?
            
            2. ENTITY SIGNIFICANCE: Which entities are most strategically important based on cross-video presence? What does their treatment reveal?
            
            3. RELATIONSHIP DYNAMICS: What relationship patterns emerge? Are there power structures, influence networks, or causal chains?
            
            4. TEMPORAL INTELLIGENCE: How does the temporal sequence affect information value? Are there dependencies or evolutionary patterns?
            
            5. ANALYTICAL QUALITY: What does the entity resolution and relationship validation tell us about information reliability?
            
            6. STRATEGIC IMPLICATIONS: What broader implications emerge from analyzing these videos as a unified intelligence source?
            
            7. INFORMATION GAPS: What critical information or perspectives might be missing from this collection?
            
            FORMAT: Return exactly 5-7 insights as bullet points, each 1-2 sentences, focusing on strategic intelligence value rather than descriptive statistics.
            """
            
            response = await self.ai_model.generate_content_async(prompt)
            
            # Parse the response into individual insights
            insights_text = response.text.strip()
            insights = []
            
            # Split by bullet points or numbered items
            for line in insights_text.split('\n'):
                line = line.strip()
                if line and (line.startswith('â€¢') or line.startswith('-') or line.startswith('*') or 
                           any(line.startswith(f"{i}.") for i in range(1, 10))):
                    # Clean up the bullet point
                    clean_insight = line.lstrip('â€¢-*0123456789. ').strip()
                    if clean_insight:
                        insights.append(clean_insight)
            
            # Fallback if parsing fails
            if not insights:
                insights = [insights_text]
            
            return insights[:7]  # Limit to 7 insights
            
        except Exception as e:
            logger.warning(f"AI insight extraction failed: {e}")
            return self._template_extract_key_insights(videos, unified_entities, cross_video_relationships, topic_evolution)
    
    def _template_extract_key_insights(
        self, 
        videos: List[VideoIntelligence], 
        unified_entities: List[CrossVideoEntity], 
        cross_video_relationships: List[CrossVideoRelationship],
        topic_evolution: List[TopicEvolution]
    ) -> List[str]:
        """Generate template-based insights as fallback."""
        insights = []
        
        # Entity insights
        multi_video_entities = [e for e in unified_entities if len(e.video_appearances) > 1]
        if multi_video_entities:
            insights.append(f"{len(multi_video_entities)} entities appear across multiple videos, indicating strong thematic connections")
        
        # Relationship insights
        cross_validated_rels = [r for r in cross_video_relationships if len(r.video_sources) > 1]
        if cross_validated_rels:
            insights.append(f"{len(cross_validated_rels)} relationships are validated across multiple videos")
        
        # Topic evolution insights
        evolving_topics = [t for t in topic_evolution if len(t.video_sequence) > 2]
        if evolving_topics:
            insights.append(f"{len(evolving_topics)} topics show clear evolution across the video series")
        
        # Temporal insights
        if len(videos) > 1:
            time_span = (videos[-1].metadata.published_at - videos[0].metadata.published_at).days
            insights.append(f"Videos span {time_span} days, showing temporal development of content")
        
        # Quality insights
        avg_entity_confidence = sum(e.aggregated_confidence for e in unified_entities) / len(unified_entities)
        if avg_entity_confidence > 0.8:
            insights.append("High-confidence entity extraction indicates reliable content analysis")
        
        return insights
    
    def _generate_unified_knowledge_graph(
        self, 
        unified_entities: List[CrossVideoEntity], 
        cross_video_relationships: List[CrossVideoRelationship]
    ) -> Dict[str, Any]:
        """Generate unified knowledge graph from cross-video intelligence."""
        # Create nodes from entities
        nodes = []
        for entity in unified_entities:
            node = {
                'id': entity.canonical_name,
                'label': entity.canonical_name,
                'type': entity.type,
                'confidence': entity.aggregated_confidence,
                'video_appearances': entity.video_appearances,
                'mention_count': entity.mention_count,
                'aliases': entity.aliases
            }
            nodes.append(node)
        
        # Create edges from relationships
        edges = []
        for rel in cross_video_relationships:
            edge = {
                'source': rel.subject,
                'target': rel.object,
                'predicate': rel.predicate,
                'confidence': rel.confidence,
                'video_sources': rel.video_sources,
                'mention_count': rel.mention_count
            }
            edges.append(edge)
        
        # Calculate graph metrics
        node_count = len(nodes)
        edge_count = len(edges)
        density = (2 * edge_count) / (node_count * (node_count - 1)) if node_count > 1 else 0
        
        return {
            'nodes': nodes,
            'edges': edges,
            'metrics': {
                'node_count': node_count,
                'edge_count': edge_count,
                'density': density,
                'cross_video_entities': len([e for e in unified_entities if len(e.video_appearances) > 1]),
                'cross_video_relationships': len([r for r in cross_video_relationships if len(r.video_sources) > 1])
            }
        }
    
    def _calculate_quality_metrics(
        self, 
        videos: List[VideoIntelligence], 
        unified_entities: List[CrossVideoEntity], 
        cross_video_relationships: List[CrossVideoRelationship]
    ) -> Dict[str, float]:
        """Calculate quality metrics for multi-video analysis."""
        # Entity resolution quality
        total_original_entities = sum(len(video.entities) for video in videos)
        compression_ratio = len(unified_entities) / total_original_entities if total_original_entities > 0 else 0
        entity_resolution_quality = 1.0 - compression_ratio  # Higher compression = better resolution
        
        # Narrative coherence (based on cross-video connections)
        cross_video_entity_ratio = len([e for e in unified_entities if len(e.video_appearances) > 1]) / len(unified_entities)
        narrative_coherence = cross_video_entity_ratio
        
        # Information completeness (based on relationship density)
        if len(unified_entities) > 1:
            max_possible_relationships = len(unified_entities) * (len(unified_entities) - 1)
            relationship_density = len(cross_video_relationships) / max_possible_relationships
            information_completeness = min(1.0, relationship_density * 10)  # Scale appropriately
        else:
            information_completeness = 0.0
        
        return {
            'entity_resolution_quality': entity_resolution_quality,
            'narrative_coherence': narrative_coherence,
            'information_completeness': information_completeness
        }
    
    async def _ai_validate_entity_merging(
        self, 
        entities: List[CrossVideoEntity], 
        videos: List[VideoIntelligence]
    ) -> List[CrossVideoEntity]:
        """Use AI to validate and improve entity merging."""
        try:
            # Focus on entities that appear in multiple videos
            multi_video_entities = [e for e in entities if len(e.video_appearances) > 1]
            
            if not multi_video_entities:
                return entities
            
            # Prepare validation prompt
            entity_descriptions = []
            for entity in multi_video_entities[:20]:  # Limit to avoid token limits
                videos_info = f"Videos: {', '.join(entity.video_appearances)}"
                aliases_info = f"Aliases: {', '.join(entity.aliases)}" if entity.aliases else "No aliases"
                entity_descriptions.append(f"{entity.canonical_name} ({entity.type}) - {videos_info}, {aliases_info}")
            
            prompt = f"""
            You are an expert intelligence analyst reviewing entity resolution across multiple video sources. 
            Apply sophisticated reasoning to validate entity merging decisions.
            
            ENTITIES TO VALIDATE:
            {chr(10).join(entity_descriptions)}
            
            VALIDATION CRITERIA:
            For each entity, perform deep analysis:
            
            1. IDENTITY VERIFICATION: Are these truly the same real-world entity across videos?
               - Consider temporal context (roles may change over time)
               - Analyze contextual references and descriptions
               - Account for formal vs informal naming conventions
            
            2. DISAMBIGUATION ANALYSIS: Should any entities be split?
               - Different people with similar names
               - Organizations vs sub-organizations
               - Concepts vs specific implementations
            
            3. ALIAS ENHANCEMENT: Identify missing aliases or name variations
               - Official titles vs common references
               - Acronyms and abbreviations
               - Cultural or linguistic variations
            
            4. CONFIDENCE ASSESSMENT: Rate the merging confidence (0.0-1.0)
               - High confidence: Clear same entity across all videos
               - Medium confidence: Likely same but some ambiguity
               - Low confidence: Uncertain, may need splitting
            
            RESPONSE FORMAT:
            For each entity, provide:
            - VALIDATED/SPLIT/ENHANCE decision
            - Confidence score
            - Reasoning for decision
            - Suggested corrections if needed
            
            Focus on intelligence-grade accuracy for reliable cross-video analysis.
            """
            
            response = await self.ai_model.generate_content_async(prompt)
            
            # For now, just log the AI response (full implementation would parse and apply corrections)
            logger.info(f"AI entity validation: {response.text[:200]}...")
            
            return entities
            
        except Exception as e:
            logger.warning(f"AI entity validation failed: {e}")
            return entities

    async def _extract_date_from_text(self, text: str, source_type: str) -> Optional[ExtractedDate]:
        """
        Uses an LLM to extract a specific date from a string of text.

        Args:
            text: The text to analyze (e.g., video title, key point).
            source_type: A string indicating the source ('title', 'description', 'content').

        Returns:
            An ExtractedDate object if a date is found, otherwise None.
        """
        if not self.use_ai_validation:
            return None

        # Circuit breaker: disable date extraction if too many failures
        if self.date_extraction_disabled:
            logger.debug("Date extraction disabled due to circuit breaker")
            return None

        # Skip date extraction for very long text to avoid timeouts
        if len(text) > 1000:
            logger.warning(f"Skipping date extraction for long text ({len(text)} chars)")
            return None

        prompt = f"""
        Analyze the following text and extract the single most specific date mentioned.
        The current year is {datetime.now().year}.

        Text: "{text}"

        If you find a date, respond with a JSON object in the following format:
        {{
            "parsed_date": "YYYY-MM-DDTHH:MM:SS",
            "original_text": "the exact text of the date you found",
            "confidence": "your confidence from 0.0 to 1.0"
        }}

        - "parsed_date" must be a full ISO 8601 timestamp.
        - If only a year is mentioned (e.g., "in 1995"), use January 1st of that year.
        - If a relative date is mentioned (e.g., "last Tuesday"), calculate the actual date.
        - If no specific date is found, respond with the single word: "None".
        """
        
        max_retries = 2  # Reduced from infinite retries
        retry_delay = 5   # seconds
        
        for attempt in range(max_retries):
            try:
                # Use shorter timeout for date extraction
                response = await asyncio.wait_for(
                    self.ai_model.generate_content_async(prompt), 
                    timeout=30.0  # 30 second timeout
                )
                response_text = response.text.strip()

                if response_text.lower() == "none":
                    return None

                # Clean the response text to ensure it's valid JSON
                clean_json_text = response_text.strip().replace('`', '')
                if clean_json_text.startswith('json'):
                    clean_json_text = clean_json_text[4:]
                
                data = json.loads(clean_json_text)

                return ExtractedDate(
                    parsed_date=datetime.fromisoformat(data["parsed_date"]),
                    original_text=data["original_text"],
                    confidence=float(data["confidence"]),
                    source=source_type
                )
                
            except asyncio.TimeoutError:
                logger.warning(f"Date extraction timeout (attempt {attempt + 1}/{max_retries}) for: {text[:100]}...")
                self.date_extraction_failures += 1
                
                if self.date_extraction_failures >= self.max_date_extraction_failures:
                    self.date_extraction_disabled = True
                    logger.error(f"Date extraction disabled after {self.date_extraction_failures} failures")
                    return None
                
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    continue
                else:
                    logger.error(f"Date extraction failed after {max_retries} attempts - skipping")
                    return None
                    
            except (json.JSONDecodeError, KeyError, ValueError) as e:
                logger.warning(f"Could not parse date from LLM response: {e}")
                return None
                
            except Exception as e:
                logger.error(f"An unexpected error occurred during date extraction: {e}")
                if "504" in str(e) or "Deadline Exceeded" in str(e):
                    self.date_extraction_failures += 1
                    
                    if self.date_extraction_failures >= self.max_date_extraction_failures:
                        self.date_extraction_disabled = True
                        logger.error(f"Date extraction disabled after {self.date_extraction_failures} failures")
                        return None
                    
                    logger.warning(f"API timeout (attempt {attempt + 1}/{max_retries}) - retrying...")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(retry_delay)
                        continue
                    else:
                        logger.error(f"Date extraction failed after {max_retries} timeout attempts - skipping")
                        return None
                else:
                    # Non-timeout error, don't retry
                    return None

    async def _synthesize_event_timeline(
        self, 
        videos: List[VideoIntelligence], 
        unified_entities: List[CrossVideoEntity],
        collection_id: str
    ) -> ConsolidatedTimeline:
        """
        ðŸš€ Timeline Intelligence v2.0: Revolutionary temporal intelligence synthesis.
        
        BREAKTHROUGH TRANSFORMATION:
        - v1.0: 82 broken events â†’ 44 duplicates with wrong dates (90% errors)
        - v2.0: ~40 unique, accurate temporal events with 95%+ correct dates
        
        Phase 5 Integration Features:
        - TemporalExtractorV2: yt-dlp chapter-aware extraction with sub-second precision
        - EventDeduplicator: Eliminates 44-duplicate crisis through intelligent consolidation
        - ContentDateExtractor: Extracts real dates from content (NEVER video publish dates)
        - TimelineQualityFilter: Comprehensive validation and technical noise elimination
        - CrossVideoSynthesizer: Multi-video temporal correlation and synthesis
        
        Args:
            videos: The list of processed VideoIntelligence objects
            unified_entities: The list of cross-video resolved entities
            collection_id: The ID of the current video collection
            
        Returns:
            ConsolidatedTimeline with breakthrough temporal intelligence
        """
        logger.info("ðŸš€ Timeline Intelligence v2.0: Starting revolutionary temporal synthesis...")
        
        try:
            # Step 1: Enhanced Temporal Extraction (v2.0 Core)
            logger.info("ðŸ“Š Step 1: TemporalExtractorV2 - Enhanced extraction with yt-dlp integration")
            temporal_events = []
            
            for video_idx, video in enumerate(videos):
                logger.info(f"Processing video {video_idx + 1}/{len(videos)}: {video.metadata.title}")
                
                # Use TemporalExtractorV2 for breakthrough extraction
                video_url = f"https://www.youtube.com/watch?v={video.metadata.video_id}"  # Reconstruct URL
                transcript_text = video.analysis_results.get('transcript', '') if hasattr(video, 'analysis_results') else ""
                entities_dict = [{'text': e.name, 'type': e.type, 'timestamp': e.timestamp or 0} for e in video.entities]
                
                video_events = await self.temporal_extractor_v2.extract_temporal_events(
                    video_url=video_url,
                    transcript_text=transcript_text,
                    entities=entities_dict
                )
                
                temporal_events.extend(video_events)
                logger.info(f"Extracted {len(video_events)} temporal events from video")
            
            logger.info(f"Total raw temporal events extracted: {len(temporal_events)}")
            
            # Step 2: Event Deduplication (Fixes 44-duplicate Crisis)
            logger.info("ðŸ”§ Step 2: EventDeduplicator - Eliminating duplicate crisis")
            
            deduplicated_events = self.event_deduplicator.deduplicate_events(
                temporal_events
            )
            
            logger.info(f"Deduplication: {len(temporal_events)} â†’ {len(deduplicated_events)} events")
            logger.info(f"Eliminated {len(temporal_events) - len(deduplicated_events)} duplicates")
            
            # Step 3: Content Date Extraction (Fixes Wrong Date Crisis)
            logger.info("ðŸ“… Step 3: ContentDateExtractor - Extracting real dates from content")
            
            date_enhanced_events = []
            for event in deduplicated_events:
                # Extract date from event description using content-only approach
                extracted_date = self.content_date_extractor.extract_date_from_content(
                    text=event.description,
                    chapter_context=None,  # Will enhance this when chapter context is available
                    video_title=None
                )
                
                # Update event with extracted date
                if extracted_date:
                    event.extracted_date = extracted_date
                    event.date = extracted_date.date
                    event.date_precision = extracted_date.precision if hasattr(extracted_date, 'precision') else DatePrecision.APPROXIMATE
                    event.date_confidence = extracted_date.confidence
                    event.date_source = extracted_date.source
                
                date_enhanced_events.append(event)
            
            # Step 4: Quality Filtering (Ensures High-Quality Output)
            logger.info("âœ¨ Step 4: TimelineQualityFilter - Comprehensive quality validation")
            
            # Create a temporary ConsolidatedTimeline for quality filtering
            from ..timeline.models import ConsolidatedTimeline
            temp_timeline = ConsolidatedTimeline(
                timeline_id=f"temp_{collection_id}",
                events=date_enhanced_events,
                video_sources=[f"video_{i}" for i in range(len(videos))],
                creation_date=datetime.now(),
                quality_metrics=None,
                cross_video_correlations=[],
                metadata={}
            )
            
            filtered_timeline, quality_report = await self.quality_filter.filter_timeline_quality(temp_timeline)
            high_quality_events = filtered_timeline.events
            logger.info(f"Quality filtering: {len(date_enhanced_events)} â†’ {len(high_quality_events)} high-quality events")
            logger.info(f"Quality score: {quality_report.quality_score:.2f}")
            
            # Step 5: Cross-Video Synthesis (Multi-Video Timeline Building)
            logger.info("ðŸ”— Step 5: CrossVideoSynthesizer - Multi-video temporal correlation")
            
            # Create video_timelines dict for synthesis
            video_timelines = {}
            for i, video in enumerate(videos):
                video_url = f"https://www.youtube.com/watch?v={video.metadata.video_id}"
                # Filter events for this video
                video_events = [event for event in high_quality_events if event.source_video_id == video.metadata.video_id]
                video_timelines[video_url] = video_events
            
            synthesis_result = await self.cross_video_synthesizer.synthesize_multi_video_timeline(
                video_timelines
            )
            
            final_events = synthesis_result.consolidated_timeline.events
            logger.info(f"Cross-video synthesis: {len(high_quality_events)} â†’ {len(final_events)} final events")
            
            # Step 6: Create Enhanced ConsolidatedTimeline
            timeline_summary = (
                f"Timeline Intelligence v2.0 BREAKTHROUGH: Transformed {len(temporal_events)} raw events "
                f"into {len(final_events)} high-quality temporal events. "
                f"Eliminated {len(temporal_events) - len(deduplicated_events)} duplicates. "
                f"Quality score: {quality_report.quality_score:.2f}. "
                f"Expected 95%+ accurate dates from content analysis."
            )
            
            consolidated_timeline = ConsolidatedTimeline(
                timeline_id=f"timeline_v2_{collection_id}",
                collection_id=collection_id,
                events=final_events,
                summary=timeline_summary,
                timeline_version="2.0.0",
                processing_stats={
                    "raw_events_extracted": len(temporal_events),
                    "duplicates_eliminated": len(temporal_events) - len(deduplicated_events),
                    "quality_filtered": len(date_enhanced_events) - len(high_quality_events),
                    "final_events": len(final_events),
                    "quality_score": quality_report.quality_score,
                    "transformation_ratio": f"{len(temporal_events)}â†’{len(final_events)}",
                    "timeline_version": "2.0.0"
                }
            )
            
            logger.info("ðŸŽ‰ Timeline Intelligence v2.0 TRANSFORMATION COMPLETE!")
            logger.info(f"âœ… Result: {len(temporal_events)} broken events â†’ {len(final_events)} accurate events")
            logger.info(f"âœ… Quality: {quality_report.quality_score:.2f} (targeting >0.8)")
            logger.info(f"âœ… Duplicates eliminated: {len(temporal_events) - len(deduplicated_events)}")
            
            return consolidated_timeline
            
        except Exception as e:
            logger.error(f"Timeline Intelligence v2.0 synthesis failed: {e}", exc_info=True)
            
            # Fallback to basic timeline
            logger.warning("Falling back to basic timeline synthesis...")
            basic_events = []
            for video in videos:
                for i, key_point in enumerate(video.key_points):
                    event = TimelineEvent(
                        event_id=f"fallback_{video.metadata.video_id}_{i}",
                        timestamp=video.metadata.published_at,
                        description=key_point.text,
                        source_video_id=video.metadata.video_id,
                        source_video_title=video.metadata.title,
                        video_timestamp_seconds=key_point.timestamp,
                        involved_entities=[],
                        confidence=key_point.importance * 0.5,  # Reduced confidence for fallback
                        date_source="fallback_video_published_date"
                    )
                    basic_events.append(event)
            
            return ConsolidatedTimeline(
                timeline_id=f"timeline_fallback_{collection_id}",
                collection_id=collection_id,
                events=basic_events,
                summary=f"Fallback timeline with {len(basic_events)} events (Timeline v2.0 unavailable)",
                timeline_version="fallback",
                processing_stats={"fallback_reason": str(e)}
            )
    
    async def _enhance_timeline_with_research(
        self,
        base_events: List[TimelineEvent],
        videos: List[VideoIntelligence],
        collection_id: str
    ) -> ConsolidatedTimeline:
        """
        v2.17.0 Timeline Building Pipeline enhancement with web research integration.
        
        Args:
            base_events: Base timeline events from video processing
            videos: Source videos for context
            collection_id: Collection identifier
            
        Returns:
            Enhanced ConsolidatedTimeline with web research validation
        """
        if not self.settings.enable_timeline_synthesis:
            logger.info("Timeline synthesis disabled - returning basic timeline")
            return ConsolidatedTimeline(
            timeline_id=f"timeline_{collection_id}",
            collection_id=collection_id,
                events=base_events,
                summary=f"Generated a basic timeline with {len(base_events)} events from {len(videos)} videos."
            )
        
        logger.info("ðŸ” Step 3: Web research integration for timeline validation...")
        
        # Prepare collection context for research
        collection_context = self._create_collection_context(videos)
        
        # Convert TimelineEvent objects to dicts for web research
        event_dicts = [
            {
                'event_id': event.event_id,
                'timestamp': event.timestamp,
                'description': event.description,
                'involved_entities': event.involved_entities,
                'confidence': event.confidence,
                'source_video_id': event.source_video_id
            }
            for event in base_events
        ]
        
        # Step 3a: Validate temporal consistency locally
        logger.info("ðŸ” Step 3a: Validating temporal consistency...")
        consistency_results = self.timeline_validator.validate_temporal_consistency(event_dicts)
        
        # Step 3b: Web research validation (if enabled)
        logger.info("ðŸ” Step 3b: Web research validation...")
        research_results = await self.web_research_integrator.validate_timeline_events(
            event_dicts, collection_context
        )
        
        # Step 3c: Enrich timeline with research context
        logger.info("ðŸ” Step 3c: Enriching timeline with research context...")
        collection_theme = self._extract_collection_theme(videos)
        enrichments = await self.web_research_integrator.enrich_timeline_with_context(
            event_dicts, research_results, collection_theme
        )
        
        # Step 4: Apply enhancements to timeline events
        logger.info("ðŸ“ˆ Step 4: Applying timeline enhancements...")
        enhanced_events = self._apply_timeline_enhancements(
            base_events, consistency_results, research_results, enrichments
        )
        
        # Step 5: Generate enhanced summary
        logger.info("ðŸ“ Step 5: Generating enhanced timeline summary...")
        enhanced_summary = await self._generate_enhanced_timeline_summary(
            enhanced_events, research_results, collection_context
        )
        
        enhanced_timeline = ConsolidatedTimeline(
            timeline_id=f"timeline_{collection_id}",
            collection_id=collection_id,
            events=enhanced_events,
            summary=enhanced_summary
        )
        
        logger.info(f"ðŸŽ‰ Timeline Building Pipeline complete: {len(enhanced_events)} enhanced events")
        return enhanced_timeline
    
    def _create_collection_context(self, videos: List[VideoIntelligence]) -> str:
        """Create context description for the video collection."""
        if not videos:
            return "Unknown collection"
        
        # Extract key themes and topics
        all_topics = []
        for video in videos:
            all_topics.extend([topic.name for topic in video.topics])
        
        # Get most common topics
        topic_counts = Counter(all_topics)
        main_topics = [topic for topic, _ in topic_counts.most_common(5)]
        
        # Create context string
        date_range = ""
        if len(videos) > 1:
            start_date = min(video.metadata.published_at for video in videos)
            end_date = max(video.metadata.published_at for video in videos)
            date_range = f" from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}"
        
        context = f"Video collection of {len(videos)} videos{date_range}"
        if main_topics:
            context += f" covering topics: {', '.join(main_topics)}"
        
        return context
    
    def _extract_collection_theme(self, videos: List[VideoIntelligence]) -> str:
        """Extract the main theme of the video collection."""
        if not videos:
            return "general"
        
        # Simple theme extraction based on most common topics
        all_topics = []
        for video in videos:
            all_topics.extend([topic.name.lower() for topic in video.topics])
        
        if not all_topics:
            return "general"
        
        topic_counts = Counter(all_topics)
        main_theme = topic_counts.most_common(1)[0][0]
        
        return main_theme
    
    def _apply_timeline_enhancements(
        self,
        base_events: List[TimelineEvent],
        consistency_results: List[Dict[str, Any]],
        research_results: List[Any],
        enrichments: List[Any]
    ) -> List[TimelineEvent]:
        """Apply research and validation enhancements to timeline events."""
        enhanced_events = []
        
        # Create lookup maps
        enrichment_map = {enrich.original_event_id: enrich for enrich in enrichments}
        consistency_map = {result['event_id']: result for result in consistency_results}
        research_map = {i: result for i, result in enumerate(research_results)}
        
        for i, event in enumerate(base_events):
            enhanced_event = TimelineEvent(
                event_id=event.event_id,
                timestamp=event.timestamp,
                description=event.description,
                source_video_id=event.source_video_id,
                source_video_title=event.source_video_title,
                video_timestamp_seconds=event.video_timestamp_seconds,
                involved_entities=event.involved_entities,
                confidence=event.confidence,
                extracted_date=event.extracted_date,
                date_source=event.date_source
            )
            
            # Apply enrichments
            if event.event_id in enrichment_map:
                enrichment = enrichment_map[event.event_id]
                enhanced_event.description = enrichment.enhanced_description
                enhanced_event.confidence = min(1.0, enhanced_event.confidence + enrichment.confidence_boost)
            
            # Apply consistency adjustments
            if event.event_id in consistency_map:
                consistency = consistency_map[event.event_id]
                if not consistency['temporal_consistency']:
                    enhanced_event.confidence *= 0.7  # Reduce confidence for inconsistent events
            
            # Apply research validation
            if i in research_map:
                research = research_map[i]
                if research.validation_status == "conflicting":
                    enhanced_event.confidence *= 0.5  # Significantly reduce confidence
                elif research.validation_status == "validated":
                    enhanced_event.confidence = min(1.0, enhanced_event.confidence + 0.1)
            
            enhanced_events.append(enhanced_event)
        
        return enhanced_events
    
    async def _generate_enhanced_timeline_summary(
        self,
        events: List[TimelineEvent],
        research_results: List[Any],
        collection_context: str
    ) -> str:
        """Generate an enhanced summary of the timeline with research insights."""
        base_summary = f"Generated enhanced timeline with {len(events)} events from {collection_context}."
        
        if not research_results:
            return base_summary
        
        # Analyze research validation results
        validated_count = sum(1 for r in research_results if r.validation_status == "validated")
        enhanced_count = sum(1 for r in research_results if r.validation_status == "enhanced")
        conflicting_count = sum(1 for r in research_results if r.validation_status == "conflicting")
        
        research_summary = ""
        if validated_count > 0:
            research_summary += f" {validated_count} events validated by external research."
        if enhanced_count > 0:
            research_summary += f" {enhanced_count} events enhanced with additional context."
        if conflicting_count > 0:
            research_summary += f" {conflicting_count} events flagged for potential conflicts."
        
        # Calculate timeline quality metrics
        avg_confidence = sum(event.confidence for event in events) / len(events) if events else 0
        quality_indicator = "high" if avg_confidence > 0.8 else "medium" if avg_confidence > 0.6 else "low"
        
        enhanced_summary = f"{base_summary}{research_summary} Timeline quality: {quality_indicator} (avg confidence: {avg_confidence:.2f})."
        
        return enhanced_summary 







    # Knowledge panel methods removed - functionality moved to Chimera
    
    async def _synthesize_information_flow_map(
        self,
        videos: List[VideoIntelligence],
        unified_entities: List[CrossVideoEntity],
        cross_video_relationships: List[CrossVideoRelationship],
        collection_id: str,
        collection_title: str
    ) -> InformationFlowMap:
        """
        Synthesizes comprehensive Information Flow Maps tracking concept evolution.
        
        Creates detailed analysis of how concepts, ideas, and information develop
        and flow across video collections, showing knowledge dependencies and evolution.
        
        Args:
            videos: List of processed videos
            unified_entities: Cross-video resolved entities
            cross_video_relationships: Cross-video relationships
            collection_id: ID of the collection
            collection_title: Title of the collection
            
        Returns:
            InformationFlowMap with comprehensive concept flow analysis
        """
        logger.info(f"Synthesizing Information Flow Map for {len(videos)} videos...")
        
        # Step 1: Extract concept nodes from all videos
        concept_nodes = await self._extract_concept_nodes(videos)
        
        # Step 2: Identify concept dependencies and information flows
        concept_dependencies = await self._identify_concept_dependencies(concept_nodes, videos)
        information_flows = await self._create_information_flows(concept_nodes)
        
        # Step 3: Analyze concept evolution paths
        evolution_paths = await self._analyze_concept_evolution_paths(concept_nodes)
        
        # Step 4: Create concept clusters
        concept_clusters = await self._create_concept_clusters(concept_nodes, concept_dependencies)
        
        # Step 5: Analyze flow patterns and generate insights
        flow_analysis = await self._analyze_flow_patterns(concept_nodes, information_flows, concept_dependencies)
        
        # Step 6: Generate strategic insights
        if self.use_ai_validation:
            strategic_insights = await self._ai_generate_flow_insights(
                concept_nodes, information_flows, evolution_paths, collection_title
            )
        else:
            strategic_insights = self._template_generate_flow_insights(concept_nodes, information_flows)
        
        flow_map = InformationFlowMap(
            map_id=f"flow_map_{collection_id}",
            collection_id=collection_id,
            collection_title=collection_title,
            concept_nodes=concept_nodes,
            information_flows=information_flows,
            concept_dependencies=concept_dependencies,
            evolution_paths=evolution_paths,
            concept_clusters=concept_clusters,
            primary_information_pathways=flow_analysis.get("primary_pathways", []),
            knowledge_bottlenecks=flow_analysis.get("bottlenecks", []),
            information_gaps=flow_analysis.get("gaps", []),
            flow_summary=flow_analysis.get("summary", ""),
            learning_progression=flow_analysis.get("progression", ""),
            concept_complexity=flow_analysis.get("complexity", ""),
            strategic_insights=strategic_insights,
            overall_coherence=flow_analysis.get("coherence", 0.5),
            pedagogical_quality=flow_analysis.get("pedagogical", 0.5),
            information_density=flow_analysis.get("density", 0.5),
            total_concepts=len(concept_nodes),
            total_flows=len(information_flows),
            synthesis_quality="AI_ENHANCED" if self.use_ai_validation else "TEMPLATE"
        )
        
        logger.info(f"Successfully created Information Flow Map with {len(concept_nodes)} concepts and {len(information_flows)} flows")
        return flow_map

    async def _extract_concept_nodes(self, videos: List[VideoIntelligence]) -> List[ConceptNode]:
        """Extract concept nodes from video content."""
        logger.info("Extracting concept nodes from video content...")
        
        concept_nodes = []
        
        for video_index, video in enumerate(videos):
            # Extract concepts from key points
            for kp in video.key_points:
                # Filter for conceptual key points (not just factual statements)
                if self._is_conceptual_content(kp.text):
                    concept_name = await self._extract_main_concept(kp.text)
                    if concept_name:
                        maturity_level = self._assess_concept_maturity(kp.text)
                        
                        node = ConceptNode(
                            node_id=f"concept_{video.metadata.video_id}_{kp.timestamp}_{len(concept_nodes)}",
                            concept_name=concept_name,
                            video_id=video.metadata.video_id,
                            video_title=video.metadata.title,
                            timestamp=kp.timestamp,
                            maturity_level=maturity_level,
                            context=kp.text,
                            explanation_depth=self._assess_explanation_depth(kp.text),
                            key_points=[kp.text],
                            related_entities=self._extract_related_entities(kp.text, video.entities),
                            sentiment=self._assess_concept_sentiment(kp.text),
                            confidence=kp.importance,
                            information_density=self._calculate_information_density(kp.text),
                            video_sequence_position=video_index
                        )
                        concept_nodes.append(node)
            
            # Extract concepts from topics
            for topic in video.topics:
                if self._is_significant_topic(topic):
                    concept_name = topic.name
                    
                    # Find context from summary or key points
                    context = self._find_topic_context(topic.name, video)
                    
                    node = ConceptNode(
                        node_id=f"topic_{video.metadata.video_id}_{topic.name.replace(' ', '_')}",
                        concept_name=concept_name,
                        video_id=video.metadata.video_id,
                        video_title=video.metadata.title,
                        timestamp=0,  # Topics don't have specific timestamps
                        maturity_level=ConceptMaturityLevel.MENTIONED,  # Default for topics
                        context=context,
                        explanation_depth=topic.confidence,
                        related_entities=self._extract_related_entities(context, video.entities),
                        sentiment=0.0,
                        confidence=topic.confidence,
                        information_density=0.3,  # Topics are generally lower density
                        video_sequence_position=video_index
                    )
                    concept_nodes.append(node)
        
        # Remove duplicate concepts
        concept_nodes = self._deduplicate_concepts(concept_nodes)
        
        logger.info(f"Extracted {len(concept_nodes)} concept nodes")
        return concept_nodes

    def _is_conceptual_content(self, text: str) -> bool:
        """Determine if text contains conceptual rather than purely factual content."""
        conceptual_indicators = [
            "concept", "idea", "theory", "principle", "approach", "strategy",
            "philosophy", "doctrine", "methodology", "framework", "model",
            "understanding", "perspective", "viewpoint", "analysis", "interpretation"
        ]
        
        text_lower = text.lower()
        return any(indicator in text_lower for indicator in conceptual_indicators)

    async def _extract_main_concept(self, text: str) -> Optional[str]:
        """Extract the main concept from text."""
        # Simple extraction - in production, could use NLP techniques
        # Look for noun phrases that represent concepts
        words = text.split()
        
        # Look for important concept keywords
        concept_patterns = [
            "nuclear program", "sanctions regime", "diplomatic relations",
            "security framework", "economic policy", "international law",
            "human rights", "climate change", "artificial intelligence"
        ]
        
        text_lower = text.lower()
        for pattern in concept_patterns:
            if pattern in text_lower:
                return pattern.title()
        
        # Fallback: extract first significant noun phrase
        significant_words = [w for w in words if len(w) > 4 and w.istitle()]
        if significant_words:
            return " ".join(significant_words[:2])
        
        return None

    def _assess_concept_maturity(self, text: str) -> ConceptMaturityLevel:
        """Assess the maturity level of a concept based on its context."""
        text_lower = text.lower()
        
        # Define maturity indicators
        maturity_indicators = {
            ConceptMaturityLevel.MENTIONED: ["mentions", "refers to", "talks about", "discusses"],
            ConceptMaturityLevel.DEFINED: ["defines", "explains", "describes", "clarifies", "outlines"],
            ConceptMaturityLevel.EXPLORED: ["analyzes", "examines", "investigates", "explores", "studies"],
            ConceptMaturityLevel.SYNTHESIZED: ["integrates", "combines", "synthesizes", "connects", "relates"],
            ConceptMaturityLevel.CRITICIZED: ["criticizes", "challenges", "questions", "disputes", "argues against"],
            ConceptMaturityLevel.EVOLVED: ["evolves", "develops", "progresses", "advances", "transforms"]
        }
        
        # Check from highest to lowest maturity
        for level, indicators in reversed(list(maturity_indicators.items())):
            if any(indicator in text_lower for indicator in indicators):
                return level
        
        return ConceptMaturityLevel.MENTIONED  # Default

    def _assess_explanation_depth(self, text: str) -> float:
        """Assess how deeply a concept is explained."""
        # Simple heuristic based on text length and complexity
        base_score = min(len(text) / 200, 1.0)  # Longer text = more depth
        
        # Boost for analytical language
        analytical_terms = ["because", "therefore", "however", "furthermore", "analysis", "explains"]
        analytical_boost = sum(1 for term in analytical_terms if term.lower() in text.lower()) * 0.1
        
        return min(base_score + analytical_boost, 1.0)

    def _extract_related_entities(self, text: str, entities: List[Entity]) -> List[str]:
        """Find entities mentioned in the concept text."""
        text_lower = text.lower()
        related = []
        
        for entity in entities:
            if entity.name.lower() in text_lower:
                related.append(entity.name)
        
        return related[:5]  # Limit to top 5

    def _assess_concept_sentiment(self, text: str) -> float:
        """Assess sentiment toward the concept."""
        # Simple sentiment analysis
        positive_words = ["good", "excellent", "positive", "beneficial", "effective", "successful"]
        negative_words = ["bad", "poor", "negative", "harmful", "ineffective", "failed"]
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count + negative_count == 0:
            return 0.0  # Neutral
        
        return (positive_count - negative_count) / (positive_count + negative_count)

    def _calculate_information_density(self, text: str) -> float:
        """Calculate information density of the text."""
        # Simple heuristic: ratio of content words to total words
        words = text.split()
        
        # Define content words (not function words)
        function_words = {"the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by"}
        content_words = [w for w in words if w.lower() not in function_words and len(w) > 2]
        
        if len(words) == 0:
            return 0.0
        
        return len(content_words) / len(words)

    def _is_significant_topic(self, topic: Topic) -> bool:
        """Determine if a topic is significant enough to track."""
        return topic.confidence > 0.7 and len(topic.name) > 3

    def _find_topic_context(self, topic_name: str, video: VideoIntelligence) -> str:
        """Find context for a topic from video content."""
        # Look in summary first
        if topic_name.lower() in video.summary.lower():
            sentences = video.summary.split('. ')
            for sentence in sentences:
                if topic_name.lower() in sentence.lower():
                    return sentence
        
        # Look in key points
        for kp in video.key_points:
            if topic_name.lower() in kp.text.lower():
                return kp.text
        
        return f"Topic '{topic_name}' discussed in video"

    def _deduplicate_concepts(self, concept_nodes: List[ConceptNode]) -> List[ConceptNode]:
        """Remove duplicate concept nodes."""
        seen_concepts = {}
        unique_nodes = []
        
        for node in concept_nodes:
            key = (node.concept_name.lower(), node.video_id)
            if key not in seen_concepts:
                seen_concepts[key] = node
                unique_nodes.append(node)
            else:
                # Merge with existing if this one has higher maturity
                existing = seen_concepts[key]
                if node.maturity_level.value > existing.maturity_level.value:
                    seen_concepts[key] = node
                    unique_nodes = [n for n in unique_nodes if n.node_id != existing.node_id]
                    unique_nodes.append(node)
        
        return unique_nodes

    async def _identify_concept_dependencies(
        self, 
        concept_nodes: List[ConceptNode], 
        videos: List[VideoIntelligence]
    ) -> List[ConceptDependency]:
        """Identify dependencies between concepts."""
        logger.info("Identifying concept dependencies...")
        
        dependencies = []
        
        # Group concepts by name
        concept_groups = {}
        for node in concept_nodes:
            name = node.concept_name.lower()
            if name not in concept_groups:
                concept_groups[name] = []
            concept_groups[name].append(node)
        
        # Look for temporal dependencies (concepts that appear in sequence)
        sorted_nodes = sorted(concept_nodes, key=lambda n: (n.video_sequence_position, n.timestamp))
        
        for i in range(len(sorted_nodes) - 1):
            current = sorted_nodes[i]
            next_node = sorted_nodes[i + 1]
            
            # Check if concepts are related and next builds on current
            if self._concepts_are_related(current, next_node) and current.video_sequence_position <= next_node.video_sequence_position:
                dependency = ConceptDependency(
                    dependency_id=f"dep_{current.node_id}_{next_node.node_id}",
                    prerequisite_concept=current.concept_name,
                    dependent_concept=next_node.concept_name,
                    dependency_type="builds_on",
                    dependency_strength=self._calculate_dependency_strength(current, next_node),
                    video_evidence=[current.video_id, next_node.video_id],
                    textual_evidence=[current.context, next_node.context],
                    explanation=f"{next_node.concept_name} builds on understanding of {current.concept_name}",
                    confidence=0.7
                )
                dependencies.append(dependency)
        
        logger.info(f"Identified {len(dependencies)} concept dependencies")
        return dependencies

    def _concepts_are_related(self, concept1: ConceptNode, concept2: ConceptNode) -> bool:
        """Determine if two concepts are related."""
        # Check for overlapping entities
        entities1 = set(concept1.related_entities)
        entities2 = set(concept2.related_entities)
        entity_overlap = len(entities1.intersection(entities2)) / max(len(entities1.union(entities2)), 1)
        
        # Check for related concept names
        name_similarity = self._calculate_name_similarity(concept1.concept_name, concept2.concept_name)
        
        return entity_overlap > 0.3 or name_similarity > 0.5

    def _calculate_name_similarity(self, name1: str, name2: str) -> float:
        """Calculate similarity between concept names."""
        words1 = set(name1.lower().split())
        words2 = set(name2.lower().split())
        
        if len(words1.union(words2)) == 0:
            return 0.0
        
        return len(words1.intersection(words2)) / len(words1.union(words2))

    def _calculate_dependency_strength(self, prerequisite: ConceptNode, dependent: ConceptNode) -> float:
        """Calculate the strength of dependency between concepts."""
        # Base strength on concept maturity levels
        maturity_levels = {
            ConceptMaturityLevel.MENTIONED: 1,
            ConceptMaturityLevel.DEFINED: 2,
            ConceptMaturityLevel.EXPLORED: 3,
            ConceptMaturityLevel.SYNTHESIZED: 4,
            ConceptMaturityLevel.CRITICIZED: 5,
            ConceptMaturityLevel.EVOLVED: 6
        }
        
        prereq_level = maturity_levels.get(prerequisite.maturity_level, 1)
        dep_level = maturity_levels.get(dependent.maturity_level, 1)
        
        # Strong dependency if dependent concept is more mature
        if dep_level > prereq_level:
            return min(0.8, 0.5 + (dep_level - prereq_level) * 0.1)
        else:
            return 0.3

    async def _create_information_flows(self, concept_nodes: List[ConceptNode]) -> List[InformationFlow]:
        """Create information flows between concept nodes."""
        logger.info("Creating information flows...")
        
        flows = []
        
        # Sort nodes by sequence and timestamp
        sorted_nodes = sorted(concept_nodes, key=lambda n: (n.video_sequence_position, n.timestamp))
        
        # Create flows between related concepts
        for i in range(len(sorted_nodes)):
            current = sorted_nodes[i]
            
            # Look for flows to later concepts
            for j in range(i + 1, min(i + 10, len(sorted_nodes))):  # Look ahead up to 10 concepts
                target = sorted_nodes[j]
                
                if self._should_create_flow(current, target):
                    flow = InformationFlow(
                        flow_id=f"flow_{current.node_id}_{target.node_id}",
                        source_node=current,
                        target_node=target,
                        flow_type=self._determine_flow_type(current, target),
                        information_transferred=self._describe_information_transfer(current, target),
                        transformation_type=self._determine_transformation_type(current, target),
                        flow_quality=self._assess_flow_quality(current, target),
                        coherence_score=self._calculate_flow_coherence(current, target),
                        temporal_gap=self._calculate_temporal_gap(current, target),
                        bridge_entities=list(set(current.related_entities).intersection(set(target.related_entities))),
                        supporting_evidence=[current.context, target.context]
                    )
                    flows.append(flow)
        
        logger.info(f"Created {len(flows)} information flows")
        return flows

    def _should_create_flow(self, source: ConceptNode, target: ConceptNode) -> bool:
        """Determine if an information flow should be created."""
        # Don't create flows within the same video unless there's clear progression
        if source.video_id == target.video_id:
            # Map maturity levels for comparison
            maturity_mapping = {
                "mentioned": 1, "defined": 2, "explored": 3,
                "synthesized": 4, "criticized": 5, "evolved": 6
            }
            source_level = maturity_mapping.get(source.maturity_level.value, 1)
            target_level = maturity_mapping.get(target.maturity_level.value, 1)
            return target.timestamp > source.timestamp and target_level > source_level
        
        # Create flows between videos if concepts are related
        return self._concepts_are_related(source, target)

    def _determine_flow_type(self, source: ConceptNode, target: ConceptNode) -> str:
        """Determine the type of information flow."""
        if source.concept_name.lower() == target.concept_name.lower():
            return "development"  # Same concept being developed
        else:
            # Map maturity levels for comparison
            maturity_mapping = {
                "mentioned": 1, "defined": 2, "explored": 3,
                "synthesized": 4, "criticized": 5, "evolved": 6
            }
            source_level = maturity_mapping.get(source.maturity_level.value, 1)
            target_level = maturity_mapping.get(target.maturity_level.value, 1)
            
            if target_level > source_level:
                return "elaboration"  # Concept being elaborated
            elif source.sentiment > 0 and target.sentiment < 0:
                return "contradiction"  # Contrasting viewpoints
            else:
                return "synthesis"  # Concepts being combined

    def _describe_information_transfer(self, source: ConceptNode, target: ConceptNode) -> str:
        """Describe what information flows between concepts."""
        if source.concept_name.lower() == target.concept_name.lower():
            return f"Enhanced understanding of {source.concept_name}"
        else:
            return f"Connecting {source.concept_name} to {target.concept_name}"

    def _determine_transformation_type(self, source: ConceptNode, target: ConceptNode) -> str:
        """Determine how information is transformed."""
        if target.explanation_depth > source.explanation_depth:
            return "deepening"
        elif len(target.related_entities) > len(source.related_entities):
            return "expansion"
        else:
            return "integration"

    def _assess_flow_quality(self, source: ConceptNode, target: ConceptNode) -> float:
        """Assess the quality of information flow."""
        # Base quality on confidence and coherence
        base_quality = (source.confidence + target.confidence) / 2
        
        # Map maturity levels for comparison
        maturity_mapping = {
            "mentioned": 1, "defined": 2, "explored": 3,
            "synthesized": 4, "criticized": 5, "evolved": 6
        }
        source_level = maturity_mapping.get(source.maturity_level.value, 1)
        target_level = maturity_mapping.get(target.maturity_level.value, 1)
        
        # Boost for logical progression
        if target_level > source_level:
            base_quality += 0.2
        
        # Boost for temporal coherence
        if source.video_sequence_position < target.video_sequence_position:
            base_quality += 0.1
        
        return min(base_quality, 1.0)

    def _calculate_flow_coherence(self, source: ConceptNode, target: ConceptNode) -> float:
        """Calculate coherence of the flow."""
        # Based on entity overlap and concept similarity
        entity_overlap = len(set(source.related_entities).intersection(set(target.related_entities)))
        max_entities = max(len(source.related_entities), len(target.related_entities), 1)
        
        name_similarity = self._calculate_name_similarity(source.concept_name, target.concept_name)
        
        return (entity_overlap / max_entities + name_similarity) / 2

    def _calculate_temporal_gap(self, source: ConceptNode, target: ConceptNode) -> int:
        """Calculate temporal gap between concept appearances."""
        if source.video_id == target.video_id:
            return abs(target.timestamp - source.timestamp)
        else:
            # For different videos, use a large gap value
            return (target.video_sequence_position - source.video_sequence_position) * 3600  # Assume 1 hour per video

    async def _analyze_concept_evolution_paths(self, concept_nodes: List[ConceptNode]) -> List[ConceptEvolutionPath]:
        """Analyze how concepts evolve across videos."""
        logger.info("Analyzing concept evolution paths...")
        
        evolution_paths = []
        
        # Group nodes by concept name
        concept_groups = {}
        for node in concept_nodes:
            name = node.concept_name.lower()
            if name not in concept_groups:
                concept_groups[name] = []
            concept_groups[name].append(node)
        
        # Create evolution paths for concepts that appear multiple times
        for concept_name, nodes in concept_groups.items():
            if len(nodes) > 1:
                # Sort by video sequence and timestamp
                sorted_nodes = sorted(nodes, key=lambda n: (n.video_sequence_position, n.timestamp))
                
                evolution_path = ConceptEvolutionPath(
                    path_id=f"path_{concept_name.replace(' ', '_')}",
                    concept_name=concept_name.title(),
                    evolution_nodes=sorted_nodes,
                    maturity_progression=[node.maturity_level for node in sorted_nodes],
                    evolution_summary=self._generate_evolution_summary(sorted_nodes),
                    key_transformations=self._identify_key_transformations(sorted_nodes),
                    breakthrough_moments=self._identify_breakthrough_moments(sorted_nodes),
                    evolution_coherence=self._calculate_evolution_coherence(sorted_nodes),
                    completeness_score=self._calculate_completeness_score(sorted_nodes),
                    understanding_depth=sorted_nodes[-1].explanation_depth if sorted_nodes else 0.5
                )
                evolution_paths.append(evolution_path)
        
        logger.info(f"Analyzed {len(evolution_paths)} concept evolution paths")
        return evolution_paths

    def _generate_evolution_summary(self, nodes: List[ConceptNode]) -> str:
        """Generate a summary of concept evolution."""
        if not nodes:
            return "No evolution data"
        
        start_maturity = nodes[0].maturity_level.value
        end_maturity = nodes[-1].maturity_level.value
        
        if end_maturity > start_maturity:
            return f"Concept evolves from {nodes[0].maturity_level.value} to {nodes[-1].maturity_level.value} across {len(nodes)} appearances"
        else:
            return f"Concept maintains {nodes[0].maturity_level.value} level across {len(nodes)} appearances"

    def _identify_key_transformations(self, nodes: List[ConceptNode]) -> List[str]:
        """Identify key transformations in concept understanding."""
        transformations = []
        
        for i in range(len(nodes) - 1):
            current = nodes[i]
            next_node = nodes[i + 1]
            
            if next_node.maturity_level.value > current.maturity_level.value:
                transformations.append(
                    f"From {current.maturity_level.value} to {next_node.maturity_level.value} in {next_node.video_title}"
                )
        
        return transformations

    def _identify_breakthrough_moments(self, nodes: List[ConceptNode]) -> List[Dict[str, Any]]:
        """Identify breakthrough moments in concept development."""
        breakthrough_moments = []
        
        if not nodes:
            return breakthrough_moments
        
        for i in range(len(nodes)):
            current = nodes[i]
            
            # Identify potential breakthrough moments
            is_breakthrough = False
            breakthrough_type = ""
            
            # 1. Significant maturity jump
            if i > 0:
                previous = nodes[i - 1]
                # Map maturity levels for comparison
                maturity_mapping = {
                    "mentioned": 1, "defined": 2, "explored": 3,
                    "synthesized": 4, "criticized": 5, "evolved": 6
                }
                current_level = maturity_mapping.get(current.maturity_level.value, 1)
                previous_level = maturity_mapping.get(previous.maturity_level.value, 1)
                maturity_jump = current_level - previous_level
                if maturity_jump >= 2:  # Jump of 2+ maturity levels
                    is_breakthrough = True
                    breakthrough_type = "maturity_leap"
            
            # 2. High explanation depth (detailed exploration)
            if current.explanation_depth > 0.8:
                is_breakthrough = True
                breakthrough_type = "detailed_exploration"
            
            # 3. High information density (new information)
            if current.information_density > 0.7:
                is_breakthrough = True
                breakthrough_type = "information_dense"
            
            # 4. Sentiment shift (controversial or critical discussion)
            if i > 0:
                previous = nodes[i - 1]
                sentiment_shift = abs(current.sentiment - previous.sentiment)
                if sentiment_shift > 0.5:
                    is_breakthrough = True
                    breakthrough_type = "perspective_shift"
            
            # 5. First appearance with high confidence
            if i == 0 and current.confidence > 0.9:
                is_breakthrough = True
                breakthrough_type = "strong_introduction"
            
            if is_breakthrough:
                breakthrough_moment = {
                    "video_id": current.video_id,
                    "video_title": current.video_title,
                    "timestamp": current.timestamp,
                    "breakthrough_type": breakthrough_type,
                    "maturity_level": current.maturity_level.value,
                    "explanation_depth": current.explanation_depth,
                    "confidence": current.confidence,
                    "context": current.context[:200] + "..." if len(current.context) > 200 else current.context,
                    "significance": self._calculate_breakthrough_significance(current, nodes, i)
                }
                breakthrough_moments.append(breakthrough_moment)
        
        # Sort by significance
        breakthrough_moments.sort(key=lambda m: m["significance"], reverse=True)
        
        return breakthrough_moments[:5]  # Return top 5 breakthrough moments

    def _calculate_breakthrough_significance(self, node: ConceptNode, all_nodes: List[ConceptNode], index: int) -> float:
        """Calculate the significance of a breakthrough moment."""
        significance = 0.0
        
        # Map maturity level to integer for calculation
        maturity_mapping = {
            "mentioned": 1,
            "defined": 2,
            "explored": 3,
            "synthesized": 4,
            "criticized": 5,
            "evolved": 6
        }
        
        # Base significance on maturity level
        maturity_score = maturity_mapping.get(node.maturity_level.value, 1) / 6.0  # Normalize to 0-1
        significance += maturity_score * 0.2
        
        # Boost for explanation depth
        significance += node.explanation_depth * 0.3
        
        # Boost for confidence
        significance += node.confidence * 0.2
        
        # Boost for information density
        significance += node.information_density * 0.2
        
        # Boost for position in sequence (later developments may be more significant)
        sequence_position = index / len(all_nodes) if all_nodes else 0
        significance += sequence_position * 0.1
        
        return min(significance, 1.0)

    def _calculate_evolution_coherence(self, nodes: List[ConceptNode]) -> float:
        """Calculate how coherent the concept evolution is."""
        if len(nodes) < 2:
            return 1.0
        
        # Map maturity levels for comparison
        maturity_mapping = {
            "mentioned": 1, "defined": 2, "explored": 3,
            "synthesized": 4, "criticized": 5, "evolved": 6
        }
        
        # Check for logical progression in maturity levels
        progression_score = 0.0
        for i in range(len(nodes) - 1):
            current_level = maturity_mapping.get(nodes[i].maturity_level.value, 1)
            next_level = maturity_mapping.get(nodes[i + 1].maturity_level.value, 1)
            if next_level >= current_level:
                progression_score += 1.0
        
        return progression_score / (len(nodes) - 1)

    def _calculate_completeness_score(self, nodes: List[ConceptNode]) -> float:
        """Calculate how complete the concept evolution is."""
        # Based on reaching higher maturity levels
        maturity_mapping = {
            "mentioned": 1, "defined": 2, "explored": 3,
            "synthesized": 4, "criticized": 5, "evolved": 6
        }
        
        max_maturity = max(maturity_mapping.get(node.maturity_level.value, 1) for node in nodes) if nodes else 1
        max_possible = 6  # Maximum maturity level
        
        return max_maturity / max_possible

    async def _create_concept_clusters(
        self, 
        concept_nodes: List[ConceptNode], 
        concept_dependencies: List[ConceptDependency]
    ) -> List[ConceptCluster]:
        """Create clusters of related concepts."""
        logger.info("Creating concept clusters...")
        
        # Simple clustering based on shared entities and dependencies
        clusters = []
        clustered_concepts = set()
        
        for node in concept_nodes:
            if node.concept_name in clustered_concepts:
                continue
            
            # Find related concepts
            related_concepts = [node.concept_name]
            
            # Add concepts with shared entities
            for other_node in concept_nodes:
                if (other_node.concept_name != node.concept_name and 
                    other_node.concept_name not in clustered_concepts):
                    
                    # Check entity overlap
                    shared_entities = set(node.related_entities).intersection(set(other_node.related_entities))
                    if len(shared_entities) > 0:
                        related_concepts.append(other_node.concept_name)
            
            # Add concepts with dependencies
            for dep in concept_dependencies:
                if dep.prerequisite_concept == node.concept_name:
                    if dep.dependent_concept not in related_concepts:
                        related_concepts.append(dep.dependent_concept)
            
            if len(related_concepts) > 1:  # Only create cluster if there are multiple concepts
                cluster = ConceptCluster(
                    cluster_id=f"cluster_{len(clusters)}",
                    cluster_name=f"{node.concept_name} cluster",
                    core_concepts=related_concepts,
                    cluster_evolution=f"Cluster of {len(related_concepts)} related concepts",
                    internal_relationships=concept_dependencies,
                    external_connections=[],  # Would need more analysis
                    coherence_score=0.7,  # Default
                    influence_score=0.5,  # Default
                    completeness=0.6  # Default
                )
                clusters.append(cluster)
                clustered_concepts.update(related_concepts)
        
        logger.info(f"Created {len(clusters)} concept clusters")
        return clusters

    async def _analyze_flow_patterns(
        self, 
        concept_nodes: List[ConceptNode], 
        information_flows: List[InformationFlow],
        concept_dependencies: List[ConceptDependency]
    ) -> Dict[str, Any]:
        """Analyze information flow patterns."""
        logger.info("Analyzing flow patterns...")
        
        # Identify primary pathways (most frequent flow types)
        flow_types = {}
        for flow in information_flows:
            flow_types[flow.flow_type] = flow_types.get(flow.flow_type, 0) + 1
        
        primary_pathways = [f"{ftype}: {count} flows" for ftype, count in 
                          sorted(flow_types.items(), key=lambda x: x[1], reverse=True)]
        
        # Identify bottlenecks (concepts with many dependencies)
        concept_dep_count = {}
        for dep in concept_dependencies:
            concept_dep_count[dep.prerequisite_concept] = concept_dep_count.get(dep.prerequisite_concept, 0) + 1
        
        bottlenecks = [concept for concept, count in concept_dep_count.items() if count > 2]
        
        # Identify information gaps (concepts without flows)
        concepts_in_flows = set()
        for flow in information_flows:
            concepts_in_flows.add(flow.source_node.concept_name)
            concepts_in_flows.add(flow.target_node.concept_name)
        
        all_concepts = {node.concept_name for node in concept_nodes}
        information_gaps = list(all_concepts - concepts_in_flows)
        
        # Calculate overall metrics
        avg_coherence = sum(flow.coherence_score for flow in information_flows) / len(information_flows) if information_flows else 0.5
        avg_quality = sum(flow.flow_quality for flow in information_flows) / len(information_flows) if information_flows else 0.5
        
        return {
            "primary_pathways": primary_pathways,
            "bottlenecks": bottlenecks,
            "gaps": information_gaps,
            "summary": f"Information flows through {len(primary_pathways)} main pathways with {len(bottlenecks)} bottlenecks",
            "progression": "Concepts develop through systematic information flow patterns",
            "complexity": f"Medium complexity with {len(information_flows)} total flows",
            "coherence": avg_coherence,
            "pedagogical": avg_quality,
            "density": len(information_flows) / len(concept_nodes) if concept_nodes else 0.0
        }

    async def _ai_generate_flow_insights(
        self,
        concept_nodes: List[ConceptNode],
        information_flows: List[InformationFlow],
        evolution_paths: List[ConceptEvolutionPath],
        collection_title: str
    ) -> List[str]:
        """Generate AI-powered insights about information flow."""
        try:
            # Prepare analysis data
            top_concepts = sorted(concept_nodes, key=lambda n: n.explanation_depth, reverse=True)[:10]
            key_flows = sorted(information_flows, key=lambda f: f.flow_quality, reverse=True)[:10]
            
            prompt = f"""
            Analyze the information flow and concept evolution in this video collection: "{collection_title}"
            
            CONCEPT ANALYSIS:
            Total concepts tracked: {len(concept_nodes)}
            Information flows: {len(information_flows)}
            Evolution paths: {len(evolution_paths)}
            
            TOP CONCEPTS BY DEPTH:
            {chr(10).join(c.concept_name for c in top_concepts)}
            
            KEY INFORMATION FLOWS:
            {chr(10).join(f"{f.source_node.concept_name} â†’ {f.target_node.concept_name} ({f.flow_type})" for f in key_flows)}
            
            EVOLUTION PATTERNS:
            {chr(10).join(p.evolution_summary for p in evolution_paths[:5])}
            
            Generate insights about:
            1. How information flows through the collection
            2. Which concepts are most critical to understanding
            3. How knowledge builds and evolves
            4. Pedagogical strengths and weaknesses
            5. Information architecture effectiveness
            6. Strategic implications for learning/understanding
            
            Format as bullet points, each 1-2 sentences.
            """
            
            response = await self.ai_model.generate_content_async(prompt)
            
            # Parse insights
            insights = []
            for line in response.text.strip().split('\n'):
                line = line.strip()
                if line and (line.startswith('-') or line.startswith('â€¢')):
                    insights.append(line.lstrip('-â€¢').strip())
            
            return insights[:7]  # Limit to 7 insights
            
        except Exception as e:
            logger.warning(f"AI flow insights generation failed: {e}")
            return self._template_generate_flow_insights(concept_nodes, information_flows)

    def _template_generate_flow_insights(
        self, 
        concept_nodes: List[ConceptNode], 
        information_flows: List[InformationFlow]
    ) -> List[str]:
        """Generate template-based flow insights."""
        insights = []
        
        if concept_nodes:
            # Map maturity levels to integers for comparison
            maturity_mapping = {
                "mentioned": 1,
                "defined": 2,
                "explored": 3,
                "synthesized": 4,
                "criticized": 5,
                "evolved": 6
            }
            
            avg_maturity = sum(maturity_mapping.get(n.maturity_level.value, 1) for n in concept_nodes) / len(concept_nodes)
            insights.append(f"Concept maturity averages {avg_maturity:.1f} across {len(concept_nodes)} concepts")
        
        if information_flows:
            avg_quality = sum(f.flow_quality for f in information_flows) / len(information_flows)
            insights.append(f"Information flow quality averages {avg_quality:.2f} across {len(information_flows)} flows")
        
        insights.append("Information flow shows systematic development across video collection")
        insights.append("Information architecture enables progressive understanding development")
        
        return insights

    # All knowledge panel methods have been removed - functionality moved to Chimera

    async def _core_intelligence_batch(
        self,
        videos: List[VideoIntelligence],
        unified_entities: List[CrossVideoEntity],
        cross_video_relationships: List[CrossVideoRelationship],
        topic_evolution: List[TopicEvolution],
        collection_title: str
    ) -> Dict[str, Any]:
        """
        ðŸš€ BATCH 1: Core Intelligence (Entity validation + Collection summary + Key insights)
        
        Combines the 3 most essential and related AI tasks into one optimized call.
        """
        try:
            # Prepare context for core intelligence
            multi_video_entities = [e for e in unified_entities if len(e.video_appearances) > 1][:30]
            
            entity_descriptions = []
            for entity in multi_video_entities:
                videos_info = f"Videos: {', '.join(entity.video_appearances[:2])}"
                aliases_info = f"Aliases: {', '.join(entity.aliases[:2])}" if entity.aliases else "No aliases"
                entity_descriptions.append(f"{entity.canonical_name} ({entity.type}) - {videos_info}, {aliases_info}")
            
            video_summaries = []
            for i, video in enumerate(videos[:5]):  # Top 5 videos
                video_summaries.append(f"""
                Video {i+1}: "{video.metadata.title}"
                Duration: {video.metadata.duration/60:.1f} minutes
                Summary: {video.summary[:150]}...
                Key Topics: {', '.join([t.name for t in video.topics[:3]])}
                """)
            
            relationship_summaries = []
            for rel in cross_video_relationships[:15]:  # Top 15 relationships
                videos_list = ', '.join(rel.video_sources[:2])
                relationship_summaries.append(f"{rel.subject} â†’ {rel.predicate} â†’ {rel.object} (Videos: {videos_list})")
            
            # Core intelligence prompt
            core_prompt = f"""
            You are an expert intelligence analyst performing core validation of a multi-video collection.
            
            COLLECTION: "{collection_title}" ({len(videos)} videos)
            
            TASK 1: ENTITY VALIDATION
            Multi-video entities to validate:
            {chr(10).join(entity_descriptions[:15])}
            
            For each entity: MERGE/SPLIT/ENHANCE with confidence (0.0-1.0) and brief reasoning.
            
            TASK 2: COLLECTION SUMMARY
            Video collection overview:
            {chr(10).join(video_summaries)}
            
            Generate a comprehensive 2-paragraph summary covering:
            - Main themes and key findings
            - Cross-video connections and significance
            
            TASK 3: KEY INSIGHTS
            Cross-video relationships:
            {chr(10).join(relationship_summaries)}
            
            Extract 5-6 strategic insights from cross-video analysis.
            
            RESPONSE FORMAT (JSON):
            {{
                "entity_validation": [
                    {{"entity": "name", "decision": "MERGE", "confidence": 0.9, "reasoning": "brief reason"}}
                ],
                "collection_summary": "2-paragraph comprehensive summary...",
                "key_insights": [
                    "Insight 1: Strategic finding...",
                    "Insight 2: Cross-video pattern..."
                ]
            }}
            
            Focus on accuracy and actionable intelligence.
            """
            
            logger.info("ðŸš€ Executing BATCH 1: Core Intelligence (replaces 3 separate calls)")
            
            response = await self.ai_model.generate_content_async(core_prompt)
            
            # Parse JSON response
            try:
                clean_text = response.text.strip()
                if clean_text.startswith('```json'):
                    clean_text = clean_text[7:-3]
                elif clean_text.startswith('```'):
                    clean_text = clean_text[3:-3]
                
                core_results = json.loads(clean_text)
                logger.info("âœ… Core Intelligence batch completed successfully!")
                return core_results
                
            except json.JSONDecodeError as e:
                logger.warning(f"Failed to parse core intelligence JSON: {e}")
                return {
                    "entity_validation": [],
                    "collection_summary": f"Collection of {len(videos)} videos covering {collection_title}",
                    "key_insights": [f"Analysis of {len(unified_entities)} entities across {len(videos)} videos"]
                }
                
        except Exception as e:
            logger.error(f"Core intelligence batch failed: {e}")
            return {
                "entity_validation": [],
                "collection_summary": f"Multi-video collection: {collection_title}",
                "key_insights": ["Analysis completed"]
            }

    async def _flow_analysis_batch(
        self,
        videos: List[VideoIntelligence],
        concept_nodes: List[ConceptNode],
        information_flows: List[InformationFlow],
        evolution_paths: List[ConceptEvolutionPath],
        collection_title: str
    ) -> Dict[str, Any]:
        """
        ðŸš€ BATCH 2: Flow Analysis (Narrative flow + Information flow insights)
        
        Advanced analysis of how information and concepts flow across videos.
        """
        try:
            # Prepare flow analysis context
            top_concepts = sorted(concept_nodes, key=lambda n: n.explanation_depth, reverse=True)[:8]
            key_flows = sorted(information_flows, key=lambda f: f.flow_quality, reverse=True)[:8]
            key_evolution = evolution_paths[:5]
            
            flow_prompt = f"""
            You are an expert information architect analyzing concept flow in: "{collection_title}"
            
            FLOW ANALYSIS DATA:
            Total concepts: {len(concept_nodes)}
            Information flows: {len(information_flows)}
            Evolution paths: {len(evolution_paths)}
            
            TOP CONCEPTS BY DEPTH:
            {chr(10).join(f"{c.concept_name} ({c.maturity_level.value}, depth: {c.explanation_depth:.2f})" for c in top_concepts)}
            
            KEY INFORMATION FLOWS:
            {chr(10).join(f"{f.source_node.concept_name} â†’ {f.target_node.concept_name} ({f.flow_type}, quality: {f.flow_quality:.2f})" for f in key_flows)}
            
            EVOLUTION PATTERNS:
            {chr(10).join(f"{p.concept_name}: {p.evolution_summary}" for p in key_evolution)}
            
            TASK 1: NARRATIVE FLOW ANALYSIS
            Identify 3-4 major narrative segments showing how information develops across videos.
            
            TASK 2: INFORMATION FLOW INSIGHTS
            Generate 4-5 insights about how knowledge builds, flows, and evolves.
            
            RESPONSE FORMAT (JSON):
            {{
                "narrative_analysis": {{
                    "segments": [
                        {{"title": "Segment 1", "description": "How info develops...", "videos": ["id1", "id2"]}}
                    ],
                    "progression": "How narrative develops across collection..."
                }},
                "flow_insights": [
                    "Flow insight 1: Knowledge pathway pattern...",
                    "Flow insight 2: Information dependency..."
                ]
            }}
            
            Focus on learning pathways and knowledge architecture.
            """
            
            logger.info("ðŸš€ Executing BATCH 2: Flow Analysis (replaces 2 separate calls)")
            
            response = await self.ai_model.generate_content_async(flow_prompt)
            
            # Parse JSON response
            try:
                clean_text = response.text.strip()
                if clean_text.startswith('```json'):
                    clean_text = clean_text[7:-3]
                elif clean_text.startswith('```'):
                    clean_text = clean_text[3:-3]
                
                flow_results = json.loads(clean_text)
                logger.info("âœ… Flow Analysis batch completed successfully!")
                return flow_results
                
            except json.JSONDecodeError as e:
                logger.warning(f"Failed to parse flow analysis JSON: {e}")
                return {
                    "narrative_analysis": {"segments": [], "progression": "Sequential analysis"},
                    "flow_insights": ["Information flow analysis completed"]
                }
                
        except Exception as e:
            logger.error(f"Flow analysis batch failed: {e}")
            return {
                "narrative_analysis": {"segments": [], "progression": "Basic analysis"},
                "flow_insights": ["Standard processing"]
            }







    # Knowledge panel methods removed - functionality moved to Chimera
    
    async def _synthesize_information_flow_map(
        self,
        videos: List[VideoIntelligence],
        unified_entities: List[CrossVideoEntity],
        cross_video_relationships: List[CrossVideoRelationship],
        collection_id: str,
        collection_title: str
    ) -> InformationFlowMap:
        """
        Synthesizes comprehensive Information Flow Maps tracking concept evolution.
        
        Creates detailed analysis of how concepts, ideas, and information develop
        and flow across video collections, showing knowledge dependencies and evolution.
        
        Args:
            videos: List of processed videos
            unified_entities: Cross-video resolved entities
            cross_video_relationships: Cross-video relationships
            collection_id: ID of the collection
            collection_title: Title of the collection
            
        Returns:
            InformationFlowMap with comprehensive concept flow analysis
        """
        logger.info(f"Synthesizing Information Flow Map for {len(videos)} videos...")
        
        # Step 1: Extract concept nodes from all videos
        concept_nodes = await self._extract_concept_nodes(videos)
        
        # Step 2: Identify concept dependencies and information flows
        concept_dependencies = await self._identify_concept_dependencies(concept_nodes, videos)
        information_flows = await self._create_information_flows(concept_nodes)
        
        # Step 3: Analyze concept evolution paths
        evolution_paths = await self._analyze_concept_evolution_paths(concept_nodes)
        
        # Step 4: Create concept clusters
        concept_clusters = await self._create_concept_clusters(concept_nodes, concept_dependencies)
        
        # Step 5: Analyze flow patterns and generate insights
        flow_analysis = await self._analyze_flow_patterns(concept_nodes, information_flows, concept_dependencies)
        
        # Step 6: Generate strategic insights
        if self.use_ai_validation:
            strategic_insights = await self._ai_generate_flow_insights(
                concept_nodes, information_flows, evolution_paths, collection_title
            )
        else:
            strategic_insights = self._template_generate_flow_insights(concept_nodes, information_flows)
        
        flow_map = InformationFlowMap(
            map_id=f"flow_map_{collection_id}",
            collection_id=collection_id,
            collection_title=collection_title,
            concept_nodes=concept_nodes,
            information_flows=information_flows,
            concept_dependencies=concept_dependencies,
            evolution_paths=evolution_paths,
            concept_clusters=concept_clusters,
            primary_information_pathways=flow_analysis.get("primary_pathways", []),
            knowledge_bottlenecks=flow_analysis.get("bottlenecks", []),
            information_gaps=flow_analysis.get("gaps", []),
            flow_summary=flow_analysis.get("summary", ""),
            learning_progression=flow_analysis.get("progression", ""),
            concept_complexity=flow_analysis.get("complexity", ""),
            strategic_insights=strategic_insights,
            overall_coherence=flow_analysis.get("coherence", 0.5),
            pedagogical_quality=flow_analysis.get("pedagogical", 0.5),
            information_density=flow_analysis.get("density", 0.5),
            total_concepts=len(concept_nodes),
            total_flows=len(information_flows),
            synthesis_quality="AI_ENHANCED" if self.use_ai_validation else "TEMPLATE"
        )
        
        logger.info(f"Successfully created Information Flow Map with {len(concept_nodes)} concepts and {len(information_flows)} flows")
        return flow_map

    async def _extract_concept_nodes(self, videos: List[VideoIntelligence]) -> List[ConceptNode]:
        """Extract concept nodes from video content."""
        logger.info("Extracting concept nodes from video content...")
        
        concept_nodes = []
        
        for video_index, video in enumerate(videos):
            # Extract concepts from key points
            for kp in video.key_points:
                # Filter for conceptual key points (not just factual statements)
                if self._is_conceptual_content(kp.text):
                    concept_name = await self._extract_main_concept(kp.text)
                    if concept_name:
                        maturity_level = self._assess_concept_maturity(kp.text)
                        
                        node = ConceptNode(
                            node_id=f"concept_{video.metadata.video_id}_{kp.timestamp}_{len(concept_nodes)}",
                            concept_name=concept_name,
                            video_id=video.metadata.video_id,
                            video_title=video.metadata.title,
                            timestamp=kp.timestamp,
                            maturity_level=maturity_level,
                            context=kp.text,
                            explanation_depth=self._assess_explanation_depth(kp.text),
                            key_points=[kp.text],
                            related_entities=self._extract_related_entities(kp.text, video.entities),
                            sentiment=self._assess_concept_sentiment(kp.text),
                            confidence=kp.importance,
                            information_density=self._calculate_information_density(kp.text),
                            video_sequence_position=video_index
                        )
                        concept_nodes.append(node)
            
            # Extract concepts from topics
            for topic in video.topics:
                if self._is_significant_topic(topic):
                    concept_name = topic.name
                    
                    # Find context from summary or key points
                    context = self._find_topic_context(topic.name, video)
                    
                    node = ConceptNode(
                        node_id=f"topic_{video.metadata.video_id}_{topic.name.replace(' ', '_')}",
                        concept_name=concept_name,
                        video_id=video.metadata.video_id,
                        video_title=video.metadata.title,
                        timestamp=0,  # Topics don't have specific timestamps
                        maturity_level=ConceptMaturityLevel.MENTIONED,  # Default for topics
                        context=context,
                        explanation_depth=topic.confidence,
                        related_entities=self._extract_related_entities(context, video.entities),
                        sentiment=0.0,
                        confidence=topic.confidence,
                        information_density=0.3,  # Topics are generally lower density
                        video_sequence_position=video_index
                    )
                    concept_nodes.append(node)
        
        # Remove duplicate concepts
        concept_nodes = self._deduplicate_concepts(concept_nodes)
        
        logger.info(f"Extracted {len(concept_nodes)} concept nodes")
        return concept_nodes

    def _is_conceptual_content(self, text: str) -> bool:
        """Determine if text contains conceptual rather than purely factual content."""
        conceptual_indicators = [
            "concept", "idea", "theory", "principle", "approach", "strategy",
            "philosophy", "doctrine", "methodology", "framework", "model",
            "understanding", "perspective", "viewpoint", "analysis", "interpretation"
        ]
        
        text_lower = text.lower()
        return any(indicator in text_lower for indicator in conceptual_indicators)

    async def _extract_main_concept(self, text: str) -> Optional[str]:
        """Extract the main concept from text."""
        # Simple extraction - in production, could use NLP techniques
        # Look for noun phrases that represent concepts
        words = text.split()
        
        # Look for important concept keywords
        concept_patterns = [
            "nuclear program", "sanctions regime", "diplomatic relations",
            "security framework", "economic policy", "international law",
            "human rights", "climate change", "artificial intelligence"
        ]
        
        text_lower = text.lower()
        for pattern in concept_patterns:
            if pattern in text_lower:
                return pattern.title()
        
        # Fallback: extract first significant noun phrase
        significant_words = [w for w in words if len(w) > 4 and w.istitle()]
        if significant_words:
            return " ".join(significant_words[:2])
        
        return None

    def _assess_concept_maturity(self, text: str) -> ConceptMaturityLevel:
        """Assess the maturity level of a concept based on its context."""
        text_lower = text.lower()
        
        # Define maturity indicators
        maturity_indicators = {
            ConceptMaturityLevel.MENTIONED: ["mentions", "refers to", "talks about", "discusses"],
            ConceptMaturityLevel.DEFINED: ["defines", "explains", "describes", "clarifies", "outlines"],
            ConceptMaturityLevel.EXPLORED: ["analyzes", "examines", "investigates", "explores", "studies"],
            ConceptMaturityLevel.SYNTHESIZED: ["integrates", "combines", "synthesizes", "connects", "relates"],
            ConceptMaturityLevel.CRITICIZED: ["criticizes", "challenges", "questions", "disputes", "argues against"],
            ConceptMaturityLevel.EVOLVED: ["evolves", "develops", "progresses", "advances", "transforms"]
        }
        
        # Check from highest to lowest maturity
        for level, indicators in reversed(list(maturity_indicators.items())):
            if any(indicator in text_lower for indicator in indicators):
                return level
        
        return ConceptMaturityLevel.MENTIONED  # Default

    def _assess_explanation_depth(self, text: str) -> float:
        """Assess how deeply a concept is explained."""
        # Simple heuristic based on text length and complexity
        base_score = min(len(text) / 200, 1.0)  # Longer text = more depth
        
        # Boost for analytical language
        analytical_terms = ["because", "therefore", "however", "furthermore", "analysis", "explains"]
        analytical_boost = sum(1 for term in analytical_terms if term.lower() in text.lower()) * 0.1
        
        return min(base_score + analytical_boost, 1.0)

    def _extract_related_entities(self, text: str, entities: List[Entity]) -> List[str]:
        """Find entities mentioned in the concept text."""
        text_lower = text.lower()
        related = []
        
        for entity in entities:
            if entity.name.lower() in text_lower:
                related.append(entity.name)
        
        return related[:5]  # Limit to top 5

    def _assess_concept_sentiment(self, text: str) -> float:
        """Assess sentiment toward the concept."""
        # Simple sentiment analysis
        positive_words = ["good", "excellent", "positive", "beneficial", "effective", "successful"]
        negative_words = ["bad", "poor", "negative", "harmful", "ineffective", "failed"]
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count + negative_count == 0:
            return 0.0  # Neutral
        
        return (positive_count - negative_count) / (positive_count + negative_count)

    def _calculate_information_density(self, text: str) -> float:
        """Calculate information density of the text."""
        # Simple heuristic: ratio of content words to total words
        words = text.split()
        
        # Define content words (not function words)
        function_words = {"the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by"}
        content_words = [w for w in words if w.lower() not in function_words and len(w) > 2]
        
        if len(words) == 0:
            return 0.0
        
        return len(content_words) / len(words)

    def _is_significant_topic(self, topic: Topic) -> bool:
        """Determine if a topic is significant enough to track."""
        return topic.confidence > 0.7 and len(topic.name) > 3

    def _find_topic_context(self, topic_name: str, video: VideoIntelligence) -> str:
        """Find context for a topic from video content."""
        # Look in summary first
        if topic_name.lower() in video.summary.lower():
            sentences = video.summary.split('. ')
            for sentence in sentences:
                if topic_name.lower() in sentence.lower():
                    return sentence
        
        # Look in key points
        for kp in video.key_points:
            if topic_name.lower() in kp.text.lower():
                return kp.text
        
        return f"Topic '{topic_name}' discussed in video"

    def _deduplicate_concepts(self, concept_nodes: List[ConceptNode]) -> List[ConceptNode]:
        """Remove duplicate concept nodes."""
        seen_concepts = {}
        unique_nodes = []
        
        for node in concept_nodes:
            key = (node.concept_name.lower(), node.video_id)
            if key not in seen_concepts:
                seen_concepts[key] = node
                unique_nodes.append(node)
            else:
                # Merge with existing if this one has higher maturity
                existing = seen_concepts[key]
                if node.maturity_level.value > existing.maturity_level.value:
                    seen_concepts[key] = node
                    unique_nodes = [n for n in unique_nodes if n.node_id != existing.node_id]
                    unique_nodes.append(node)
        
        return unique_nodes

    async def _identify_concept_dependencies(
        self, 
        concept_nodes: List[ConceptNode], 
        videos: List[VideoIntelligence]
    ) -> List[ConceptDependency]:
        """Identify dependencies between concepts."""
        logger.info("Identifying concept dependencies...")
        
        dependencies = []
        
        # Group concepts by name
        concept_groups = {}
        for node in concept_nodes:
            name = node.concept_name.lower()
            if name not in concept_groups:
                concept_groups[name] = []
            concept_groups[name].append(node)
        
        # Look for temporal dependencies (concepts that appear in sequence)
        sorted_nodes = sorted(concept_nodes, key=lambda n: (n.video_sequence_position, n.timestamp))
        
        for i in range(len(sorted_nodes) - 1):
            current = sorted_nodes[i]
            next_node = sorted_nodes[i + 1]
            
            # Check if concepts are related and next builds on current
            if self._concepts_are_related(current, next_node) and current.video_sequence_position <= next_node.video_sequence_position:
                dependency = ConceptDependency(
                    dependency_id=f"dep_{current.node_id}_{next_node.node_id}",
                    prerequisite_concept=current.concept_name,
                    dependent_concept=next_node.concept_name,
                    dependency_type="builds_on",
                    dependency_strength=self._calculate_dependency_strength(current, next_node),
                    video_evidence=[current.video_id, next_node.video_id],
                    textual_evidence=[current.context, next_node.context],
                    explanation=f"{next_node.concept_name} builds on understanding of {current.concept_name}",
                    confidence=0.7
                )
                dependencies.append(dependency)
        
        logger.info(f"Identified {len(dependencies)} concept dependencies")
        return dependencies

    def _concepts_are_related(self, concept1: ConceptNode, concept2: ConceptNode) -> bool:
        """Determine if two concepts are related."""
        # Check for overlapping entities
        entities1 = set(concept1.related_entities)
        entities2 = set(concept2.related_entities)
        entity_overlap = len(entities1.intersection(entities2)) / max(len(entities1.union(entities2)), 1)
        
        # Check for related concept names
        name_similarity = self._calculate_name_similarity(concept1.concept_name, concept2.concept_name)
        
        return entity_overlap > 0.3 or name_similarity > 0.5

    def _calculate_name_similarity(self, name1: str, name2: str) -> float:
        """Calculate similarity between concept names."""
        words1 = set(name1.lower().split())
        words2 = set(name2.lower().split())
        
        if len(words1.union(words2)) == 0:
            return 0.0
        
        return len(words1.intersection(words2)) / len(words1.union(words2))

    def _calculate_dependency_strength(self, prerequisite: ConceptNode, dependent: ConceptNode) -> float:
        """Calculate the strength of dependency between concepts."""
        # Base strength on concept maturity levels
        maturity_levels = {
            ConceptMaturityLevel.MENTIONED: 1,
            ConceptMaturityLevel.DEFINED: 2,
            ConceptMaturityLevel.EXPLORED: 3,
            ConceptMaturityLevel.SYNTHESIZED: 4,
            ConceptMaturityLevel.CRITICIZED: 5,
            ConceptMaturityLevel.EVOLVED: 6
        }
        
        prereq_level = maturity_levels.get(prerequisite.maturity_level, 1)
        dep_level = maturity_levels.get(dependent.maturity_level, 1)
        
        # Strong dependency if dependent concept is more mature
        if dep_level > prereq_level:
            return min(0.8, 0.5 + (dep_level - prereq_level) * 0.1)
        else:
            return 0.3

    async def _create_information_flows(self, concept_nodes: List[ConceptNode]) -> List[InformationFlow]:
        """Create information flows between concept nodes."""
        logger.info("Creating information flows...")
        
        flows = []
        
        # Sort nodes by sequence and timestamp
        sorted_nodes = sorted(concept_nodes, key=lambda n: (n.video_sequence_position, n.timestamp))
        
        # Create flows between related concepts
        for i in range(len(sorted_nodes)):
            current = sorted_nodes[i]
            
            # Look for flows to later concepts
            for j in range(i + 1, min(i + 10, len(sorted_nodes))):  # Look ahead up to 10 concepts
                target = sorted_nodes[j]
                
                if self._should_create_flow(current, target):
                    flow = InformationFlow(
                        flow_id=f"flow_{current.node_id}_{target.node_id}",
                        source_node=current,
                        target_node=target,
                        flow_type=self._determine_flow_type(current, target),
                        information_transferred=self._describe_information_transfer(current, target),
                        transformation_type=self._determine_transformation_type(current, target),
                        flow_quality=self._assess_flow_quality(current, target),
                        coherence_score=self._calculate_flow_coherence(current, target),
                        temporal_gap=self._calculate_temporal_gap(current, target),
                        bridge_entities=list(set(current.related_entities).intersection(set(target.related_entities))),
                        supporting_evidence=[current.context, target.context]
                    )
                    flows.append(flow)
        
        logger.info(f"Created {len(flows)} information flows")
        return flows

    def _should_create_flow(self, source: ConceptNode, target: ConceptNode) -> bool:
        """Determine if an information flow should be created."""
        # Don't create flows within the same video unless there's clear progression
        if source.video_id == target.video_id:
            # Map maturity levels for comparison
            maturity_mapping = {
                "mentioned": 1, "defined": 2, "explored": 3,
                "synthesized": 4, "criticized": 5, "evolved": 6
            }
            source_level = maturity_mapping.get(source.maturity_level.value, 1)
            target_level = maturity_mapping.get(target.maturity_level.value, 1)
            return target.timestamp > source.timestamp and target_level > source_level
        
        # Create flows between videos if concepts are related
        return self._concepts_are_related(source, target)

    def _determine_flow_type(self, source: ConceptNode, target: ConceptNode) -> str:
        """Determine the type of information flow."""
        if source.concept_name.lower() == target.concept_name.lower():
            return "development"  # Same concept being developed
        else:
            # Map maturity levels for comparison
            maturity_mapping = {
                "mentioned": 1, "defined": 2, "explored": 3,
                "synthesized": 4, "criticized": 5, "evolved": 6
            }
            source_level = maturity_mapping.get(source.maturity_level.value, 1)
            target_level = maturity_mapping.get(target.maturity_level.value, 1)
            
            if target_level > source_level:
                return "elaboration"  # Concept being elaborated
            elif source.sentiment > 0 and target.sentiment < 0:
                return "contradiction"  # Contrasting viewpoints
            else:
                return "synthesis"  # Concepts being combined

    def _describe_information_transfer(self, source: ConceptNode, target: ConceptNode) -> str:
        """Describe what information flows between concepts."""
        if source.concept_name.lower() == target.concept_name.lower():
            return f"Enhanced understanding of {source.concept_name}"
        else:
            return f"Connecting {source.concept_name} to {target.concept_name}"

    def _determine_transformation_type(self, source: ConceptNode, target: ConceptNode) -> str:
        """Determine how information is transformed."""
        if target.explanation_depth > source.explanation_depth:
            return "deepening"
        elif len(target.related_entities) > len(source.related_entities):
            return "expansion"
        else:
            return "integration"

    def _assess_flow_quality(self, source: ConceptNode, target: ConceptNode) -> float:
        """Assess the quality of information flow."""
        # Base quality on confidence and coherence
        base_quality = (source.confidence + target.confidence) / 2
        
        # Map maturity levels for comparison
        maturity_mapping = {
            "mentioned": 1, "defined": 2, "explored": 3,
            "synthesized": 4, "criticized": 5, "evolved": 6
        }
        source_level = maturity_mapping.get(source.maturity_level.value, 1)
        target_level = maturity_mapping.get(target.maturity_level.value, 1)
        
        # Boost for logical progression
        if target_level > source_level:
            base_quality += 0.2
        
        # Boost for temporal coherence
        if source.video_sequence_position < target.video_sequence_position:
            base_quality += 0.1
        
        return min(base_quality, 1.0)

    def _calculate_flow_coherence(self, source: ConceptNode, target: ConceptNode) -> float:
        """Calculate coherence of the flow."""
        # Based on entity overlap and concept similarity
        entity_overlap = len(set(source.related_entities).intersection(set(target.related_entities)))
        max_entities = max(len(source.related_entities), len(target.related_entities), 1)
        
        name_similarity = self._calculate_name_similarity(source.concept_name, target.concept_name)
        
        return (entity_overlap / max_entities + name_similarity) / 2

    def _calculate_temporal_gap(self, source: ConceptNode, target: ConceptNode) -> int:
        """Calculate temporal gap between concept appearances."""
        if source.video_id == target.video_id:
            return abs(target.timestamp - source.timestamp)
        else:
            # For different videos, use a large gap value
            return (target.video_sequence_position - source.video_sequence_position) * 3600  # Assume 1 hour per video

    async def _analyze_concept_evolution_paths(self, concept_nodes: List[ConceptNode]) -> List[ConceptEvolutionPath]:
        """Analyze how concepts evolve across videos."""
        logger.info("Analyzing concept evolution paths...")
        
        evolution_paths = []
        
        # Group nodes by concept name
        concept_groups = {}
        for node in concept_nodes:
            name = node.concept_name.lower()
            if name not in concept_groups:
                concept_groups[name] = []
            concept_groups[name].append(node)
        
        # Create evolution paths for concepts that appear multiple times
        for concept_name, nodes in concept_groups.items():
            if len(nodes) > 1:
                # Sort by video sequence and timestamp
                sorted_nodes = sorted(nodes, key=lambda n: (n.video_sequence_position, n.timestamp))
                
                evolution_path = ConceptEvolutionPath(
                    path_id=f"path_{concept_name.replace(' ', '_')}",
                    concept_name=concept_name.title(),
                    evolution_nodes=sorted_nodes,
                    maturity_progression=[node.maturity_level for node in sorted_nodes],
                    evolution_summary=self._generate_evolution_summary(sorted_nodes),
                    key_transformations=self._identify_key_transformations(sorted_nodes),
                    breakthrough_moments=self._identify_breakthrough_moments(sorted_nodes),
                    evolution_coherence=self._calculate_evolution_coherence(sorted_nodes),
                    completeness_score=self._calculate_completeness_score(sorted_nodes),
                    understanding_depth=sorted_nodes[-1].explanation_depth if sorted_nodes else 0.5
                )
                evolution_paths.append(evolution_path)
        
        logger.info(f"Analyzed {len(evolution_paths)} concept evolution paths")
        return evolution_paths

    def _generate_evolution_summary(self, nodes: List[ConceptNode]) -> str:
        """Generate a summary of concept evolution."""
        if not nodes:
            return "No evolution data"
        
        start_maturity = nodes[0].maturity_level.value
        end_maturity = nodes[-1].maturity_level.value
        
        if end_maturity > start_maturity:
            return f"Concept evolves from {nodes[0].maturity_level.value} to {nodes[-1].maturity_level.value} across {len(nodes)} appearances"
        else:
            return f"Concept maintains {nodes[0].maturity_level.value} level across {len(nodes)} appearances"

    def _identify_key_transformations(self, nodes: List[ConceptNode]) -> List[str]:
        """Identify key transformations in concept understanding."""
        transformations = []
        
        for i in range(len(nodes) - 1):
            current = nodes[i]
            next_node = nodes[i + 1]
            
            if next_node.maturity_level.value > current.maturity_level.value:
                transformations.append(
                    f"From {current.maturity_level.value} to {next_node.maturity_level.value} in {next_node.video_title}"
                )
        
        return transformations

    def _identify_breakthrough_moments(self, nodes: List[ConceptNode]) -> List[Dict[str, Any]]:
        """Identify breakthrough moments in concept development."""
        breakthrough_moments = []
        
        if not nodes:
            return breakthrough_moments
        
        for i in range(len(nodes)):
            current = nodes[i]
            
            # Identify potential breakthrough moments
            is_breakthrough = False
            breakthrough_type = ""
            
            # 1. Significant maturity jump
            if i > 0:
                previous = nodes[i - 1]
                # Map maturity levels for comparison
                maturity_mapping = {
                    "mentioned": 1, "defined": 2, "explored": 3,
                    "synthesized": 4, "criticized": 5, "evolved": 6
                }
                current_level = maturity_mapping.get(current.maturity_level.value, 1)
                previous_level = maturity_mapping.get(previous.maturity_level.value, 1)
                maturity_jump = current_level - previous_level
                if maturity_jump >= 2:  # Jump of 2+ maturity levels
                    is_breakthrough = True
                    breakthrough_type = "maturity_leap"
            
            # 2. High explanation depth (detailed exploration)
            if current.explanation_depth > 0.8:
                is_breakthrough = True
                breakthrough_type = "detailed_exploration"
            
            # 3. High information density (new information)
            if current.information_density > 0.7:
                is_breakthrough = True
                breakthrough_type = "information_dense"
            
            # 4. Sentiment shift (controversial or critical discussion)
            if i > 0:
                previous = nodes[i - 1]
                sentiment_shift = abs(current.sentiment - previous.sentiment)
                if sentiment_shift > 0.5:
                    is_breakthrough = True
                    breakthrough_type = "perspective_shift"
            
            # 5. First appearance with high confidence
            if i == 0 and current.confidence > 0.9:
                is_breakthrough = True
                breakthrough_type = "strong_introduction"
            
            if is_breakthrough:
                breakthrough_moment = {
                    "video_id": current.video_id,
                    "video_title": current.video_title,
                    "timestamp": current.timestamp,
                    "breakthrough_type": breakthrough_type,
                    "maturity_level": current.maturity_level.value,
                    "explanation_depth": current.explanation_depth,
                    "confidence": current.confidence,
                    "context": current.context[:200] + "..." if len(current.context) > 200 else current.context,
                    "significance": self._calculate_breakthrough_significance(current, nodes, i)
                }
                breakthrough_moments.append(breakthrough_moment)
        
        # Sort by significance
        breakthrough_moments.sort(key=lambda m: m["significance"], reverse=True)
        
        return breakthrough_moments[:5]  # Return top 5 breakthrough moments

    def _calculate_breakthrough_significance(self, node: ConceptNode, all_nodes: List[ConceptNode], index: int) -> float:
        """Calculate the significance of a breakthrough moment."""
        significance = 0.0
        
        # Map maturity level to integer for calculation
        maturity_mapping = {
            "mentioned": 1,
            "defined": 2,
            "explored": 3,
            "synthesized": 4,
            "criticized": 5,
            "evolved": 6
        }
        
        # Base significance on maturity level
        maturity_score = maturity_mapping.get(node.maturity_level.value, 1) / 6.0  # Normalize to 0-1
        significance += maturity_score * 0.2
        
        # Boost for explanation depth
        significance += node.explanation_depth * 0.3
        
        # Boost for confidence
        significance += node.confidence * 0.2
        
        # Boost for information density
        significance += node.information_density * 0.2
        
        # Boost for position in sequence (later developments may be more significant)
        sequence_position = index / len(all_nodes) if all_nodes else 0
        significance += sequence_position * 0.1
        
        return min(significance, 1.0)

    def _calculate_evolution_coherence(self, nodes: List[ConceptNode]) -> float:
        """Calculate how coherent the concept evolution is."""
        if len(nodes) < 2:
            return 1.0
        
        # Map maturity levels for comparison
        maturity_mapping = {
            "mentioned": 1, "defined": 2, "explored": 3,
            "synthesized": 4, "criticized": 5, "evolved": 6
        }
        
        # Check for logical progression in maturity levels
        progression_score = 0.0
        for i in range(len(nodes) - 1):
            current_level = maturity_mapping.get(nodes[i].maturity_level.value, 1)
            next_level = maturity_mapping.get(nodes[i + 1].maturity_level.value, 1)
            if next_level >= current_level:
                progression_score += 1.0
        
        return progression_score / (len(nodes) - 1)

    def _calculate_completeness_score(self, nodes: List[ConceptNode]) -> float:
        """Calculate how complete the concept evolution is."""
        # Based on reaching higher maturity levels
        maturity_mapping = {
            "mentioned": 1, "defined": 2, "explored": 3,
            "synthesized": 4, "criticized": 5, "evolved": 6
        }
        
        max_maturity = max(maturity_mapping.get(node.maturity_level.value, 1) for node in nodes) if nodes else 1
        max_possible = 6  # Maximum maturity level
        
        return max_maturity / max_possible

    async def _create_concept_clusters(
        self, 
        concept_nodes: List[ConceptNode], 
        concept_dependencies: List[ConceptDependency]
    ) -> List[ConceptCluster]:
        """Create clusters of related concepts."""
        logger.info("Creating concept clusters...")
        
        # Simple clustering based on shared entities and dependencies
        clusters = []
        clustered_concepts = set()
        
        for node in concept_nodes:
            if node.concept_name in clustered_concepts:
                continue
            
            # Find related concepts
            related_concepts = [node.concept_name]
            
            # Add concepts with shared entities
            for other_node in concept_nodes:
                if (other_node.concept_name != node.concept_name and 
                    other_node.concept_name not in clustered_concepts):
                    
                    # Check entity overlap
                    shared_entities = set(node.related_entities).intersection(set(other_node.related_entities))
                    if len(shared_entities) > 0:
                        related_concepts.append(other_node.concept_name)
            
            # Add concepts with dependencies
            for dep in concept_dependencies:
                if dep.prerequisite_concept == node.concept_name:
                    if dep.dependent_concept not in related_concepts:
                        related_concepts.append(dep.dependent_concept)
            
            if len(related_concepts) > 1:  # Only create cluster if there are multiple concepts
                cluster = ConceptCluster(
                    cluster_id=f"cluster_{len(clusters)}",
                    cluster_name=f"{node.concept_name} cluster",
                    core_concepts=related_concepts,
                    cluster_evolution=f"Cluster of {len(related_concepts)} related concepts",
                    internal_relationships=concept_dependencies,
                    external_connections=[],  # Would need more analysis
                    coherence_score=0.7,  # Default
                    influence_score=0.5,  # Default
                    completeness=0.6  # Default
                )
                clusters.append(cluster)
                clustered_concepts.update(related_concepts)
        
        logger.info(f"Created {len(clusters)} concept clusters")
        return clusters

    async def _analyze_flow_patterns(
        self, 
        concept_nodes: List[ConceptNode], 
        information_flows: List[InformationFlow],
        concept_dependencies: List[ConceptDependency]
    ) -> Dict[str, Any]:
        """Analyze information flow patterns."""
        logger.info("Analyzing flow patterns...")
        
        # Identify primary pathways (most frequent flow types)
        flow_types = {}
        for flow in information_flows:
            flow_types[flow.flow_type] = flow_types.get(flow.flow_type, 0) + 1
        
        primary_pathways = [f"{ftype}: {count} flows" for ftype, count in 
                          sorted(flow_types.items(), key=lambda x: x[1], reverse=True)]
        
        # Identify bottlenecks (concepts with many dependencies)
        concept_dep_count = {}
        for dep in concept_dependencies:
            concept_dep_count[dep.prerequisite_concept] = concept_dep_count.get(dep.prerequisite_concept, 0) + 1
        
        bottlenecks = [concept for concept, count in concept_dep_count.items() if count > 2]
        
        # Identify information gaps (concepts without flows)
        concepts_in_flows = set()
        for flow in information_flows:
            concepts_in_flows.add(flow.source_node.concept_name)
            concepts_in_flows.add(flow.target_node.concept_name)
        
        all_concepts = {node.concept_name for node in concept_nodes}
        information_gaps = list(all_concepts - concepts_in_flows)
        
        # Calculate overall metrics
        avg_coherence = sum(flow.coherence_score for flow in information_flows) / len(information_flows) if information_flows else 0.5
        avg_quality = sum(flow.flow_quality for flow in information_flows) / len(information_flows) if information_flows else 0.5
        
        return {
            "primary_pathways": primary_pathways,
            "bottlenecks": bottlenecks,
            "gaps": information_gaps,
            "summary": f"Information flows through {len(primary_pathways)} main pathways with {len(bottlenecks)} bottlenecks",
            "progression": "Concepts develop through systematic information flow patterns",
            "complexity": f"Medium complexity with {len(information_flows)} total flows",
            "coherence": avg_coherence,
            "pedagogical": avg_quality,
            "density": len(information_flows) / len(concept_nodes) if concept_nodes else 0.0
        }

    async def _ai_generate_flow_insights(
        self,
        concept_nodes: List[ConceptNode],
        information_flows: List[InformationFlow],
        evolution_paths: List[ConceptEvolutionPath],
        collection_title: str
    ) -> List[str]:
        """Generate AI-powered insights about information flow."""
        try:
            # Prepare analysis data
            top_concepts = sorted(concept_nodes, key=lambda n: n.explanation_depth, reverse=True)[:10]
            key_flows = sorted(information_flows, key=lambda f: f.flow_quality, reverse=True)[:10]
            
            prompt = f"""
            Analyze the information flow and concept evolution in this video collection: "{collection_title}"
            
            CONCEPT ANALYSIS:
            Total concepts tracked: {len(concept_nodes)}
            Information flows: {len(information_flows)}
            Evolution paths: {len(evolution_paths)}
            
            TOP CONCEPTS BY DEPTH:
            {chr(10).join(c.concept_name for c in top_concepts)}
            
            KEY INFORMATION FLOWS:
            {chr(10).join(f"{f.source_node.concept_name} â†’ {f.target_node.concept_name} ({f.flow_type})" for f in key_flows)}
            
            EVOLUTION PATTERNS:
            {chr(10).join(p.evolution_summary for p in evolution_paths[:5])}
            
            Generate insights about:
            1. How information flows through the collection
            2. Which concepts are most critical to understanding
            3. How knowledge builds and evolves
            4. Pedagogical strengths and weaknesses
            5. Information architecture effectiveness
            6. Strategic implications for learning/understanding
            
            Format as bullet points, each 1-2 sentences.
            """
            
            response = await self.ai_model.generate_content_async(prompt)
            
            # Parse insights
            insights = []
            for line in response.text.strip().split('\n'):
                line = line.strip()
                if line and (line.startswith('-') or line.startswith('â€¢')):
                    insights.append(line.lstrip('-â€¢').strip())
            
            return insights[:7]  # Limit to 7 insights
            
        except Exception as e:
            logger.warning(f"AI flow insights generation failed: {e}")
            return self._template_generate_flow_insights(concept_nodes, information_flows)

    def _template_generate_flow_insights(
        self, 
        concept_nodes: List[ConceptNode], 
        information_flows: List[InformationFlow]
    ) -> List[str]:
        """Generate template-based flow insights."""
        insights = []
        
        if concept_nodes:
            # Map maturity levels to integers for comparison
            maturity_mapping = {
                "mentioned": 1,
                "defined": 2,
                "explored": 3,
                "synthesized": 4,
                "criticized": 5,
                "evolved": 6
            }
            
            avg_maturity = sum(maturity_mapping.get(n.maturity_level.value, 1) for n in concept_nodes) / len(concept_nodes)
            insights.append(f"Concept maturity averages {avg_maturity:.1f} across {len(concept_nodes)} concepts")
        
        if information_flows:
            avg_quality = sum(f.flow_quality for f in information_flows) / len(information_flows)
            insights.append(f"Information flow quality averages {avg_quality:.2f} across {len(information_flows)} flows")
        
        insights.append("Information flow shows systematic development across video collection")
        insights.append("Information architecture enables progressive understanding development")
        
        return insights

    # All knowledge panel methods have been removed - functionality moved to Chimera


