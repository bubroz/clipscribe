# Extraction Technology in ClipScribe v2.16.0

This document explains the advanced extraction technologies, `GLiNER` and `REBEL`, that power ClipScribe's video intelligence features, plus planned enhancements for v2.17.0 Enhanced Temporal Intelligence.

## ðŸŽ¯ **MAJOR BREAKTHROUGH: REBEL Relationship Extraction Fixed**

**Critical Achievement**: ClipScribe achieved a major breakthrough by fixing the REBEL relationship extraction pipeline, enabling **meaningful knowledge graph construction** for the first time.

### **The Problem (Before Fix)**
- REBEL model was generating output but parser couldn't read it
- Zero relationships extracted from video content
- Knowledge graphs had nodes but no meaningful connections
- Space-separated output format was incompatible with XML parser

### **The Solution (Now Working)**
- **Complete parser rewrite** with dual parsing strategy
- **10-19 relationships per video** successfully extracted
- **Real knowledge graphs** with meaningful connections
- **Space-separated parsing** as primary method with XML fallback

### **Real Results from PBS NewsHour Content**:
- "Pegasus | spyware | instance of"
- "NSO | inception | 2010" 
- "Carmen Aristegui | employer | Aristegui Noticias"
- "United Arab Emirates | diplomatic relation | Saudi Arabia"
- "Enrique PeÃ±a Nieto | President of Mexico | position held"

## ðŸš€ **Optimized Architecture & Enhanced Temporal Intelligence (v2.17.0 Planned)**

ClipScribe is evolving with a streamlined architecture that eliminates inefficiencies while providing comprehensive temporal intelligence:

### **Optimized Video Processing Pipeline**
```
Video URL â†’ Download Video â†’ Direct Gemini 2.5 Flash Processing â†’ {
    accurate_transcript_with_timestamps,
    temporal_events_from_speech,
    visual_timestamp_recognition,
    entities_with_temporal_context
} â†’ Timeline Synthesis â†’ Video Retention â†’ Interactive Visualizations
```

### **Architecture Optimizations**:
- **Eliminates Audio Extraction**: No more videoâ†’audioâ†’processing inefficiency
- **Single Processing Call**: Direct video-to-Gemini for audio + visual intelligence
- **Video Retention Options**: Keep source material for future analysis and archival
- **Streamlined Pipeline**: Faster processing, better temporal intelligence

### **Enhanced Temporal Intelligence Capabilities**:
- **Temporal Event Extraction**: Parse historical events, dates, and chronological references from spoken content
- **Visual Timestamp Recognition**: Extract dates and times visible on screen (documents, calendars, news chyrons, presentations)
- **Accurate Transcript Segmentation**: Word-level timestamps for precise temporal mapping
- **Cross-Video Timeline Building**: Correlate events across video collections with temporal synthesis
- **Visual Intelligence Integration**: Correlate spoken content with visual cues for better context

### **Video Retention System**:
- **Delete Policy**: Remove videos after processing (current behavior)
- **Keep Processed**: Archive successfully processed videos for future use
- **Keep All**: Full archival system for source material preservation
- **Smart Storage**: Configurable archive directories and retention policies

### **Cost-Effective Implementation**:
- **Optimized Processing**: Eliminate duplicate work (video + audio extraction)
- **Enhanced Intelligence**: Single call gets audio + visual + temporal data
- **Minimal Cost Impact**: **12-20% cost increase for 300% more temporal intelligence**
- **Future-Ready**: Archived videos enable advanced features without reprocessing

### **Timeline Building Strategy**:
Leverage optimized video processing with enhanced temporal extraction, then synthesize rich timelines from visual + audio cues. This approach maximizes intelligence extraction per dollar while preserving source material for future capabilities.

## Entity Extraction Pipeline Architecture

ClipScribe uses a **sophisticated hybrid approach** with GLiNER as the primary extractor:

### **Pipeline Hierarchy (Cost-Optimized)**:
1. **SpaCy** (free, fast): Basic entity coverage for standard types (PERSON, ORG, LOC)
2. **GLiNER** (primary): Sophisticated transformer-based, contextually aware 
3. **REBEL**: Relationship extraction to connect entities
4. **Entity Normalization**: Smart merging, deduplication, confidence aggregation
5. **LLM Validation** (optional): High-confidence validation for critical applications

### **Why GLiNER Dominates (By Design)**:
- **Superior Contextual Understanding**: vs SpaCy's rule-based approach
- **Domain-Specific Entities**: Handles "Pegasus spyware", "NSO Group" vs SpaCy's generic PERSON/ORG
- **Better Boundary Detection**: More accurate entity spans and custom types
- **News/Video Optimized**: Designed for the content ClipScribe processes

### **Cost-Effective Intelligence**: Free (SpaCy coverage) â†’ Better (GLiNER primary) â†’ Best (LLM validation)

## Why GLiNER + REBEL is Powerful

This sophisticated hybrid model builds rich, queryable knowledge graphs from video content:

### GLiNER: The Universal Entity Finder

GLiNER finds the **NODES** (entities) of our knowledge graph. Unlike traditional Named Entity Recognition (NER) systems that are limited to a fixed set of categories (like PERSON, ORGANIZATION), GLiNER is a universal entity finder.

-   **Flexible**: It can find *any* entity type you describe with natural language, not just from a predefined list.
-   **Context-Aware**: It understands the difference between "Apple" (the company) and "apple" (the fruit) based on the surrounding text.
-   **Zero-Shot**: It doesn't require any special training to find entities in new, specialized domains.
-   **High Performance**: Comprehensive entity extraction targeting 100% completeness

**Example: Dynamic Entity Detection**

If we are analyzing a cooking video, we can ask GLiNER to find culinary-specific entities like `chef`, `dish`, `ingredient`, and `cooking_technique`. For a tech tutorial, we can ask for `programming_framework`, `database`, and `cloud_service`. GLiNER handles this dynamically without any code changes.

### REBEL: The Relationship Extractor

REBEL finds the **EDGES** (relationships) that connect the entities GLiNER discovers. It reads sentences and extracts semantic fact triples in the format `(Subject) -> [Predicate] -> (Object)`.

-   **Semantic**: It understands the meaning behind the words to find relationships like `founded`, `located in`, `works for`, or `competes with`.
-   **Directional**: It correctly identifies the subject and object in a relationship (e.g., `SpaceX manufactures Falcon 9`, not the other way around).
-   **Contextual**: It can extract implied relationships from the text.
-   **Working Output**: v2.19.0 extracts 52+ relationships with evidence chains

**Example: Extracting Knowledge Graph Triples**

From the text: *"Elon Musk founded SpaceX in 2002. The company is headquartered in Hawthorne, California."*

REBEL extracts triples like:
-   `(Elon Musk) -> [founded] -> (SpaceX)`
-   `(SpaceX) -> [inception] -> (2002)`
-   `(SpaceX) -> [headquarters location] -> (Hawthorne)`

### **Technical Implementation**

The breakthrough was achieved through a complete rewrite of the REBEL parser:

```python
def _parse_triplets(self, text: str) -> List[Dict[str, str]]:
    """Parse REBEL output with dual strategy: space-separated + XML fallback"""
    
    # Primary: Space-separated parsing
    space_separated_triplets = self._parse_space_separated(text)
    if space_separated_triplets:
        return space_separated_triplets
    
    # Fallback: XML tag parsing (legacy)
    return self._parse_xml_tags(text)
```

## The Combined Power: Building Knowledge Graphs

Together, GLiNER and REBEL allow ClipScribe to transform unstructured video transcripts into highly structured knowledge graphs.

1.  **GLiNER** identifies all the relevant entities (the nodes) - 250-300 per video
2.  **REBEL** identifies how they are all connected (the edges) - 10-19 relationships per video  
3.  **Knowledge Synthesis** correlates information across multiple videos with timeline generation
4.  **GEXF 1.3 Export** creates modern knowledge graphs for Gephi visualization
5.  The result is a rich, queryable network of facts and information that allows for deep analysis and insights far beyond a simple transcript.

## Performance Metrics

| Metric | Performance | Status |
|--------|-------------|--------|
| **Entity Extraction** | 250-300 entities per video | âœ… Working |
| **Relationship Extraction** | 10-19 relationships per video | âœ… **FIXED** |
| **Knowledge Graph Nodes** | 240+ nodes per video | âœ… Working |
| **Knowledge Graph Edges** | 9-13 edges per video | âœ… **FIXED** |
| **Processing Cost** | ~$0.41 per video | âœ… Optimized |
| **Success Rate** | 100% completion | âœ… Stable |

## Testing with News Content

**Recommendation**: Test ClipScribe's relationship extraction with news content like PBS NewsHour for best results. News content provides:
- **Clear factual relationships** between entities
- **Temporal context** for timeline synthesis  
- **Real-world connections** for meaningful knowledge graphs
- **Professional narration** for accurate extraction

Avoid music videos or entertainment content which produce poor relationship extraction quality. 

# Entity & Relationship Extraction Technology

*Last Updated: July 20, 2025*

## Overview

ClipScribe uses a sophisticated **hybrid extraction pipeline** that combines multiple AI models to achieve comprehensive entity and relationship extraction from video transcripts. Our goal is **100% completeness** - extracting every meaningful entity and relationship while maintaining high accuracy.

## v2.19.0 Quality Improvements

The latest version dramatically improves extraction quality:
- **Entities**: Now extracting 16+ meaningful entities per video (up from 0-10)
- **Relationships**: 52+ relationships with evidence chains (up from 0)
- **Language Purity**: 94.7% (filters out non-English noise)
- **Knowledge Graphs**: 88+ nodes, 52+ edges typical for news content

Key fixes:
- Fixed overly aggressive language filter that removed 70% of valid entities
- Fixed bug where Gemini's 50+ relationships were extracted but ignored
- Lowered confidence thresholds to capture more valid entities
- Made false positive detection less aggressive

## Extraction Philosophy

Rather than targeting arbitrary numbers (e.g., "extract 20-50 entities"), we target:
1. **100% Completeness**: Extract ALL entities mentioned in the video
2. **High Accuracy**: Minimize false positives through validation
3. **Rich Context**: Provide evidence and context for every extraction
4. **Relationship Coverage**: Capture all meaningful connections

## Hybrid Extraction Pipeline
