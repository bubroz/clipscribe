---
description: Testing requirements and patterns for ClipScribe
globs: 
alwaysApply: false
---
---
description: "Testing requirements and patterns for ClipScribe"
globs: ["tests/**/*.py", "**/test_*.py", "**/*_test.py", "pytest.ini"]
alwaysApply: false
---

# Testing Standards

## Test Structure

Follow the project test organization:

```
tests/
â”œâ”€â”€ unit/              # Fast, isolated unit tests
â”œâ”€â”€ integration/       # Tests with external dependencies
â””â”€â”€ fixtures/          # Test data and mocks
```

## Testing Guidelines

### Naming Conventions
- Test files: `test_<module_name>.py`
- Test classes: `Test<ClassName>`
- Test methods: `test_<behavior>_<expected_result>`

### Example Test Structure
```python
import pytest
from unittest.mock import Mock, patch

from clipscribe.extractors import SpacyExtractor


class TestSpacyExtractor:
    """Test SpaCy entity extraction functionality."""
    
    @pytest.fixture
    def extractor(self):
        """Create a SpaCy extractor instance."""
        return SpacyExtractor()
    
    def test_extract_entities_returns_correct_format(self, extractor):
        """Test that extract returns properly formatted results."""
        text = "Apple Inc. is located in Cupertino."
        results = extractor.extract(text)
        
        assert "entities" in results
        assert "relationships" in results
        assert isinstance(results["entities"], list)
    
    @patch('clipscribe.extractors.spacy_extractor.spacy.load')
    def test_model_loading_error_handling(self, mock_load):
        """Test graceful handling of model loading errors."""
        mock_load.side_effect = Exception("Model not found")
        
        with pytest.raises(ModelLoadError):
            SpacyExtractor()
```

## Test Categories

### Unit Tests
- Test individual functions/methods in isolation
- Mock all external dependencies
- Should run in < 1 second
- No network calls, file I/O, or database access

### Integration Tests
- Test component interactions
- May use real external services (with proper setup/teardown)
- Can take longer to run
- Should be skipped in CI if dependencies unavailable

## Fixtures and Mocking

### Common Fixtures
```python
@pytest.fixture
def sample_transcript():
    """Sample transcript for testing."""
    return {
        "text": "This is a test transcript.",
        "segments": [
            {"start": 0.0, "end": 2.0, "text": "This is"},
            {"start": 2.0, "end": 4.0, "text": "a test transcript."}
        ]
    }

@pytest.fixture
def mock_youtube_client():
    """Mock YouTube client for testing."""
    client = Mock()
    client.get_video_info.return_value = {
        "title": "Test Video",
        "duration": 120,
        "author": "Test Author"
    }
    return client
```

## Coverage Requirements

- Maintain minimum 80% code coverage
- Focus on testing business logic
- Don't test third-party library internals
- Use `# pragma: no cover` sparingly and with justification

## Running Tests

```bash
# Run all tests
poetry run pytest

# Run with coverage
poetry run pytest --cov=clipscribe --cov-report=html

# Run specific test file
poetry run pytest tests/unit/test_extractors.py

# Run tests matching pattern
poetry run pytest -k "extract"

# Run with verbose output
poetry run pytest -v
```

## Test Data Management

- Store test data in `tests/fixtures/`
- Use small, representative samples
- Don't commit large test files
- Mock external API responses

## Performance Testing

For performance-critical code:

```python
def test_extraction_performance(extractor, benchmark):
    """Test extraction performance."""
    text = "Large text sample..." * 1000
    
    result = benchmark(extractor.extract, text)
    
    assert benchmark.stats["mean"] < 1.0  # Should complete in < 1 second
```

## ðŸš¨ CRITICAL: Validation Before Success Declaration

**NEVER declare code "working" or "ready" without proper testing first.**

### Mandatory Validation Steps

#### 1. **Test Imports First**
```bash
# Always verify imports work before testing full functionality
poetry run python -c "from module import function; print('Import successful')"
```

#### 2. **Incremental Testing**
- **Step 1**: Test individual imports
- **Step 2**: Test core functionality in isolation  
- **Step 3**: Test component integration
- **Step 4**: Test full application/interface
- **Step 5**: Verify external connectivity (HTTP responses, etc.)

#### 3. **Error Diagnosis Protocol**
When errors occur:
1. **Read the full error message** - don't skip or assume
2. **Identify the root cause** - not just symptoms
3. **Fix the underlying issue** - not just workarounds
4. **Test the fix** - verify it actually resolves the problem
5. **Test related functionality** - ensure no regressions

#### 4. **Quality Gates**
Before moving to the next development phase:
- [ ] All imports resolve without errors
- [ ] Core functionality executes successfully  
- [ ] Integration points work as expected
- [ ] Error handling degrades gracefully
- [ ] User interface (if applicable) responds correctly

### Examples of Proper Validation

#### âœ… Good Practice
```bash
# Step 1: Test imports
poetry run python -c "from src.app import main; print('âœ… Import successful')"

# Step 2: Test core functionality
poetry run python -c "from src.app import main; main(); print('âœ… Core functionality works')"

# Step 3: Test HTTP response (for web apps)
curl -s -o /dev/null -w "%{http_code}" http://localhost:8501
# Expected: 200

# Step 4: Only then declare success
echo "âœ… All validation complete - ready for next phase"
```

#### âŒ Bad Practice
```bash
# DON'T DO THIS - assuming code works without testing
echo "âœ… Code complete - ready for next phase"  # NO TESTING!
```

### Documentation of Testing

When declaring completion of a feature:
1. **Document what was tested**
2. **Include test commands used**
3. **Show actual test results**
4. **Note any limitations or known issues**

Example:
```markdown
## âœ… Feature Complete: Authentication Module

### Tests Performed:
- Import validation: `poetry run python -c "from auth import login"`
- Function testing: `pytest tests/test_auth.py -v`
- Integration testing: `curl -X POST localhost:8000/login`

### Results:
- All imports: âœ… PASS
- Unit tests: âœ… 15/15 PASS  
- HTTP 200 response: âœ… PASS

### Ready for: Production deployment
```

### Learning from Failures

When validation reveals issues:
1. **Document what went wrong**
2. **Update testing procedures**
3. **Add preventive measures**
4. **Share lessons learned**

This prevents repeating the same mistakes and improves the development process :-)

## Common Validation Patterns

### For Streamlit Apps
```bash
# 1. Test imports
poetry run python -c "import streamlit as st; from src.module import component"

# 2. Test app launch (background)
poetry run streamlit run app.py --server.port 8501 &

# 3. Test HTTP response
curl -s -o /dev/null -w "%{http_code}" http://localhost:8501

# 4. Clean up
pkill -f streamlit
```

### For CLI Applications
```bash
# 1. Test imports
poetry run python -c "from src.cli import main"

# 2. Test help command
poetry run python -m src.cli --help

# 3. Test basic functionality
poetry run python -m src.cli test-command

# 4. Verify expected output
```

### For API Integrations  
```bash
# 1. Test configuration loading
poetry run python -c "from src.config import settings; print(settings.api_key[:8])"

# 2. Test API connectivity
poetry run python -c "from src.api import client; print(client.health_check())"

# 3. Test core API calls
poetry run python -c "from src.api import client; print(len(client.get_data()))"
```
