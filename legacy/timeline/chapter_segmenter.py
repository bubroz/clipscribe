"""Chapter-Based Video Segmenter - Leveraging yt-dlp Chapter Intelligence.

This module implements intelligent video segmentation using yt-dlp's chapter information,
a breakthrough capability that was previously ignored in ClipScribe:

CHAPTER INTELLIGENCE FEATURES:
- Automatic chapter boundary detection using yt-dlp
- Semantic chapter classification (introduction, content, conclusion)
- Chapter-aware temporal event extraction
- Content vs non-content chapter identification
- Chapter duration and pacing analysis
- Multi-chapter narrative flow analysis

SEGMENTATION STRATEGIES:
- Chapter-based: Use yt-dlp chapter boundaries (primary method)
- Content-based: Fallback segmentation using content analysis
- Hybrid approach: Combine chapters with content analysis
- Adaptive segmentation: Adjust based on video characteristics

This transforms video processing from blind transcript splitting to intelligent
content-aware segmentation using yt-dlp's chapter intelligence :-)
"""

import logging
from typing import List, Dict, Optional, Tuple, Set, Any, Union
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import re
from collections import defaultdict

from ..retrievers.universal_video_client import Chapter, TemporalMetadata
from .models import TemporalEvent, EventType, ChapterSegment

logger = logging.getLogger(__name__)


class ChapterType(Enum):
    """Classification of chapter types."""
    INTRODUCTION = "introduction"
    MAIN_CONTENT = "main_content"
    CONCLUSION = "conclusion"
    ADVERTISEMENT = "advertisement"
    CREDITS = "credits"
    TRANSITION = "transition"
    UNKNOWN = "unknown"


class SegmentationStrategy(Enum):
    """Video segmentation strategies."""
    CHAPTER_BASED = "chapter_based"      # Use yt-dlp chapters (primary)
    CONTENT_BASED = "content_based"      # Content analysis fallback
    HYBRID = "hybrid"                    # Combine chapters + content
    ADAPTIVE = "adaptive"                # Choose best strategy


@dataclass
class ChapterAnalysis:
    """Analysis of a video chapter."""
    chapter: Chapter
    chapter_type: ChapterType
    content_density: float           # How much meaningful content (0-1)
    temporal_events_count: int       # Number of temporal events found
    key_entities: List[str]          # Important entities in this chapter
    narrative_importance: float      # Importance to overall narrative (0-1)
    segmentation_confidence: float   # Confidence in chapter boundaries (0-1)
    should_process: bool            # Whether to extract events from this chapter


@dataclass
class VideoSegmentation:
    """Complete video segmentation result."""
    video_url: str
    strategy_used: SegmentationStrategy
    chapters: List[ChapterAnalysis]
    total_duration: float
    content_chapters: List[ChapterAnalysis]  # Only chapters with meaningful content
    segmentation_quality: float            # Overall segmentation quality (0-1)
    recommendations: List[str]             # Processing recommendations


class ChapterSegmenter:
    """Intelligent video segmentation using yt-dlp chapter information.
    
    BREAKTHROUGH CAPABILITY:
    - Leverages yt-dlp's chapter detection that ClipScribe previously ignored
    - Provides intelligent content boundaries instead of arbitrary time splits
    - Identifies content vs non-content sections automatically
    - Enables chapter-aware temporal event extraction
    - Dramatically improves event extraction accuracy and relevance
    
    This is a key component of Timeline v2.0's precision approach!
    """
    
    def __init__(self, enable_content_analysis: bool = True):
        """Initialize with configurable content analysis."""
        self.enable_content_analysis = enable_content_analysis
        
        # Chapter type classification patterns
        self.chapter_patterns = {
            ChapterType.INTRODUCTION: [
                r'\b(?:intro|introduction|welcome|hello|overview|preview)\b',
                r'\b(?:today we|in this video|we\'re going to)\b',
                r'\b(?:episode|part|chapter)\s*(?:one|1|intro)\b',
            ],
            ChapterType.CONCLUSION: [
                r'\b(?:conclusion|summary|wrap.?up|final|end|outro)\b',
                r'\b(?:that\'s all|thanks for watching|subscribe|like)\b',
                r'\b(?:next time|next video|coming up)\b',
            ],
            ChapterType.ADVERTISEMENT: [
                r'\b(?:sponsor|advertisement|ad|promo|partnership)\b',
                r'\b(?:brought to you by|sponsored by|thanks to)\b',
                r'\b(?:discount|coupon|offer|deal|sale)\b',
            ],
            ChapterType.CREDITS: [
                r'\b(?:credits|acknowledgments|special thanks)\b',
                r'\b(?:produced by|directed by|written by)\b',
                r'\b(?:music by|sound|audio)\b',
            ],
            ChapterType.TRANSITION: [
                r'\b(?:transition|break|pause|intermission)\b',
                r'\b(?:meanwhile|elsewhere|later|earlier)\b',
                r'\b(?:cut to|switch to|now let\'s)\b',
            ]
        }
        
        # Content quality indicators
        self.content_indicators = {
            'high_value': [
                r'\b(?:analysis|research|study|data|evidence|findings)\b',
                r'\b(?:important|significant|crucial|key|critical)\b',
                r'\b(?:because|therefore|however|although|since)\b',
            ],
            'medium_value': [
                r'\b(?:example|instance|case|situation|scenario)\b',
                r'\b(?:basically|essentially|generally|typically)\b',
                r'\b(?:here|there|this|that|these|those)\b',
            ],
            'low_value': [
                r'\b(?:um|uh|like|you know|sort of|kind of)\b',
                r'\b(?:okay|alright|well|so|yeah|right)\b',
                r'\b(?:and|but|or|the|a|an|is|was|are)\b',
            ]
        }
    
    async def segment_video(
        self, 
        video_url: str,
        temporal_metadata: TemporalMetadata,
        transcript_text: str,
        strategy: SegmentationStrategy = SegmentationStrategy.ADAPTIVE
    ) -> VideoSegmentation:
        """Perform intelligent video segmentation using optimal strategy.
        
        SEGMENTATION PIPELINE:
        1. Analyze available yt-dlp chapter information
        2. Choose optimal segmentation strategy
        3. Classify chapters by type and content value
        4. Analyze content density and narrative importance
        5. Generate processing recommendations
        
        Args:
            video_url: Video URL for identification
            temporal_metadata: yt-dlp temporal metadata including chapters
            transcript_text: Full video transcript
            strategy: Segmentation strategy to use
            
        Returns:
            Complete video segmentation with chapter analysis
        """
        logger.info(f"ðŸŽ¬ Starting video segmentation for: {video_url}")
        logger.info(f"ðŸ“š Available chapters: {len(temporal_metadata.chapters)}")
        
        # Choose optimal segmentation strategy
        if strategy == SegmentationStrategy.ADAPTIVE:
            strategy = self._choose_optimal_strategy(temporal_metadata, transcript_text)
        
        logger.info(f"ðŸŽ¯ Using segmentation strategy: {strategy.value}")
        
        # Perform segmentation based on chosen strategy
        if strategy == SegmentationStrategy.CHAPTER_BASED:
            segmentation = await self._segment_by_chapters(
                video_url, temporal_metadata, transcript_text
            )
        elif strategy == SegmentationStrategy.CONTENT_BASED:
            segmentation = await self._segment_by_content(
                video_url, temporal_metadata, transcript_text
            )
        elif strategy == SegmentationStrategy.HYBRID:
            segmentation = await self._segment_hybrid(
                video_url, temporal_metadata, transcript_text
            )
        else:
            # Fallback to chapter-based
            segmentation = await self._segment_by_chapters(
                video_url, temporal_metadata, transcript_text
            )
        
        logger.info(f"âœ… Segmentation complete: {len(segmentation.chapters)} chapters")
        logger.info(f"ðŸ“Š Content chapters: {len(segmentation.content_chapters)}")
        logger.info(f"ðŸŽ¯ Segmentation quality: {segmentation.segmentation_quality:.2f}")
        
        return segmentation
    
    def _choose_optimal_strategy(
        self, temporal_metadata: TemporalMetadata, transcript_text: str
    ) -> SegmentationStrategy:
        """Choose the optimal segmentation strategy based on available data."""
        
        # Check chapter availability and quality
        has_chapters = len(temporal_metadata.chapters) > 0
        chapter_quality = self._assess_chapter_quality(temporal_metadata.chapters)
        
        # Check transcript quality
        transcript_quality = self._assess_transcript_quality(transcript_text)
        
        logger.info(f"ðŸ“Š Chapter quality: {chapter_quality:.2f}, Transcript quality: {transcript_quality:.2f}")
        
        # Decision logic for optimal strategy
        if has_chapters and chapter_quality >= 0.7:
            return SegmentationStrategy.CHAPTER_BASED
        elif has_chapters and chapter_quality >= 0.4 and transcript_quality >= 0.6:
            return SegmentationStrategy.HYBRID
        elif transcript_quality >= 0.5:
            return SegmentationStrategy.CONTENT_BASED
        else:
            # Fallback to chapter-based even with low quality
            return SegmentationStrategy.CHAPTER_BASED if has_chapters else SegmentationStrategy.CONTENT_BASED
    
    def _assess_chapter_quality(self, chapters: List[Chapter]) -> float:
        """Assess the quality of yt-dlp chapter information."""
        if not chapters:
            return 0.0
        
        quality_score = 0.0
        
        # Check for reasonable chapter count (not too few, not too many)
        chapter_count = len(chapters)
        if 3 <= chapter_count <= 20:
            quality_score += 0.3
        elif 2 <= chapter_count <= 30:
            quality_score += 0.15
        
        # Check for meaningful chapter titles
        meaningful_titles = 0
        for chapter in chapters:
            if chapter.title and len(chapter.title.strip()) > 3:
                # Check if title contains meaningful words (not just numbers)
                words = chapter.title.split()
                meaningful_words = [w for w in words if not w.isdigit() and len(w) > 2]
                if len(meaningful_words) >= 1:
                    meaningful_titles += 1
        
        if meaningful_titles > 0:
            quality_score += 0.4 * (meaningful_titles / chapter_count)
        
        # Check for reasonable chapter durations
        reasonable_durations = 0
        for chapter in chapters:
            duration = chapter.end_time - chapter.start_time
            if 30 <= duration <= 3600:  # Between 30 seconds and 1 hour
                reasonable_durations += 1
        
        if reasonable_durations > 0:
            quality_score += 0.3 * (reasonable_durations / chapter_count)
        
        return min(quality_score, 1.0)
    
    def _assess_transcript_quality(self, transcript_text: str) -> float:
        """Assess the quality of transcript text for content-based segmentation."""
        if not transcript_text or len(transcript_text.strip()) < 100:
            return 0.0
        
        quality_score = 0.0
        
        # Check transcript length (reasonable amount of content)
        word_count = len(transcript_text.split())
        if word_count >= 500:
            quality_score += 0.3
        elif word_count >= 200:
            quality_score += 0.15
        
        # Check for sentence structure (punctuation)
        sentences = re.split(r'[.!?]+', transcript_text)
        meaningful_sentences = [s for s in sentences if len(s.strip().split()) >= 3]
        if len(meaningful_sentences) >= 10:
            quality_score += 0.3
        elif len(meaningful_sentences) >= 5:
            quality_score += 0.15
        
        # Check for content diversity (different topics/entities)
        words = transcript_text.lower().split()
        unique_words = set(words)
        vocabulary_diversity = len(unique_words) / len(words) if words else 0
        if vocabulary_diversity >= 0.3:
            quality_score += 0.4
        elif vocabulary_diversity >= 0.2:
            quality_score += 0.2
        
        return min(quality_score, 1.0)
    
    async def _segment_by_chapters(
        self, video_url: str, temporal_metadata: TemporalMetadata, transcript_text: str
    ) -> VideoSegmentation:
        """Segment video using yt-dlp chapter information (primary method)."""
        logger.info("ðŸ“š Segmenting video using yt-dlp chapters")
        
        chapter_analyses = []
        
        if not temporal_metadata.chapters:
            # Create a single chapter for the entire video
            logger.info("âš ï¸ No chapters available, creating single video chapter")
            single_chapter = Chapter(
                title="Full Video",
                start_time=0.0,
                end_time=len(transcript_text.split()) * 2,  # Rough estimate
                url=None
            )
            temporal_metadata.chapters = [single_chapter]
        
        # Analyze each chapter
        for i, chapter in enumerate(temporal_metadata.chapters):
            chapter_analysis = await self._analyze_chapter(
                chapter, transcript_text, temporal_metadata, i
            )
            chapter_analyses.append(chapter_analysis)
        
        # Identify content chapters (exclude intro/outro/ads)
        content_chapters = [
            ca for ca in chapter_analyses 
            if ca.should_process and ca.chapter_type in [
                ChapterType.MAIN_CONTENT, ChapterType.UNKNOWN
            ]
        ]
        
        # Calculate overall segmentation quality
        segmentation_quality = self._calculate_segmentation_quality(chapter_analyses)
        
        # Generate recommendations
        recommendations = self._generate_segmentation_recommendations(chapter_analyses)
        
        return VideoSegmentation(
            video_url=video_url,
            strategy_used=SegmentationStrategy.CHAPTER_BASED,
            chapters=chapter_analyses,
            total_duration=temporal_metadata.chapters[-1].end_time if temporal_metadata.chapters else 0,
            content_chapters=content_chapters,
            segmentation_quality=segmentation_quality,
            recommendations=recommendations
        )
    
    async def _analyze_chapter(
        self, 
        chapter: Chapter, 
        transcript_text: str,
        temporal_metadata: TemporalMetadata,
        chapter_index: int
    ) -> ChapterAnalysis:
        """Analyze a single chapter for content and temporal significance."""
        
        # Extract chapter text from transcript
        chapter_text = self._extract_chapter_text(
            transcript_text, chapter.start_time, chapter.end_time
        )
        
        # Classify chapter type
        chapter_type = self._classify_chapter_type(chapter.title, chapter_text)
        
        # Analyze content density
        content_density = self._calculate_content_density(chapter_text)
        
        # Estimate temporal events count
        temporal_events_count = self._estimate_temporal_events(chapter_text)
        
        # Extract key entities
        key_entities = self._extract_key_entities(chapter_text)
        
        # Calculate narrative importance
        narrative_importance = self._calculate_narrative_importance(
            chapter, chapter_text, temporal_metadata.chapters, chapter_index
        )
        
        # Assess segmentation confidence
        segmentation_confidence = self._assess_segmentation_confidence(chapter)
        
        # Determine if chapter should be processed
        should_process = self._should_process_chapter(
            chapter_type, content_density, narrative_importance
        )
        
        return ChapterAnalysis(
            chapter=chapter,
            chapter_type=chapter_type,
            content_density=content_density,
            temporal_events_count=temporal_events_count,
            key_entities=key_entities,
            narrative_importance=narrative_importance,
            segmentation_confidence=segmentation_confidence,
            should_process=should_process
        )
    
    def _extract_chapter_text(
        self, transcript_text: str, start_time: float, end_time: float
    ) -> str:
        """Extract text segment corresponding to chapter timeframe."""
        # Simple time-based text extraction (could be enhanced with word-level timing)
        words = transcript_text.split()
        total_duration = len(words) * 2  # Rough estimate: 2 seconds per word
        
        if total_duration <= 0:
            return ""
        
        start_word = int((start_time / total_duration) * len(words))
        end_word = int((end_time / total_duration) * len(words))
        
        start_word = max(0, start_word)
        end_word = min(len(words), end_word)
        
        return ' '.join(words[start_word:end_word])
    
    def _classify_chapter_type(self, title: str, content: str) -> ChapterType:
        """Classify chapter type based on title and content."""
        title_lower = title.lower() if title else ""
        content_lower = content.lower()
        
        # Check title patterns first (more reliable)
        for chapter_type, patterns in self.chapter_patterns.items():
            for pattern in patterns:
                if re.search(pattern, title_lower, re.IGNORECASE):
                    return chapter_type
        
        # Check content patterns if title doesn't match
        for chapter_type, patterns in self.chapter_patterns.items():
            pattern_matches = 0
            for pattern in patterns:
                if re.search(pattern, content_lower, re.IGNORECASE):
                    pattern_matches += 1
            
            # If multiple patterns match, classify as this type
            if pattern_matches >= 2 or (pattern_matches >= 1 and len(patterns) <= 2):
                return chapter_type
        
        # Default to main content if no specific patterns match
        return ChapterType.MAIN_CONTENT
    
    def _calculate_content_density(self, text: str) -> float:
        """Calculate content density (meaningful content vs filler)."""
        if not text:
            return 0.0
        
        text_lower = text.lower()
        words = text_lower.split()
        
        if not words:
            return 0.0
        
        # Score based on content indicators
        high_value_matches = 0
        medium_value_matches = 0
        low_value_matches = 0
        
        for indicator_type, patterns in self.content_indicators.items():
            for pattern in patterns:
                matches = len(re.findall(pattern, text_lower, re.IGNORECASE))
                if indicator_type == 'high_value':
                    high_value_matches += matches
                elif indicator_type == 'medium_value':
                    medium_value_matches += matches
                else:  # low_value
                    low_value_matches += matches
        
        # Calculate density score
        total_words = len(words)
        high_score = min((high_value_matches / total_words) * 3, 0.5)  # Up to 50%
        medium_score = min((medium_value_matches / total_words) * 2, 0.3)  # Up to 30%
        low_penalty = min((low_value_matches / total_words) * 0.5, 0.2)  # Up to 20% penalty
        
        density = high_score + medium_score - low_penalty
        return max(0.0, min(1.0, density))
    
    def _estimate_temporal_events(self, text: str) -> int:
        """Estimate number of temporal events in chapter text."""
        temporal_patterns = [
            r'\b(?:when|while|during|after|before|since|until)\b',
            r'\b\d{4}\b',  # Years
            r'\b(?:yesterday|today|tomorrow|monday|tuesday|wednesday|thursday|friday|saturday|sunday)\b',
            r'\b(?:january|february|march|april|may|june|july|august|september|october|november|december)\b',
            r'\b(?:morning|afternoon|evening|night)\b',
            r'\b(?:last|next|this)\s+(?:week|month|year)\b'
        ]
        
        total_matches = 0
        for pattern in temporal_patterns:
            matches = len(re.findall(pattern, text, re.IGNORECASE))
            total_matches += matches
        
        return total_matches
    
    def _extract_key_entities(self, text: str) -> List[str]:
        """Extract key entities from chapter text (simple approach)."""
        # Simple entity extraction - could be enhanced with NLP
        words = text.split()
        
        # Find capitalized words (potential proper nouns)
        entities = []
        for word in words:
            if word and word[0].isupper() and len(word) > 2:
                # Remove punctuation
                clean_word = re.sub(r'[^\w]', '', word)
                if clean_word and clean_word not in entities:
                    entities.append(clean_word)
        
        # Return top 10 most frequent entities
        from collections import Counter
        entity_counts = Counter(entities)
        return [entity for entity, count in entity_counts.most_common(10)]
    
    def _calculate_narrative_importance(
        self, 
        chapter: Chapter, 
        chapter_text: str,
        all_chapters: List[Chapter],
        chapter_index: int
    ) -> float:
        """Calculate narrative importance of chapter."""
        importance = 0.0
        
        # Position-based importance (middle chapters often more important)
        if len(all_chapters) > 2:
            relative_position = chapter_index / (len(all_chapters) - 1)
            if 0.2 <= relative_position <= 0.8:  # Middle chapters
                importance += 0.3
            elif relative_position <= 0.1 or relative_position >= 0.9:  # First/last
                importance += 0.1
            else:  # Other positions
                importance += 0.2
        
        # Duration-based importance (longer chapters often more important)
        duration = chapter.end_time - chapter.start_time
        if duration >= 300:  # 5+ minutes
            importance += 0.3
        elif duration >= 120:  # 2+ minutes
            importance += 0.2
        else:
            importance += 0.1
        
        # Content-based importance
        content_density = self._calculate_content_density(chapter_text)
        importance += content_density * 0.4
        
        return min(importance, 1.0)
    
    def _assess_segmentation_confidence(self, chapter: Chapter) -> float:
        """Assess confidence in chapter boundary accuracy."""
        confidence = 0.5  # Base confidence
        
        # Title quality affects confidence
        if chapter.title:
            title_words = chapter.title.split()
            meaningful_words = [w for w in title_words if len(w) > 2 and not w.isdigit()]
            if len(meaningful_words) >= 2:
                confidence += 0.3
            elif len(meaningful_words) >= 1:
                confidence += 0.15
        
        # Duration reasonableness affects confidence
        duration = chapter.end_time - chapter.start_time
        if 60 <= duration <= 1800:  # 1 minute to 30 minutes
            confidence += 0.2
        elif 30 <= duration <= 3600:  # 30 seconds to 1 hour
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _should_process_chapter(
        self, chapter_type: ChapterType, content_density: float, narrative_importance: float
    ) -> bool:
        """Determine if chapter should be processed for temporal events."""
        
        # Always skip advertisements and credits
        if chapter_type in [ChapterType.ADVERTISEMENT, ChapterType.CREDITS]:
            return False
        
        # Skip low-content chapters unless they're very important narratively
        if content_density < 0.3 and narrative_importance < 0.7:
            return False
        
        # Skip very short introductions/conclusions with low content
        if chapter_type in [ChapterType.INTRODUCTION, ChapterType.CONCLUSION]:
            if content_density < 0.4 and narrative_importance < 0.5:
                return False
        
        return True
    
    async def _segment_by_content(
        self, video_url: str, temporal_metadata: TemporalMetadata, transcript_text: str
    ) -> VideoSegmentation:
        """Fallback content-based segmentation when chapters unavailable."""
        logger.info("ðŸ“ Segmenting video using content analysis (fallback)")
        
        # Create artificial chapters based on content patterns
        artificial_chapters = self._create_content_based_chapters(transcript_text)
        
        # Analyze artificial chapters
        chapter_analyses = []
        for i, chapter in enumerate(artificial_chapters):
            chapter_analysis = await self._analyze_chapter(
                chapter, transcript_text, temporal_metadata, i
            )
            chapter_analyses.append(chapter_analysis)
        
        content_chapters = [ca for ca in chapter_analyses if ca.should_process]
        segmentation_quality = self._calculate_segmentation_quality(chapter_analyses) * 0.8  # Lower quality for content-based
        recommendations = self._generate_segmentation_recommendations(chapter_analyses)
        
        return VideoSegmentation(
            video_url=video_url,
            strategy_used=SegmentationStrategy.CONTENT_BASED,
            chapters=chapter_analyses,
            total_duration=artificial_chapters[-1].end_time if artificial_chapters else 0,
            content_chapters=content_chapters,
            segmentation_quality=segmentation_quality,
            recommendations=recommendations
        )
    
    def _create_content_based_chapters(self, transcript_text: str) -> List[Chapter]:
        """Create artificial chapters based on content analysis."""
        words = transcript_text.split()
        total_duration = len(words) * 2  # Rough estimate
        
        # Create chapters of roughly 5-minute segments
        segment_duration = 300  # 5 minutes
        chapters = []
        
        current_time = 0
        chapter_num = 1
        
        while current_time < total_duration:
            end_time = min(current_time + segment_duration, total_duration)
            
            chapter = Chapter(
                title=f"Segment {chapter_num}",
                start_time=current_time,
                end_time=end_time,
                url=None
            )
            chapters.append(chapter)
            
            current_time = end_time
            chapter_num += 1
        
        return chapters
    
    async def _segment_hybrid(
        self, video_url: str, temporal_metadata: TemporalMetadata, transcript_text: str
    ) -> VideoSegmentation:
        """Hybrid segmentation combining chapters and content analysis."""
        logger.info("ðŸ”„ Using hybrid segmentation (chapters + content analysis)")
        
        # Start with chapter-based segmentation
        chapter_segmentation = await self._segment_by_chapters(
            video_url, temporal_metadata, transcript_text
        )
        
        # Enhance with content analysis
        enhanced_chapters = []
        for chapter_analysis in chapter_segmentation.chapters:
            # Refine chapter boundaries using content analysis
            refined_analysis = await self._refine_chapter_with_content(
                chapter_analysis, transcript_text
            )
            enhanced_chapters.append(refined_analysis)
        
        content_chapters = [ca for ca in enhanced_chapters if ca.should_process]
        segmentation_quality = self._calculate_segmentation_quality(enhanced_chapters)
        recommendations = self._generate_segmentation_recommendations(enhanced_chapters)
        
        return VideoSegmentation(
            video_url=video_url,
            strategy_used=SegmentationStrategy.HYBRID,
            chapters=enhanced_chapters,
            total_duration=chapter_segmentation.total_duration,
            content_chapters=content_chapters,
            segmentation_quality=segmentation_quality,
            recommendations=recommendations
        )
    
    async def _refine_chapter_with_content(
        self, chapter_analysis: ChapterAnalysis, transcript_text: str
    ) -> ChapterAnalysis:
        """Refine chapter analysis using content analysis."""
        # Re-analyze content density with more sophisticated approach
        chapter_text = self._extract_chapter_text(
            transcript_text, chapter_analysis.chapter.start_time, chapter_analysis.chapter.end_time
        )
        
        refined_content_density = self._calculate_content_density(chapter_text) * 1.1  # Slight boost for hybrid
        refined_content_density = min(1.0, refined_content_density)
        
        # Update analysis
        chapter_analysis.content_density = refined_content_density
        chapter_analysis.should_process = self._should_process_chapter(
            chapter_analysis.chapter_type, refined_content_density, chapter_analysis.narrative_importance
        )
        
        return chapter_analysis
    
    def _calculate_segmentation_quality(self, chapter_analyses: List[ChapterAnalysis]) -> float:
        """Calculate overall segmentation quality score."""
        if not chapter_analyses:
            return 0.0
        
        # Average confidence across chapters
        avg_confidence = sum(ca.segmentation_confidence for ca in chapter_analyses) / len(chapter_analyses)
        
        # Content chapter ratio (good segmentation has reasonable content chapters)
        content_ratio = len([ca for ca in chapter_analyses if ca.should_process]) / len(chapter_analyses)
        
        # Content density distribution (good segmentation has varied content density)
        densities = [ca.content_density for ca in chapter_analyses]
        density_variance = sum((d - sum(densities)/len(densities))**2 for d in densities) / len(densities)
        density_score = min(density_variance * 2, 0.5)  # Higher variance = better segmentation
        
        quality = (avg_confidence * 0.4 + content_ratio * 0.4 + density_score * 0.2)
        return min(quality, 1.0)
    
    def _generate_segmentation_recommendations(
        self, chapter_analyses: List[ChapterAnalysis]
    ) -> List[str]:
        """Generate recommendations for processing optimization."""
        recommendations = []
        
        content_chapters = [ca for ca in chapter_analyses if ca.should_process]
        total_chapters = len(chapter_analyses)
        
        if len(content_chapters) == 0:
            recommendations.append("No content chapters identified - consider lowering quality thresholds")
        elif len(content_chapters) == total_chapters:
            recommendations.append("All chapters marked for processing - consider quality filtering")
        
        avg_content_density = sum(ca.content_density for ca in chapter_analyses) / total_chapters if total_chapters > 0 else 0
        if avg_content_density < 0.3:
            recommendations.append("Low overall content density - transcript quality may be poor")
        
        high_confidence_chapters = len([ca for ca in chapter_analyses if ca.segmentation_confidence >= 0.8])
        if high_confidence_chapters / total_chapters < 0.5:
            recommendations.append("Low segmentation confidence - consider content-based fallback")
        
        if not recommendations:
            recommendations.append("Segmentation quality is good - proceed with temporal event extraction")
        
        return recommendations 