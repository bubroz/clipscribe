---
description: X.AI live search integration rules for real-time grounded intelligence
globs: 
alwaysApply: false
---
---
description: X.AI live search integration rules for real-time grounded intelligence
globs: ["chimera_researcher/retrievers/xai/**/*.py", "chimera_researcher/retrievers/xai_live_search.py", "tools/xai_search.py", "examples/*xai*.py", "tests/*xai*.py"]
alwaysApply: false
---
# X.AI Live Search Integration Rules

## Core Integration Strategy
- **Primary Search Provider**: X.AI Live Search for real-time, grounded intelligence.
- **API Endpoint**: All searches are conducted through the `/v1/chat/completions` endpoint.
- **Methodology**: The model is given a user query and a `search_parameters` object. It uses live data to generate a direct, grounded response. Citations can be returned with the response.

## X.AI Configuration

### Environment Setup
```bash
# Required environment variables for X.AI
XAI_API_KEY=your_xai_api_key_here
RETRIEVER=xai
XAI_MODEL=grok-3-latest
```

### X.AI Live Search Implementation
The correct way to implement X.AI search is to call the `chat/completions` endpoint with a `search_parameters` object.

```python
import os
import httpx
import asyncio
from typing import List, Dict, Optional, Literal

class XAILiveSearchRetriever:
    """
    X.AI Live Search via the chat completions endpoint.
    This retriever gets a grounded response directly, not just a list of URLs.
    """
    def __init__(self, query: str, headers: Optional[Dict] = None):
        self.query = query  # The user's prompt/question
        self.headers = headers or {}
        self.api_key = os.getenv("XAI_API_KEY")
        self.base_url = "https://api.x.ai/v1/chat/completions"
        self.model = os.getenv("XAI_MODEL", "grok-3-latest")

    async def search(
        self,
        search_mode: Literal["auto", "on", "off"] = "on",
        sources: Optional[List[Dict]] = None,
        from_date: Optional[str] = None,
        to_date: Optional[str] = None,
        max_search_results: int = 20,
        return_citations: bool = True
    ) -> Dict:
        """
        Execute a grounded search query using X.AI's chat completions.

        Args:
            search_mode: 'on' to force search, 'auto' to let the model decide.
            sources: A list of configured sources (web, x, news, rss).
            from_date: Start date for search (YYYY-MM-DD).
            to_date: End date for search (YYYY-MM-DD).
            max_search_results: Max sources to consider.
            return_citations: Whether to return source citations.

        Returns:
            The JSON response from the chat completions endpoint.
        """
        if not self.api_key:
            raise ValueError("XAI_API_KEY not found in environment.")

        # Construct the search_parameters object
        search_params = {
            "mode": search_mode,
            "return_citations": return_citations,
            "max_search_results": max_search_results
        }
        if sources:
            search_params["sources"] = sources
        if from_date:
            search_params["from_date"] = from_date
        if to_date:
            search_params["to_date"] = to_date

        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": self.query}],
            "search_parameters": search_params
        }

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                self.base_url,
                headers={"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"},
                json=payload
            )
            response.raise_for_status()
            return response.json()

```

## Advanced Search Patterns

The power of this integration comes from configuring the `sources` in the `search_parameters`.

### Example: Searching News in a Specific Country
```python
xai_retriever = XAILiveSearchRetriever("What are the latest tech regulations in Germany?")
results = await xai_retriever.search(
    sources=[
        {"type": "news", "country": "DE"}
    ]
)
# The 'results' dictionary will contain a generated answer and citations.
```

### Example: Searching Specific X Handles
```python
xai_retriever = XAILiveSearchRetriever("What are the latest updates on Grok?")
results = await xai_retriever.search(
    sources=[
        {"type": "x", "x_handles": ["grok", "xai"]}
    ]
)
```

### Example: Searching Allowed Websites and Excluding Others
```python
xai_retriever = XAILiveSearchRetriever("What are the latest releases at xAI, excluding Wikipedia?")
results = await xai_retriever.search(
    sources=[
        {"type": "web", "allowed_websites": ["x.ai"]},
        {"type": "news", "excluded_websites": ["wikipedia.org"]}
    ]
)
```

### Example: Time-Bounded Historical Search
```python
xai_retriever = XAILiveSearchRetriever("What was the most viral meme in 2022?")
results = await xai_retriever.search(
    from_date="2022-01-01",
    to_date="2022-12-31"
)
```

## Best Practices
1.  **Use the Chat Endpoint**: Always use `/v1/chat/completions` for Live Search.
2.  **Structure the `sources` Object**: Carefully construct the `sources` array to target the exact information needed.
3.  **Handle Citations**: When `return_citations` is `true`, parse the `citations` field from the response to get source URLs.
4.  **Use `mode: 'on'` for Reliability**: When you know you need search, set `mode` to `"on"` instead of `"auto"` to ensure it runs.
5.  **Error Handling**: Wrap calls in `try...except` to handle `httpx.HTTPStatusError` for API-level errors.

By aligning with the official documentation, our X.AI integration will be significantly more powerful and efficient.
