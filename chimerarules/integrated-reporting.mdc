---
description: Integrated Intelligence Reporting System that correlates data from ALL Chimera features to provide holistic insights, cross-feature analytics, and AI-powered recommendations
globs: 
alwaysApply: false
---
---
description: Integrated Intelligence Reporting System that correlates data from ALL Chimera features to provide holistic insights, cross-feature analytics, and AI-powered recommendations
globs: 
  - "**/reporting/**/*.py"
  - "**/unified_collector*.py"
  - "**/correlation_engine*.py"
  - "**/insights_generator*.py"
  - "chimera_researcher/reporting/*.py"
  - "backend/services/reporting_service.py"
alwaysApply: false
---
# Integrated Intelligence Reporting System Rules

*Last Updated: June 22, 2025*

## Core Philosophy
The Integrated Reporting System is NOT just another analytics feature - it's the **intelligence synthesis layer** that makes Chimera a unified platform rather than a collection of tools.

## Key Principles

1. **Cross-Feature Correlation is Everything** - The value comes from connections between features
2. **AI-Powered Insights** - Use Gemini to generate actionable recommendations
3. **Proactive Intelligence** - Don't just report what happened, predict what's needed
4. **Flexible Delivery** - Support multiple output channels without tight coupling

## Architecture Components

### 1. Unified Data Collector
```python
class UnifiedDataCollector:
    """Collects data from ALL Chimera components."""
    
    def __init__(self):
        self.collectors = {
            "analytics": AnalyticsCollector(),
            "watcher": WatcherCollector(),
            "knowledge_graph": KnowledgeGraphCollector(),
            "sat_usage": SATUsageCollector(),
            "entity_extraction": EntityExtractionCollector(),
            "research_quality": ResearchQualityCollector(),
            "cost_tracking": CostTrackingCollector(),
            "llm_performance": LLMPerformanceCollector(),
            "monitoring_status": MonitoringStatusCollector(),
            "validation_feedback": ValidationFeedbackCollector()
        }
    
    async def collect_all_data(self, time_window: str = "7d") -> Dict[str, Any]:
        """Collect data from all features within time window."""
        data = {}
        for name, collector in self.collectors.items():
            try:
                data[name] = await collector.collect(time_window)
            except Exception as e:
                logger.warning(f"Failed to collect {name}: {e}")
                data[name] = None
        return data
```

### 2. Correlation Engine
```python
class CorrelationEngine:
    """Finds patterns and relationships across features."""
    
    def analyze_correlations(self, data: Dict[str, Any]) -> List[Correlation]:
        correlations = []
        
        # Example: Watcher trends vs Knowledge Graph gaps
        if data.get("watcher") and data.get("knowledge_graph"):
            trending_topics = data["watcher"]["trending_topics"]
            graph_entities = data["knowledge_graph"]["entities"]
            
            for topic in trending_topics:
                if not self._topic_in_graph(topic, graph_entities):
                    correlations.append(Correlation(
                        type="knowledge_gap",
                        severity="high",
                        features=["watcher", "knowledge_graph"],
                        insight=f"Trending topic '{topic}' lacks knowledge graph coverage",
                        recommendation=f"Run deep research on '{topic}' to populate graph"
                    ))
        
        # Example: SAT usage vs Research quality
        if data.get("sat_usage") and data.get("research_quality"):
            sat_diversity = data["sat_usage"]["technique_diversity"]
            avg_quality = data["research_quality"]["average_score"]
            
            if sat_diversity < 0.3 and avg_quality < 0.7:
                correlations.append(Correlation(
                    type="technique_impact",
                    severity="medium",
                    features=["sat_usage", "research_quality"],
                    insight="Low SAT diversity correlates with lower research quality",
                    recommendation="Encourage use of underutilized SAT techniques"
                ))
        
        return correlations
```

### 3. AI Insights Generator
```python
class AIInsightsGenerator:
    """Uses Gemini to generate predictive insights."""
    
    def __init__(self, llm_provider):
        self.llm = llm_provider
        self.insight_prompt = """
Analyze this integrated data from Chimera's features and provide:
1. Key patterns and anomalies
2. Predictive insights about what might happen next
3. Specific actionable recommendations
4. Priority ranking of actions

Data:
{data}

Correlations found:
{correlations}

Format as structured insights with clear actions.
"""
    
    async def generate_insights(
        self, 
        data: Dict[str, Any], 
        correlations: List[Correlation]
    ) -> List[Insight]:
        prompt = self.insight_prompt.format(
            data=json.dumps(data, indent=2),
            correlations=[c.to_dict() for c in correlations]
        )
        
        response = await self.llm.agenerate([prompt])
        return self._parse_insights(response)
```

## Cross-Feature Intelligence Patterns

### 1. Knowledge Gap Detection
```python
# Detect when monitoring finds topics not in knowledge graph
if watcher.new_topic not in knowledge_graph.entities:
    recommendation = "Run targeted research to fill knowledge gap"
```

### 2. Cost-ROI Analysis
```python
# Correlate costs with value generated
cost_per_entity = total_cost / new_entities_discovered
if cost_per_entity > threshold:
    recommendation = "Optimize extraction settings to reduce costs"
```

### 3. Quality Prediction
```python
# Predict research quality based on multiple factors
factors = {
    "source_diversity": len(unique_sources) / total_sources,
    "sat_technique_used": technique_id in high_quality_techniques,
    "entity_extraction_enabled": bool(entities_found),
    "knowledge_graph_integration": bool(graph_connections)
}
predicted_quality = quality_model.predict(factors)
```

### 4. Proactive Monitoring
```python
# Suggest monitoring based on research patterns
if research_frequency[topic] > threshold and topic not in monitored_topics:
    recommendation = f"Start continuous monitoring for '{topic}'"
```

## Delivery Channel Implementation

### 1. Local File Reports (Default)
```python
class LocalFileDelivery:
    """Always available, no external dependencies."""
    
    async def deliver(self, report: IntegratedReport):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save structured data
        json_path = f"reports/integrated/data_{timestamp}.json"
        with open(json_path, "w") as f:
            json.dump(report.to_dict(), f, indent=2)
        
        # Save human-readable report
        md_path = f"reports/integrated/report_{timestamp}.md"
        with open(md_path, "w") as f:
            f.write(report.to_markdown())
        
        # Save interactive HTML
        html_path = f"reports/integrated/dashboard_{timestamp}.html"
        with open(html_path, "w") as f:
            f.write(report.to_html())
```

### 2. Webhook Delivery
```python
class WebhookDelivery:
    """Send to any webhook endpoint."""
    
    async def deliver(self, report: IntegratedReport, webhook_url: str):
        async with httpx.AsyncClient() as client:
            await client.post(
                webhook_url,
                json={
                    "type": "integrated_intelligence_report",
                    "timestamp": report.timestamp,
                    "insights": report.insights,
                    "recommendations": report.recommendations,
                    "metrics": report.metrics
                }
            )
```

### 3. Future Channels (Extensible)
```python
# Easy to add new channels
delivery_channels = {
    "local": LocalFileDelivery(),
    "webhook": WebhookDelivery(),
    "google_docs": GoogleDocsDelivery(),  # Future
    "discord": DiscordDelivery(),         # Future
    "email": EmailDelivery()              # Future
}
```

## Integration Checklist

When implementing a new collector:

- [ ] Create collector class inheriting from `BaseCollector`
- [ ] Implement `collect()` method returning structured data
- [ ] Add to `UnifiedDataCollector.collectors` dictionary
- [ ] Create correlation rules in `CorrelationEngine`
- [ ] Add insights patterns for AI generator
- [ ] Write tests for data collection
- [ ] Document data schema
- [ ] Add to monitoring dashboard

## Common Patterns to Detect

1. **Redundant Research** - Multiple researches on same topic without monitoring
2. **Missed Opportunities** - High-value entities discovered but not followed up
3. **Cost Spikes** - Sudden increase in API costs without quality improvement
4. **Coverage Gaps** - Important topics with low research/monitoring coverage
5. **Quality Indicators** - Patterns that predict high/low quality outcomes

## Testing Requirements

```python
@pytest.mark.asyncio
async def test_cross_feature_correlation():
    """Test correlation detection between features."""
    collector = UnifiedDataCollector()
    engine = CorrelationEngine()
    
    # Mock data from different features
    mock_data = {
        "watcher": {"trending_topics": ["AI Safety", "GPT-5"]},
        "knowledge_graph": {"entities": ["OpenAI", "Anthropic"]},
    }
    
    correlations = engine.analyze_correlations(mock_data)
    
    # Should detect knowledge gap
    assert any(c.type == "knowledge_gap" for c in correlations)
    assert any("AI Safety" in c.insight for c in correlations)
```

## Dashboard Requirements

The integrated reporting dashboard should show:

1. **Cross-Feature Health Score** - Overall system effectiveness
2. **Active Correlations** - Live view of detected patterns
3. **AI Recommendations** - Prioritized action items
4. **Historical Trends** - How correlations change over time
5. **ROI Metrics** - Value generated from acting on insights

Remember: The goal is to make Chimera **greater than the sum of its parts** through intelligent correlation and synthesis! :-)
